[
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "Multi-class classification: many labels, only one correct Binary classification: two labels, only one correct Multi-label classification: many labels, several can be correct Multi-class classification: many labels, only one correct Text classification is an extremely popular task. You enjoy working text classifiers in your mail agent: it classifies letters and filters spam. Other applications include document classification, review classification, etc. Text classifiers are often used not as an individual task, but as part of bigger pipelines. For example, a voice assistant classifies your utterance to understand what you want (e.g., set the alarm, order a taxi or just chat) and passes your message to different models depending on the classifier's decision. Another example is a web search engine: it can use classifiers to identify the query language, to predict the type of your query (e.g., informational, navigational, transactional), to understand whether you what to see pictures or video in addition to documents, etc. Since most of the classification datasets assume that only one label is correct (you will see this right now!), in the lecture we deal with this type of classification, i.e. the single-label classification . We mention multi-label classification in a separate section ( Multi-Label Classification ). Datasets for Classification Datasets for text classification are very different in terms of size (both dataset size and examples' size), what is classified, and the number of labels. Look at the statistics below. Dataset Type Number of labels Size (train/test) Avg. length (tokens) SST sentiment 5 or 2 8.5k / 1.1k 19 IMDb Review sentiment 2 25k / 25k 271 Yelp Review sentiment 5 or 2 650k / 50k 179 Amazon Review sentiment 5 or 2 3m / 650k 79 TREC question 6 5.5k / 0.5k 10 Yahoo! Answers question 10 1.4m / 60k 131 AG’s News topic 4 120k / 7.6k 44 Sogou News topic 6 54k / 6k 737 DBPedia topic 14 560k / 70k 67 Some of the datasets can be downloaded here . Some of the datasets can be downloaded here . The most popular datasets are for sentiment classification . They consist of reviews of movies, places or restaurants, and products. There are also datasets for question type classification and topic classification. To better understand typical classification tasks, below you can look at the examples from different datasets. How to: pick a dataset and look at the examples to get a feeling of the task. Or you can come back to this later! SST is a sentiment classification dataset which consists of movie reviews (from Rotten Tomatoes html files). The dataset consists of parse trees of the sentences, and not only entire sentences, but also smaller phrases have a sentiment label. There are five labels: 1 (very negative), 2 (negative), 3 (neutral), 4 (positive), and 5 (very positive) (alternatively, labels can be 0-4). Depending on the used labels, you can get either binary SST-2 dataset (if you only consider positivity and negativity) or fine-grained sentiment SST-5 (when using all labels). Note that the dataset size mentioned above (8.5k/2.2k/1.1k for train/dev/test) is in the number of sentences. However, it also",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "labels can be 0-4). Depending on the used labels, you can get either binary SST-2 dataset (if you only consider positivity and negativity) or fine-grained sentiment SST-5 (when using all labels). Note that the dataset size mentioned above (8.5k/2.2k/1.1k for train/dev/test) is in the number of sentences. However, it also has 215,154 phrases that compose each sentence in the dataset. For more details, see the original paper . Look how sentiment of a sentence is composed from its parts. Label : 3 Review : Makes even the claustrophobic on-board quarters seem fun . Label : 1 Review : Ultimately feels empty and unsatisfying , like swallowing a Communion wafer without the wine . Label : 5 Review : A quiet treasure -- a film to be savored . IMDb is a large dataset of informal movie reviews from the Internet Movie Database. The collection allows no more than 30 reviews per movie. The dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The reviews are highly polarized: they are only negative (with the highest score 4 out of 10) or positive (with the lowest score 7 out of 10). For more details, see the original paper . Label : negative Review Hobgoblins .... Hobgoblins .... where do I begin?!? This film gives Manos - The Hands of Fate and Future War a run for their money as the worst film ever made . This one is fun to laugh at , where as Manos was just painful to watch . Hobgoblins will end up in a time capsule somewhere as the perfect movie to describe the term : \" 80 's cheeze \" . The acting ( and I am using this term loosely ) is atrocious , the Hobgoblins are some of the worst puppets you will ever see , and the garden tool fight has to be seen to be believed . The movie was the perfect vehicle for MST3 K , and that version is the only way to watch this mess . This movie gives Mike and the bots lots of ammunition to pull some of the funniest one - liners they have ever done . If you try to watch this without the help of Mike and the bots ..... God help you ! ! Label : positive Review One of my favorite movies I saw at preview in Seattle . Tom Hulce was amazing , with out words could convey his feelings / thoughts . I actually sent Mike Ferrell some donation money to help the film get distributed . It is good . System says I need more lines but do not want to give away plot stuff . I was in the audience in Seattle with Hulce and director , a writer I think and Mike Ferrell . They talked for about an hour afterwords . Not really a dry eye in the house . Why Hollywood continues to be stupid I do not know . ( actually I do know , it is our fault ,",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "and director , a writer I think and Mike Ferrell . They talked for about an hour afterwords . Not really a dry eye in the house . Why Hollywood continues to be stupid I do not know . ( actually I do know , it is our fault , look what we watch)Well you get what you pay for guys . Get this and see it with someone special . It is a gem . Label : negative Review Okay , if you have a couple hours to waste , or if you just really hate your life , I would say watch this movie . If anything it 's good for a few laughs . Not only do you have obese , topless natives , but also special effects so bad they are probably outlawed in most states . Seriuosly , the rating of ' PG ' is pretty humorous too , once you see the Native Porn Extravaganza . I would n't give this movie to my retarded nephew . You could n't even show this to Iraqi prisoners without violating the Geneva Convention . The plot is sketchy , and cliché , and dumb , and stupid . The acting is horrible , and the ending is so painful to watch I actually began pouring salt into my eye just to take my mind off of the idiocy filling my TV screen . Label : positive Review I really liked this movie ... it was cute . I enjoyed it , but if you did n't , that is your fault . Emma Roberts played a good Nancy Drew , even though she is n't quite like the books . The old fashion outfits are weird when you see them in modern times , but she looks good on them . To me , the rich girls did n't have outfits that made them look rich . I mean , it looks like they got all the clothes -blindfolded- at a garage sale and just decided to put it on all together . All of the outfits were tacky , especially when they wore the penny loafers with their regular outfits . I do not want to make the movie look bad , because it definitely was n't ! Just go to the theater and watch it ! ! ! You will enjoy it ! Label : negative Review I always found Betsy Drake rather creepy , and this movie reinforces that . As another review said , this is a stalker movie that is n't very funny . I watched it because it has CG in it , but he hardly gets any screen time . It 's no \" North by Northwest \" ... Label : negative Review This movie was on t.v the other day , and I did n't enjoy it at all . The first George of the jungle was a good comedy , but the sequel .... completely awful . The new actor and actress to play the lead roles were n't",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "negative Review This movie was on t.v the other day , and I did n't enjoy it at all . The first George of the jungle was a good comedy , but the sequel .... completely awful . The new actor and actress to play the lead roles were n't good at all , they should of had the original actor ( Brendon Fraiser ) and original actress ( i forgot her name ) so this movie gets the 0 out of ten rating , not a film that you can sit down and watch and enjoy , this is a film that you turn to another channel or take it back to the shop if hired or bought . It was good to see Ape the ape back , but was n't as fun as the first , they should of had the new George as Georges son grown up , and still had Bredon and ( what s her face ) in the film , that would 've been a bit better then it was . Label : positive Review I loved This Movie . When I saw it on Febuary 3rd I knew I had to buy It ! ! ! It comes out to buy on July 24th ! ! ! It has cool deaths scenes , Hot girls , great cast , good story , good acting . Great Slasher Film . the Movies is about some serial killer killing off four girls . SEE this movies Label : positive Review gone in 60 seconds is a very good action comedy film that made over $ 100 million but got blasted by most critics . I personally thought this was a great film . The story was believable and has probobly the greatest cast ever for this type of movie including 3 academy award winners nicolas cage , robert duvall and the very hot anjolina jolie . other than the lame stunt at the end this is a perfect blend of action comedy and drama . my score is * * * * ( out of * * * * ) Label : positive Review This is one of the most interesting movies I have ever seen . I love the backwoods feel of this movie . The movie is very realistic and believable . This seems to take place in another era , maybe the late 60 's or early 70 's . Henry Thomas works well with the young baby . Very moving story and worth a look . Label : positive Review I admit it 's very silly , but I 've practically memorized the damn thing ! It holds a lot of good childhood memories for me ( my brother and I saw it opening day ) and I have respect for any movie with FNM on the soundtrack . Label : positive Review I love the series ! Many of the stereotypes portraying Southerrners as hicks are very apparent , but such people do exist all too frequently . The portrayal of",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "it opening day ) and I have respect for any movie with FNM on the soundtrack . Label : positive Review I love the series ! Many of the stereotypes portraying Southerrners as hicks are very apparent , but such people do exist all too frequently . The portrayal of Southern government rings all too true as well , but the sympathetic characters reminds one of the many good things about the South as well . Some things never change , and we see the \" good old boys \" every day ! There is a Lucas Buck in every Southern town who has only to make a phone call to make things happen , and the storybook \" po ' white trash \" are all too familiar . Aside from the supernatural elements , everything else could very well happen in the modern South ! I somehow think Trinity , SC must have been in Barnwell County ! The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 . Depending on the number of labels, you can get either Yelp Full (all 5 labels) or Yelp Polarity (positive and negative classes) dataset. Full has 130,000 training samples and 10,000 testing samples in each star, and the Polarity dataset has 280,000 training samples and 19,000 test samples in each polarity. For more details, see the Kaggle Challenge page . Label : 4 Review I had a serious craving for Roti. So glad I found this place. A very small menu selection but it had exactly what I wanted. The serving for $8.20 after tax is enough for 2 meals. I know where to go from now on for a great meal with leftovers. This is a noteworthy place to bring my Uncle T.J. who's a Trini when he comes to visit. Label : 2 Review The actual restaurant is fine, the service is friendly and good. I am not going to go in to the food other than to say, no. Oh well $340 bucks and all I can muster is a no. Label : 5 Review What a cool little place tucked away behind a strip mall. Would never have found this if it was not suggested by a good friend who raved about the cappuccino! He is world traveler, so, it's a must try if it's the best cup he's ever had. He was right! Don't know if it's in the beans or the care that they take to make it with a fab froth decoration on top. My hubby and I loved the caramel brulee taste.. My son loved the hot \"\"warm\"\" cocoa. Yeah, we walked in as a family last night and almost everyone turned our way since we did not fit the hip college crowd. Everyone was really friendly, though. The sweet young man behind the counter gave my son some micro cinnamon doughnuts and scored major points with the little dude! We will be back. Label : 3 Review Jersey Mike's is okay. It's a chain place, and a bit over priced for",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "crowd. Everyone was really friendly, though. The sweet young man behind the counter gave my son some micro cinnamon doughnuts and scored major points with the little dude! We will be back. Label : 3 Review Jersey Mike's is okay. It's a chain place, and a bit over priced for fast food. I ordered a philly cheese steak. It was mostly bread, with a few thing microscopic slices of meat. A little cheese too. And a sliver or two of peppers. But mostly, it was bread. I think it's funny the people that work here try to make small talk with you. \"So, what are you guys up to tonight?\" I think it would be fun to just try and f*#k with them, and say something like, \"Oh you know, smoking a little meth and just chilling with some hookers.\" See what they say to that. Label : 5 Review Love it!!! Wish we still lived in Arizona as Chino is the one thing we miss. Every time I think about Chino Bandido my mouth starts watering. If I am ever in the state again I will drive out of my way just to go to it again. YUMMY! Label : 4 Review I have been here a few times, but mainly at dinner. Dinner Has always been great, great waiters and staff, with a LCD TV playing almost famous famous bolywood movies. LOL It cracks me up when they dance.....LOL...anyhow, Great vegetarian choices for people who eat there veggies, but trust me, I am a MEAT eater, Chicekn Tika masala, Little dry but still good. My favorite is there Palak Paneer. Great for the vegetarian. I have also tried there lunch Buffet for 11.00. I give a Thumbs up.. Good, serve yourself, and pig out!!!!!! Label : 3 Review Very VERY average. Actually disappointed in the braciole. Kelsey with the pretty smile and form fitting shorts would probably make me think about going back and trying something different. Label : 1 Review So, what kind of place serves chicken fingers and NO Ranch dressing?????? The only sauces they had was honey mustard and \"\"Canes Secret sauce\"\" Can I say EEWWWW!! I thought that the sauce tasted terrible. I am not too big a fan of honey mustard but I do On occasion eat it if there is nothing else And that wasn't even good! The coleslaw was awful also. I do have to say that the chicken fingers were very juicy but also very bland. Those were the only 2 items that I tried, not that there were really any more items on the menu, Texas toast? And Fries I think?? Overall, I would never go back. Label : 4 Review Good food, good drinks, fun bar. There are quite a few Buffalo Wild Wings in the valley and they're a fun place to grab a quick lunch or dinner and watch the game. They have a pretty good garden burger and their buffalo chips (french fry-ish things) are really good. If you like bloody mary's, they have the best one.",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "few Buffalo Wild Wings in the valley and they're a fun place to grab a quick lunch or dinner and watch the game. They have a pretty good garden burger and their buffalo chips (french fry-ish things) are really good. If you like bloody mary's, they have the best one. It's so good...really spicy and filled with celery and olives. Be careful when you come though, if there is a game on, you'll have to get there early or you definitely won't get a spot to sit. Label : 5 Review Excellent in every way. Attentive and fun owners who tend bar every weekend night - GREAT live music, excellent wine selection. Keeping Fountain Hills young.... one weekend at a time. Label : 1 Review After repeat visits it just gets worse - the service, that is. It was as if we were held hostage and could not leave for a full 25 minutes because that's how long it took to receive hour check after several requests to several different employees. Female servers might be somewhat cute but know absolutely nothing about beer and this is a brewpub. I asked if they have an seasonal beers and the reply was no, that they only sell their own beers! Even more amusing is their \"\"industrial pale ale\"\" is an IPA but it is not bitter. So, they say it's an IPA but it's not bitter, it's not a true-to-style IPA. Then people say \"\"Oh I don't like IPA's\"\" and want something else. Their attempt to rename a beer/style is actually hurting them. Amazing. The Amazon reviews dataset consists of reviews from amazon which include product and user information, ratings, and a plaintext review. The dataset is obtained from the Stanford Network Analysis Project (SNAP) . Depending on the number of labels, you can get either Amazon Full (all 5 labels) or Amazon Polarity (positive and negative classes) dataset. Full has 600,000 training samples and 130,000 testing samples in each star, and the Polarity dataset has 1800,000 training samples and 200,000 test samples in each polarity. The fields used are review title and review content. Label : 4 Review Title : good for a young reader Review Content : just got this book since i read it when i was younger and loved it. i will reread it one of these days, but i bet its pretty lame 15 years later, oh well. Label : 2 Review Title : The Castle in the Attic Review Content : i read the castle in the attic and i thoght it wasn't very entrtaning. but that's just my opinion. i thought on some of the chapters it dragged on and lost me alot. like it would talk about one thing and then another without alot of detail. for my opinion, it was an ok book. it had it's moments like whats going to happen next then it was really boring. Label : 1 Review Title : worst book in the world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Review Content : This was the worst book I have ever read in my entire life!",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "for my opinion, it was an ok book. it had it's moments like whats going to happen next then it was really boring. Label : 1 Review Title : worst book in the world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Review Content : This was the worst book I have ever read in my entire life! I was forced to read it for school. It was complicated and very boring. I would not recommend this book to anyone!! Please don't waste your time and money to read this book!! Read something else!! Label : 3 Review Title : It's okay. Review Content : I'm using it for track at school as a sprinter. It's okay, but to hold it together is a velcro strap. So you either use the laces or take them out and use only the velcro. Also, if you order them, order them a half size or a full size biggerthen your normal size or else it'll be a tight squeeze. Plus side, you can still run on your toes in them. Label : 5 Review Title : Time Well Spent Review Content : For those beginning to read the classics this one is a great hook. While the characters are complex the story is linear and the allusions are simple enough to follow. One can't help but hope Tess's life will somehow turn out right although knowing it will not. The burdens she encounters seem to do little to stop her from moving forward. Life seems so unfair to her, but Hardy handles her masterfully; indeed it is safe to say Hardy loves her more than God does. Label : 2 Review Title : Not a brilliant book but... Review Content : I didn't like this book very much. It was silly. I'm sorry that I've wasted time reading this book. There are better books to read. Label : 3 Review Title : Simple Review Content : This book was not anything special. Although I love romances, it was too simple. The symbolism was spelled out to the readers in a blunt manner. The less educated readers may appreciate it. The wording was quite beautiful at times and the plot was enchanting (perfect for a movie) but it is not heart wrenching like the movie Titantic (which was a must see!) ;) Label : 1 Review Title : WHY??????????????????????????????????????????????????? Review Content : WHY are poor, innocent school kids forced to read this intensly dull text? It is enough to put anyone off English Lit for LIFE. If anyone in authority is reading this, PLEASE take this piece of junk OFF the sylabus...PLEASE!!!!!!!!!!! Label : 4 Review Title : looking back Review Content : I have read several Thomas Hardy novels starting with The Mayor of Casterbridge many years ago in high school and I never really appreciated the style and the fact that like other Hardy novels Tess is a love story and a very good story. Worth reading Label : 5 Review Title : Great Puzzle Review Content : This is an excellent puzzle for very young children. Melissa & Doug products are",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "never really appreciated the style and the fact that like other Hardy novels Tess is a love story and a very good story. Worth reading Label : 5 Review Title : Great Puzzle Review Content : This is an excellent puzzle for very young children. Melissa & Doug products are well made and kids love them. This puzzle is wooden so kids wont destroy it if they try to roughly put the pieces in. The design is adorable and makes a great gift for any young animal lover. TREC is a dataset for classification of free factual questions. It defines a two-layered taxonomy, which represents a natural semantic classification for typical answers in the TREC task. The hierarchy contains 6 coarse classes (ABBREVIATION, ENTITY, DESCRIPTION, HUMAN, LOCATION and NUMERIC VALUE) and 50 fine classes. For more details, see the original paper . Label : DESC (description) Question : How did serfdom develop in and then leave Russia ? Label : ENTY (entity) Question : What films featured the character Popeye Doyle ? Label : HUM (human) Question : What team did baseball 's St. Louis Browns become ? Label : HUM (human) Question : What is the oldest profession ? Label : DESC (description) Question : How can I find a list of celebrities ' real names ? Label : ENTY (entity) Question : What fowl grabs the spotlight after the Chinese Year of the Monkey ? Label : ABBR (abbreviation) Question : What is the full form of .com ? Label : ENTY (entity) Question : What 's the second - most - used vowel in English ? Label : DESC (description) Question : What are liver enzymes ? Label : HUM (human) Question : Name the scar-faced bounty hunter of The Old West . Label : NUM (numeric value) Question : When was Ozzy Osbourne born ? Label : DESC (description) Question : Why do heavier objects travel downhill faster ? Label : HUM (human) Question : Who was The Pride of the Yankees ? Label : HUM (human) Question : Who killed Gandhi ? Label : LOC (location) Question : What sprawling U.S. state boasts the most airports ? Label : DESC (description) Question : What did the only repealed amendment to the U.S. Constitution deal with ? Label : NUM (numeric value) Question : How many Jews were executed in concentration camps during WWII ? Label : DESC (description) Question : What is \" Nine Inch Nails \" ? Label : DESC (description) Question : What is an annotated bibliography ? Label : NUM (numeric value) Question : What is the date of Boxing Day ? Label : ENTY (entity) Question : What articles of clothing are tokens in Monopoly ? Label : HUM (human) Question : Name 11 famous martyrs . Label : DESC (description) Question : What 's the Olympic motto ? Label : NUM (numeric value) Question : What is the origin of the name ` Scarlett ' ? The dataset is gathered from Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset . In",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "11 famous martyrs . Label : DESC (description) Question : What 's the Olympic motto ? Label : NUM (numeric value) Question : What is the origin of the name ` Scarlett ' ? The dataset is gathered from Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset . In contains the 10 largest main categories: \"Society & Culture\", \"Science & Mathematics\", \"Health, \"Education & Reference\", \"Computers & Internet\", \"Sports\", \"Business & Finance\", \"Entertainment & Music\", \"Family & Relationships\", \"Politics & Government\". Each class contains 140,000 training samples and 5,000 testing samples. The data consists of question title and content, as well as the best answer. Label : Society & Culture Question Title : Why do people have the bird, turkey for thanksgiving? Question Content : Why this bird? Any Significance? Best Answer It is believed that the pilgrims and indians shared wild turkey and venison on the original Thanksgiving. Turkey's \"Americanness\" was established by Benjamin Franklin, who had advocated for the turkey, not the bald eagle, becoming the national bird. Label : Science & Mathematics Question Title : What is an \"imaginary number\"? Question Content : What is an \"imaginary number\", and how is it treated in algebra equations? Best Answer Imaginary numbers are numbers than when squared equal a negative number, as in i^2 = -1, where i is the imaginary number. You'll also often see them represented as i = √-1 (that's the square root of -1). Don't be confused by the poorly chosen name - imaginary numbers do indeed exist and are used in advanced math, such as in the physics of electromagnetic fields. The analogy that Wikipedia uses is a good one - just like you don't need the concept of fractions to count stones, it doesn't mean that fractions don't exist. :) Label : Health Question Title : Does echinacea really help prevent colds? Question Content : Or is a waste of money... Best Answer Well, there appears to be some controvery about this. While some people swear by the stuff, others say that it has no real effect on overcoming a cold. Here are some links, one of which is from a National Institute of Health study. I hope these help you decide whether to head to the health store or not. Label : Education & Reference Question Title : How do I find an out of print book? Question Content : When I was a kid I remember seeing a book that was like an yearbook of all newspapers published by the Times during WW II. Each of the years is compiled into a different book. It gave one a very uniqie perspecitev into the UK druing the war, and even had advertisements from thaat time. Anybody out there know how to track such books? Best Answer here are several websites that you can find rare or out of print books. A couple would be alibris.com or abebooks.com. These sites list books by booksellers all over the country and some internationally. Label : Computers & Internet Question Title : How can I record audio",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "books? Best Answer here are several websites that you can find rare or out of print books. A couple would be alibris.com or abebooks.com. These sites list books by booksellers all over the country and some internationally. Label : Computers & Internet Question Title : How can I record audio directly from the browser to the web server? Question Content : For a podcasting application, I'd like my web server to be able to receive audio straight from the browser. Something like a \"Push to talk\" button. It seems it's possible to do this with Flash. Is there any other way? With Flash, do I need to buy a Macromedia server licence, or are there alternatives to have Flash on the browser talk to my server? Best Answer Userplane has an audio/video recorder that will do that - you can check it out at http://www.userplane.com/apps/videoRecorder.cfm Label : Sports Question Title : Why doesn't the NBA implement a minor leagues? Question Content : I don't want to see any more High School kids on the court, shooting airballs and missing defensive assignments. Best Answer The NBA does have minor leagues - they're called the CBA, and the International leagues. :) Seriously - because viewers seem to value explosiveness over efficiency, I think we're seeing a major shift in the average age of NBA players towards young athletes that are quicker, high-flying and more resilient to injury. I wouldn't be surprised at all if by the end of this decade the average age of the league allstars is well under 25. Label : Business & Finance Question Title : When will Google buy Yahoo? Question Content : The two businesses are very complementary in terms of strengths and weaknesses. Do we want to beat ourselves up competing with each other for resources and market share, or unite to beat MSFT? Best Answer Their respective market caps are too close for this to ever happen. Interestingly, many reporters, analysts and tech pundits that I talk to think that the supposed competition between Google and Yahoo is fallacious, and that they are very different companies with very different strategies. Google's true competitor is often seen as being Microsoft, not Yahoo. This would support your claim that they are complementary. Label : Entertainment & Music Question Title : Can someone tell me what happened in Buffy's series finale? Question Content : I had to work and missed the ending. Best Answer The gang makes an attack on the First's army, aided by Willow, who performs a powerful spell to imbue all of the Potentials with Slayer powers. Meanwhile, wearing the amulet that Angel brought, Spike becomes the decisive factor in the victory, and Sunnydale is eradicated. Buffy and the gang look back on what's left of Sunnydale, deciding what to do next... --but more importantly, there will no longer be any slaying in Sunnydale, or is that Sunnyvale.... Label : Family & Relationships Question Title : How do you know if you're in love? Question Content : Is it possible to know for sure? Best Answer In",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "deciding what to do next... --but more importantly, there will no longer be any slaying in Sunnydale, or is that Sunnyvale.... Label : Family & Relationships Question Title : How do you know if you're in love? Question Content : Is it possible to know for sure? Best Answer In my experience you just know. It's a long term feeling of always wanting to share each new experience with the other person in order to make them happy, to laugh or to know what they think about it. It's jonesing to call even though you just got off an hour long phone call with them. It's knowing that being with them makes you a better person. It's all of the above and much more. Label : Politics & Government Question Title : How come it seems like Lottery winners are always the ones that buy tickets in low income areas.? Question Content : Pure luck or Government's way of trying to balance the rich and the poor. Best Answer I would put it down to psychology. People who feel they are well-off feel no need to participate in the lottery programs. While those who feel they are less than well off think \"Why not bet a buck or two on the chance to make a few million?\". It would seem to make sense to me. addition: Yes Matt - agreed. I just didn't state it as eloquently. Feeling 'no need to participate' is as you say related to education, and those well off tend to have a better education. The AG’s corpus was obtained from news articles on the web . From these articles, only the AG’s corpus contains only the title and description fields from the the 4 largest classes. The dataset was introduced in this paper . Label : Sci/Tech Title : Learning to write with classroom blogs Description Last spring Marisa Dudiak took her second-grade class in Frederick County, Maryland, on a field trip to an American Indian farm. Label : Sports Title : Schumacher Triumphs as Ferrari Seals Formula One Title Description BUDAPEST (Reuters) - Michael Schumacher cruised to a record 12th win of the season in the Hungarian Grand Prix on Sunday to hand his Ferrari team a sixth successive constructors' title. Label : Business Title : DoCoMo and Motorola talk phones Description Japanese mobile phone company DoCoMo is in talks to buy 3G handsets from Motorola, the world's second largest handset maker. Label : World Title : Sharon 'backs settlement homes' Description Reports say Israeli PM Ariel Sharon has given the green light to new homes in West Bank settlements. Label : Business Title : Why Hugo Chavez Won a Landslide Victory Description When the rule of Venezuelan President Hugo Chavez was reaffirmed in a landslide 58-42 percent victory on Sunday, the opposition who put the recall vote on the ballot was stunned. They obviously don't spend much time in the nation's poor neighborhoods. Label : Sci/Tech Title : Free-Speech for Online Gambling Ads Sought Description The operator of a gambling news site on the Internet",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "58-42 percent victory on Sunday, the opposition who put the recall vote on the ballot was stunned. They obviously don't spend much time in the nation's poor neighborhoods. Label : Sci/Tech Title : Free-Speech for Online Gambling Ads Sought Description The operator of a gambling news site on the Internet has asked a federal judge to declare that advertisements in U.S. media for foreign online casinos and sports betting outlets are protected by free-speech rights. Label : World Title : Kerry takes legal action against Vietnam critics (AFP) Description AFP - Democratic White House hopeful John Kerry's campaign formally alleged that a group attacking his Vietnam war record had illegal ties to US President George W. Bush's reelection bid. Label : Sports Title : O'Leary: I won't quit Description The Villa manager was said to be ready to leave the midlands club unless his assistants Roy Aitken and Steve McGregor were also given new three-and-a-half year deals. Label : World Title : Egypt eyes possible return of ambassador to Israel Description CAIRO - Egypt raised the possibility Tuesday of returning an ambassador to Israel soon, according to the official Mena news agency, a move that would signal a revival of full diplomatic ties after a four-year break. Label : Sports Title : Henry wants silverware Description Arsenal striker Thierry Henry insisted there must be an end product to the Gunners' record-breaking run. As Arsenal equalled Nottingham Forest's 42-game unbeaten League run Henry said: \"Even on the pitch we didn't realise what we had done.\" Label : Sci/Tech Title : Scientists Focus on Algae in Maine Lake (AP) Description AP - Scientists would kill possibly thousands of white perch under a project to help restore the ecological balance of East Pond in the Belgrade chain of lakes in central Maine. The Sogou News corpus was obtained from the combination of the SogouCA and SogouCS news corpora. The dataset consists of news articles (title and content fields) labeled with 5 categories: “sports”, “finance”, “entertainment”, “automobile” and “technology”. The original dataset is in Chinese, but you can produce Pinyin – a phonetic romanization of Chinese. You can do it using pypinyin package combined with jieba Chinese segmentation system (this is what the paper introducing the dataset did, and this is what I show you in the examples). The models for English can then be applied to this dataset without change. The dataset was introduced in this paper , the dataset in Pinyin can be downloaded here . Lena : Here I picked very small texts - usually, the samples are much longer. Label : automobile Title : tu2 we2n -LG be1i be3n sa4i di4 2 lu2n zha4n ba4 cha2ng ha4o ko3ng jie2 de3ng qi2 sho3u fu4 pa2n ta3o lu4n Content xi1n la4ng ti3 yu4 xu4n be3i ji1ng shi2 jia1n 5 yue4 28 ri4 ,LG be1i shi4 jie4 qi2 wa2ng sa4i be3n sa4i di4 2 lu2n za4i ha2n guo2 ka1i zha4n . zho1ng guo2 qi2 sho3u cha2ng ha4o , gu3 li4 , wa2ng ya2o , shi2 yue4 ca1n jia1 bi3 sa4i . tu2 we2i xia4n cha3ng",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "be3i ji1ng shi2 jia1n 5 yue4 28 ri4 ,LG be1i shi4 jie4 qi2 wa2ng sa4i be3n sa4i di4 2 lu2n za4i ha2n guo2 ka1i zha4n . zho1ng guo2 qi2 sho3u cha2ng ha4o , gu3 li4 , wa2ng ya2o , shi2 yue4 ca1n jia1 bi3 sa4i . tu2 we2i xia4n cha3ng shu4n jia1n . wa3ng ye4 bu4 zhi1 chi2 Flash Label : automobile Title : qi4 che1 pi2n da4o Content xi1n we2n jia3n suo3 : ke4 la2i si1 le4 300C go4ng 20 zha1ng ke3 shi3 jia4n pa2n ca1o zuo4 [ fa1ng xia4ng jia4n l: sha4ng yi1 zha1ng ; fa1ng xia4ng jia4n r: xia4 yi1 zha1ng ; hui2 che1 : zha1 ka4n yua2n da4 tu2 ]\\ ge1ng duo1 tu2 pia4n : ce4 hua4 : bia1n ji2 : me3i bia1n : zhi4 zuo4 :GOODA ji4 shu4 : Label : finance Title : shi2 da2 qi1 huo4 : hua2ng ji1n za3o pi2ng (06-11) Content shi4 cha3ng jia1 da4 me3i guo2 she1ng xi1 yu4 qi1 , me3i yua2n ji4n qi1 zo3u shi4 ba3o chi2 de2 xia1ng da1ng pi2ng we3n , ji1n jia4 ga1o we4i mi2ng xia3n sho4u ya1 , xia4 jia4ng to1ng da4o ba3o chi2 wa2n ha3o , zhe4n da4ng si1 lu4 ca1o zuo4 . gua1n wa4ng wa3ng ye4 bu4 zhi1 chi2 Flash hua2ng ji1n qi1 huo4 zi1 xu4n la2n mu4 DBpedia is a crow-sourced community effort to extract structured information from Wikipedia. The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014 . From each of the 14 ontology classes, the dataset contains 40,000 randomly chosen training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000, the testing dataset - 70,000. The dataset was introduced in this paper . Label : Company Title : Marvell Software Solutions Israel Abstract Marvell Software Solutions Israel known as RADLAN Computer Communications Limited before 2007 is a wholly owned subsidiary of Marvell Technology Group that specializes in local area network (LAN) technologies. Label : EducationalInstitution Title : Adarsh English Boarding School Abstract Adarsh English Boarding School is coeducational boarding school in Phulbari a suburb of Pokhara Nepal. Nabaraj Thapa is the founder and chairman of the school. The School motto reads Education For Better Citizen. Label : Artist Title : Esfandiar Monfaredzadeh Abstract Esfandiar Monfaredzadeh (Persian : اسفندیار منفردزاده) is an Iranian composer and director. He was born in 1941 in Tehran His major works are Gheisar Dash Akol Tangna Gavaznha. He has 2 daughters Bibinaz Monfaredzadeh and Sanam Monfaredzadeh Woods (by marriage). Label : Athlete Title : Elena Yakovishina Abstract Elena Yakovishina (born September 17 1992 in Petropavlovsk-Kamchatsky Russia) is an alpine skier from Russia. She competed for Russia at the 2014 Winter Olympics in the alpine skiing events. Label : OfficeHolder Title : Jack Masters Abstract John Gerald (Jack) Masters (born September 27 1931) is a former Canadian politician. He served as mayor of the city of Thunder Bay Ontario and as a federal Member of Parliament. Label : MeanOfTransportation Title : HMS E35 Abstract HMS E35 was a British E class submarine built by John Brown",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "Gerald (Jack) Masters (born September 27 1931) is a former Canadian politician. He served as mayor of the city of Thunder Bay Ontario and as a federal Member of Parliament. Label : MeanOfTransportation Title : HMS E35 Abstract HMS E35 was a British E class submarine built by John Brown Clydebank. She was laid down on 20 May 1916 and was commissioned on 14 July 1917. Label : Building Title : Aspira Abstract Aspira is a 400 feet (122 m) tall skyscraper in the Denny Triangle neighborhood of Seattle Washington. It has 37 floors and mostly consists of apartments. Construction began in 2007 and was completed in late 2009. Label : NaturalPlace Title : Sierra de Alcaraz Abstract The Sierra de Alcaraz is a mountain range of the Cordillera Prebética located in Albacete Province southeast Spain. Its highest peak is the Pico Almenara with an altitude of 1796 m. Label : Village Title : Piskarki Abstract Piskarki [pisˈkarki] is a village in the administrative district of Gmina Jeżewo within Świecie County Kuyavian-Pomeranian Voivodeship in north-central Poland. The village has a population of 135. Label : Animal Title : Lesser small-toothed rat Abstract The Lesser Small-toothed Rat or Western Small-Toothed Rat (Macruromys elegans) is a species of rodent in the family Muridae. It is found only in West Papua Indonesia. Label : Plant Title : Vangueriopsis gossweileri Abstract Vangueriopsis gossweileri is a species of flowering plants in the family Rubiaceae. It occurs in West-Central Tropical Africa (Cabinda Province Equatorial Guinea and Gabon). Label : Album Title : Dreamland Manor Abstract Dreamland Manor is the debut album of German power metal band Savage Circus. The album sounds similar to older classic Blind Guardian. Label : Film Title : The Case of the Lucky Legs Abstract The Case of the Lucky Legs is a 1935 mystery film the third in a series of Perry Mason films starring Warren William as the famed lawyer. Label : WrittenWork Title : Everybody Loves a Good Drought Abstract Everybody Loves a Good Drought is a book written by P. Sainath about his research findings of poverty in the rural districts of India. The book won him the Magsaysay Award. General View Here we provide a general view on classification and introduce the notation. This section applies to both classical and neural approaches. We assume that we have a collection of documents with ground-truth labels. The input of a classifier is a document \\(x=(x_1, \\dots, x_n)\\) with tokens \\((x_1, \\dots, x_n)\\), the output is a label \\(y\\in 1\\dots k\\). Usually, a classifier estimates probability distribution over classes, and we want the probability of the correct class to be the highest. Get Feature Representation and Classify Text classifiers have the following structure: feature extractor A feature extractor can be either manually defined (as in classical approaches ) or learned (e.g., with neural networks ). classifier A classifier has to assign class probabilities given feature representation of a text. The most common way to do this is using logistic regression , but other variants are also possible (e.g., Naive Bayes classifier or",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "defined (as in classical approaches ) or learned (e.g., with neural networks ). classifier A classifier has to assign class probabilities given feature representation of a text. The most common way to do this is using logistic regression , but other variants are also possible (e.g., Naive Bayes classifier or SVM ). In this lecture, we'll mostly be looking at different ways to build feature representation of a text and to use this representation to get class probabilities. Generative and Discriminative Models A classification model can be either generative or discriminative . generative models Generative models learn joint probability distribution of data \\(p(x, y) = p(x|y)\\cdot p(y)\\). To make a prediction given an input \\(x\\), these models pick a class with the highest joint probability: \\(y = \\arg \\max\\limits_{k}p(x|y=k)\\cdot p(y=k)\\). discriminative models Discriminative models are interested only in the conditional probability \\(p(y|x)\\), i.e. they learn only the border between classes. To make a prediction given an input \\(x\\), these models pick a class with the highest conditional probability: \\(y = \\arg \\max\\limits_{k}p(y=k|x)\\). In this lecture, we will meet both generative and discriminative models. Classical Methods for Text Classification In this part, we consider classical approaches for text classification. They were developed long before neural networks became popular, and for small datasets can still perform comparably to neural models. Lena : Later in the course, we will learn about transfer learning which can make neural approaches better even for very small datasets. But let's take this one step at a time: for now, classical approaches are a good baseline for your models. Naive Bayes Classifier A high-level idea of the Naive Bayes approach is given below: we rewrite the conditional class probability \\(P(y=k|x)\\) using Bayes's rule and get \\(P(x|y=k)\\cdot P(y=k)\\). This is a generative model! Naive Bayes is a generative model: it models the joint probability of data. Note also the terminology: prior probability \\(P(y=k)\\): class probability before looking at data (i.e., before knowing \\(x\\)); posterior probability \\(P(y=k|x)\\): class probability after looking at data (i.e., after knowing the specific \\(x\\)); joint probability \\(P(x, y)\\): the joint probability of data (i.e., both examples \\(x\\) and labels \\(y\\)); maximum a posteriori (MAP) estimate: we pick the class with the highest posterior probability. How to define P(x|y=k) and P(y=k)? P(y=k) : count labels \\(P(y=k)\\) is very easy to get: we can just evaluate the proportion of documents with the label \\(k\\) (this is the maximum likelihood estimate, MLE). Namely, \\[P(y=k)=\\frac{N(y=k)}{\\sum\\limits_{i}N(y=i)},\\] where \\(N(y=k)\\) is the number of examples (documents) with the label \\(k\\). P(x|y=k) : use the \"naive\" assumptions, then count Here we assume that document \\(x\\) is represented as a set of features, e.g., a set of its words \\((x_1, \\dots, x_n)\\): \\[P(x| y=k)=P(x_1, \\dots, x_n|y=k).\\] The Naive Bayes assumptions are Bag of Words assumption: word order does not matter, Conditional Independence assumption: features (words) are independent given the class. Intuitively, we assume that the probability of each word to appear in a document with class \\(k\\) does not depend on context (neither word order nor other words at all). For example, we can say that",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "order does not matter, Conditional Independence assumption: features (words) are independent given the class. Intuitively, we assume that the probability of each word to appear in a document with class \\(k\\) does not depend on context (neither word order nor other words at all). For example, we can say that awesome , brilliant , great are more likely to appear in documents with a positive sentiment and awful , boring , bad are more likely in negative documents, but we know nothing about how these (or other) words influence each other. awesome brilliant great awful boring bad With these \"naive\" assumptions we get: \\[P(x| y=k)=P(x_1, \\dots, x_n|y=k)=\\prod\\limits_{t=1}^nP(x_t|y=k).\\] The probabilities \\(P(x_i|y=k)\\) are estimated as the proportion of times the word \\(x_i\\) appeared in documents of class \\(k\\) among all tokens in these documents: \\[P(x_i|y=k)=\\frac{N(x_i, y=k)}{\\sum\\limits_{t=1}^{|V|}N(x_t, y=k)},\\] where \\(N(x_i, y=k)\\) is the number of times the token \\(x_i\\) appeared in documents with the label \\(k\\), \\(V\\) is the vocabulary (more generally, a set of all possible features). What if \\(N(x_i, y=k)=0\\)? Need to avoid this! What if \\(N(x_i, y=k)=0\\), i.e. in training we haven't seen the token \\(x_i\\) in the documents with class \\(k\\)? This will null out the probability of the whole document, and this is not what we want! For example, if we haven't seen some rare words (e.g., pterodactyl or abracadabra ) in training positive examples, it does not mean that a positive document can never contain these words. pterodactyl abracadabra To avoid this, we'll use a simple trick: we add to counts of all words a small \\(\\delta\\): \\[P(x_i|y=k)=\\frac{\\color{red}{\\delta} +\\color{black} N(x_i, y=k) }{\\sum\\limits_{t=1}^{|V|}(\\color{red}{\\delta} +\\color{black}N(x_t, y=k))} = \\frac{\\color{red}{\\delta} +\\color{black} N(x_i, y=k) }{\\color{red}{\\delta\\cdot |V|}\\color{black} + \\sum\\limits_{t=1}^{|V|}\\color{black}N(x_t, y=k)} ,\\] where \\(\\delta\\) can be chosen using cross-validation. Note : this is Laplace smoothing (aka Add-1 smoothing if \\(\\delta=1\\)). We'll learn more about smoothings in the next lecture when talking about Language Modeling. Making a Prediction As we already mentioned, Naive Bayes (and, more broadly, generative models) make a prediction based on the joint probability of data and class: \\[y^{\\ast} = \\arg \\max\\limits_{k}P(x, y=k) = \\arg \\max\\limits_{k} P(y=k)\\cdot P(x|y=k).\\] Intuitively, Naive Bayes expects that some words serve as class indicators. For example, for sentiment classification tokens awesome , brilliant , great will have higher probability given positive class then negative. Similarly, tokens awful , boring , bad will have higher probability given negative class then positive. awesome brilliant great awful boring bad Final Notes on Naive Bayes Practical Note : Sum of Log-Probabilities Instead of Product of Probabilities The main expression Naive Bayes uses for classification is a product lot of probabilities: \\[P(x, y=k)=P(y=k)\\cdot P(x_1, \\dots, x_n|y)=P(y=k)\\cdot \\prod\\limits_{t=1}^nP(x_t|y=k).\\] A product of many probabilities may be very unstable numerically. Therefore, usually instead of \\(P(x, y)\\) we consider \\(\\log P(x, y)\\): \\[\\log P(x, y=k)=\\log P(y=k) + \\sum\\limits_{t=1}^n\\log P(x_t|y=k).\\] Since we care only about argmax, we can consider \\(\\log P(x, y)\\) instead of \\(P(x, y)\\). Important! Note that in practice, we will usually deal with log-probabilities and not probabilities. View in the General Framework Remember our general view on the classification task? We obtain feature representation of the",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "P(x_t|y=k).\\] Since we care only about argmax, we can consider \\(\\log P(x, y)\\) instead of \\(P(x, y)\\). Important! Note that in practice, we will usually deal with log-probabilities and not probabilities. View in the General Framework Remember our general view on the classification task? We obtain feature representation of the input text using some method, then use this feature representation for classification. In Naive Bayes, our features are words, and the feature representation is the Bag-of-Words (BOW) representation - a sum of one-hot representations of words. Indeed, to evaluate \\(P(x, y)\\) we only need to count the number of times each token appeared in the text. Feature Design In the standard setting, we used words as features. However, you can use other types of features: URL, user id, etc. Even if your data is a plain text (without fancy things such as URL, user id, etc), you can still design features in different ways. Learn how to improve Naive Bayes in this exercise in the Research Thinking section. Maximum Entropy Classifier (aka Logistic Regression) Differently from Naive Bayes, MaxEnt classifier is a discriminative model, i.e., we are interested in \\(P(y=k|x)\\) and not in the joint distribution \\(p(x, y)\\). Also, we will learn how to use features: this is in contrast to Naive Bayes, where we defined how to use the features ourselves. Here we also have to define features manually, but we have more freedom: features do not have to be categorical (in Naive Bayes, they had to!). We can use the BOW representation or come up with something more interesting. The general classification pipeline here is as follows: get \\(\\color{#7aab00}{h}\\color{black}=(\\color{#7aab00}{f_1}\\color{black}, \\color{#7aab00}{f_2}\\color{black}, \\dots, \\color{#7aab00}{f_n}\\color{black}{)}\\) - feature representation of the input text; take \\(w^{(i)}=(w_1^{(i)}, \\dots, w_n^{(i)})\\) - vectors with feature weights for each of the classes; for each class, weigh features, i.e. take the dot product of feature representation \\(\\color{#7aab00}{h}\\) with feature weights \\(w^{(k)}\\): \\[w^{(k)}\\color{#7aab00}{h}\\color{black} = w_1^{(k)}\\cdot\\color{#7aab00}{f_1}\\color{black}+\\dots+ w_n^{(k)}\\cdot\\color{#7aab00}{f_n}\\color{black}{, \\ \\ \\ \\ \\ k=1, \\dots, K.} \\] To get a bias term in the sum above, we define one of the features being 1 (e.g., \\(\\color{#7aab00}{f_0}=1\\)). Then \\[w^{(k)}\\color{#7aab00}{h}\\color{black} = \\color{red}{w_0^{(k)}}\\color{black} + w_1^{(k)}\\cdot\\color{#7aab00}{f_1}\\color{black}+\\dots+ w_n^{(k)}\\cdot\\color{#7aab00}{f_{n}}\\color{black}{, \\ \\ \\ \\ \\ k=1, \\dots, K.} \\] get class probabilities using softmax: \\[P(class=k|\\color{#7aab00}{h}\\color{black})= \\frac{\\exp(w^{(k)}\\color{#7aab00}{h}\\color{black})}{\\sum\\limits_{i=1}^K \\exp(w^{(i)}\\color{#7aab00}{h}\\color{black})}.\\] Softmax normalizes the \\(K\\) values we got at the previous step to a probability distribution over output classes. Look at the illustration below (classes are shown in different colors). Training: Maximum Likelihood Estimate Given training examples \\(x^1, \\dots, x^N\\) with labels \\(y^1, \\dots, y^N\\), \\(y^i\\in\\{1, \\dots, K\\}\\), we pick those weights \\(w^{(k)}, k=1..K\\) which maximize the probability of the training data: \\[w^{\\ast}=\\arg \\max\\limits_{w}\\sum\\limits_{i=1}^N\\log P(y=y^i|x^i).\\] In other words, we choose parameters such that the data is more likely to appear. Therefore, this is called the Maximum Likelihood Estimate (MLE) of the parameters. To find the parameters maximizing the data log-likelihood, we use gradient ascent: gradually improve weights during multiple iterations over the data. At each step, we maximize the probability a model assigns to the correct class. Equvalence to minimizing cross-entropy Note that maximizing data log-likelihood is equivalent to minimizing cross entropy between the",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "the parameters maximizing the data log-likelihood, we use gradient ascent: gradually improve weights during multiple iterations over the data. At each step, we maximize the probability a model assigns to the correct class. Equvalence to minimizing cross-entropy Note that maximizing data log-likelihood is equivalent to minimizing cross entropy between the target probability distribution \\(p^{\\ast} = (0, \\dots, 0, 1, 0, \\dots)\\) (1 for the target label, 0 for the rest) and the predicted by the model distribution \\(p=(p_1, \\dots, p_K), p_i=p(i|x)\\): \\[Loss(p^{\\ast}, p^{})= - p^{\\ast} \\log(p) = -\\sum\\limits_{i=1}^{K}p_i^{\\ast} \\log(p_i).\\] Since only one of \\(p_i^{\\ast}\\) is non-zero (1 for the target label \\(k\\), 0 for the rest), we will get \\(Loss(p^{\\ast}, p) = -\\log(p_{k})=-\\log(p(k| x)).\\) This equivalence is very important for you to understand: when talking about neural approaches, people usually say that they minimize the cross-entropy loss. Do not forget that this is the same as maximizing the data log-likelihood. Naive Bayes vs Logistic Regression Let's finalize this part by discussing the advantages and drawbacks of logistic regression and Naive Bayes. simplicity Both methods are simple; Naive Bayes is the simplest one. interpretability Both methods are interpretable: you can look at the features which influenced the predictions most (in Naive Bayes - usually words, in logistic regression - whatever you defined). training speed Naive Bayes is very fast to train - it requires only one pass through the training data to evaluate the counts. For logistic regression, this is not the case: you have to go over the data many times until the gradient ascent converges. independence assumptions Naive Bayes is too \"naive\" - it assumed that features (words) are conditionally independent given class. Logistic regression does not make this assumption - we can hope it is better. text representation: manual Both methods use manually defined feature representation (in Naive Bayes, BOW is the standard choice, but you still choose this yourself). While manually defined features are good for interpretability, they may be no so good for performance - you are likely to miss something which can be useful for the task. SVM for Text Classification One more method for text classification based on manually designed features is SVM. The most basic (and popular) features for SVMs are bag-of-words and bag-of-ngrams ( ngram is a tuple of n words). With these simple features, SVMs with linear kernel perform better than Naive Bayes (see, for example, the paper Question Classification using Support Vector Machines ). Text Classification with Neural Networks Instead of manually defined features, let a neural network to learn useful features . The main idea of neural-network-based classification is that feature representation of the input text can be obtained using a neural network. In this setting, we feed the embeddings of the input tokens to a neural network, and this neural network gives us a vector representation of the input text. After that, this vector is used for classification. When dealing with neural networks, we can think about the classification part (i.e., how to get class probabilities from a vector representation of a text) in a very simple way. Vector representation",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "network gives us a vector representation of the input text. After that, this vector is used for classification. When dealing with neural networks, we can think about the classification part (i.e., how to get class probabilities from a vector representation of a text) in a very simple way. Vector representation of a text has some dimensionality \\(d\\), but in the end, we need a vector of size \\(K\\) (probabilities for \\(K\\) classes). To get a \\(K\\)-sized vector from a \\(d\\)-sized, we can use a linear layer. Once we have a \\(K\\)-sized vector, all is left is to apply the softmax operation to convert the raw numbers into class probabilities. Classification Part: This is Logistic Regression! Let us look closer to the neural network classifier. The way we use vector representation of the input text is exactly the same as we did with logistic regression: we weigh features according to feature weights for each class. The only difference from logistic regression is where the features come from: they are either defined manually (as we did before) or obtained by a neural network. Intuition: Text Representation Points in the Direction of Class Representation If we look at this final linear layer more closely, we will see that the columns of its matrix are vectors \\(w_i\\). These vectors can be thought of as vector representations of classes. A good neural network will learn to represent input texts in such a way that text vectors will point in the direction of the corresponding class vectors. Training and the Cross-Entropy Loss Neural classifiers are trained to predict probability distributions over classes. Intuitively, at each step we maximize the probability a model assigns to the correct class. The standard loss function is the cross-entropy loss . Cross-entropy loss for the target probability distribution \\(p^{\\ast} = (0, \\dots, 0, 1, 0, \\dots)\\) (1 for the target label, 0 for the rest) and the predicted by the model distribution \\(p=(p_1, \\dots, p_K), p_i=p(i|x)\\): \\[Loss(p^{\\ast}, p^{})= - p^{\\ast} \\log(p) = -\\sum\\limits_{i=1}^{K}p_i^{\\ast} \\log(p_i).\\] Since only one of \\(p_i^{\\ast}\\) is non-zero (1 for the target label \\(k\\), 0 for the rest), we will get \\(Loss(p^{\\ast}, p) = -\\log(p_{k})=-\\log(p(k| x)).\\) Look at the illustration for one training example. In training, we gradually improve model weights during multiple iterations over the data: we iterate over training examples (or batches of examples) and make gradient updates. At each step, we maximize the probability a model assigns to the correct class. At the same time, we minimize sum of the probabilities of incorrect classes: since sum of all probabilities is constant, by increasing one probability we decrease sum of all the rest ( Lena : Here I usually imagine a bunch of kittens eating from the same bowl: one kitten always eats at the expense of the others ). Lena : Here I usually imagine a bunch of kittens eating from the same bowl: one kitten always eats at the expense of the others Look at the illustration of the training process. Recap: This is equivalent to maximizing the data likelihood Do not forget that when talking",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "others ). Lena : Here I usually imagine a bunch of kittens eating from the same bowl: one kitten always eats at the expense of the others Look at the illustration of the training process. Recap: This is equivalent to maximizing the data likelihood Do not forget that when talking about MaxEnt classifier (logistic regression) , we showed that minimizing cross-entropy is equivalent to maximizing the data likelihood. Therefore, here we are also trying to get the Maximum Likelihood Estimate (MLE) of model parameters. Models for Text Classification We need a model that can produce a fixed-sized vector for inputs of different lengths. In this part, we will look at different ways to get a vector representation of an input text using neural networks. Note that while input texts can have different lengths, the vector representation of a text has to have a fixed size: otherwise, a network will not \"work\". We begin with the simplest approaches which use only word embeddings (without adding a model on top of that). Then we look at recurrent and convolutional networks. Lena: A bit later in the course, you will learn about Transformers and the most recent classification techniques using large pretrained models. Basics: Bag of Embeddings (BOE) and Weighted BOE The simplest you can do is use only word embeddings without any neural network on top of that. To get vector representation of a text, we can either sum all token embeddings (Bag of Embeddings) or use a weighted sum of these embeddings (with weights, for example, being tf-idf or something else). Bag of Embeddings (ideally, along with Naive Bayes) should be a baseline for any model with a neural network: if you can't do better than that, it's not worth using NNs at all. This can be the case if you don't have much data. While Bag of Embeddings (BOE) is sometimes called Bag of Words (BOW), note that these two are very different . BOE is the sum of embeddings and BOW is the sum of one-hot vectors: BOE knows a lot more about language. The pretrained embeddings (e.g., Word2Vec or GloVe) understand similarity between words. For example, awesome , brilliant , great will be represented with unrelated features in BOW but similar word vectors in BOE. awesome brilliant great Note also that to use a weighted sum of embeddings, you need to come up with a way to get weights. However, this is exactly what we wanted to avoid by using neural networks: we don't want to introduce manual features, but rather let a network to learn useful patterns. Bag of Embeddings as Features for SVM You can use SVM on top of BOE! The only difference from SVMs in classical approaches (on top of bag-of-words and bag-of-ngrams) if the choice of a kernel: here the RBF kernel is better. Models: Recurrent (RNN/LSTM/etc) Recurrent networks are a natural way to process text in a sense that, similar to humans, they \"read\" a sequence of tokens one by one and process the information. Hopefully, at each step the network will \"remember\" everything",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "of a kernel: here the RBF kernel is better. Models: Recurrent (RNN/LSTM/etc) Recurrent networks are a natural way to process text in a sense that, similar to humans, they \"read\" a sequence of tokens one by one and process the information. Hopefully, at each step the network will \"remember\" everything it has read before. Basics: Recurrent Neural Networks • RNN cell • At each step, a recurrent network receives a new input vector (e.g., token embedding) and the previous network state (which, hopefully, encodes all previous information). Using this input, the RNN cell computes the new state which it gives as output. This new state now contains information about both current input and the information from previous steps. • RNN reads a sequence of tokens • Look at the illustration: RNN reads a text token by token, at each step using a new token embedding and the previous state. Note that the RNN cell is the same at each step! • Vanilla RNN • The simplest recurrent network, Vanilla RNN , transforms \\(h_{t-1}\\) and \\(x_t\\) linearly, then applies a non-linearity (most often, the \\(\\tanh\\) function): \\[h_t = \\tanh(h_{t-1}W_h + x_tW_t).\\] Vanilla RNNs suffer from the vanishing and exploding gradients problem. To alleviate this problem, more complex recurrent cells (e.g., LSTM, GRU, etc) perform several operations on the input and use gates. For more details of RNN basics, look at the Colah's blog post . Recurrent Neural Networks for Text Classification Here we (finally!) look at how we can use recurrent models for text classification. Everything you will see here will apply to all recurrent cells, and by \"RNN\" in this part I refer to recurrent cells in general (e.g. vanilla RNN, LSTM, GRU, etc). Let us recall what we need: We need a model that can produce a fixed-sized vector for inputs of different lengths. • Simple : read a text, take the final state • The most simple recurrent model is a one-layer RNN network. In this network, we have to take the state which knows more about input text. Therefore, we have to use the last state - only this state saw all input tokens. • Multiple layers : feed the states from one RNN to the next one • To get a better text representation, you can stack multiple layers. In this case, inputs for the higher RNN are representations coming from the previous layer. The main hypothesis is that with several layers, lower layers will catch local phenomena (e.g., phrases), while higher layers will be able to learn more high-level things (e.g., topic). • Bidirectional : use final states from forward and backward RNNs. • Previous approaches may have a problem: the last state can easily \"forget\" earlier tokens. Even strong models such as LSTMs can still suffer from that! To avoid this, we can use two RNNs: forward , which reads input from left to right, and backward , which reads input from right to left. Then we can use the final states from both models: one will better remember the final part of a text, another",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "from that! To avoid this, we can use two RNNs: forward , which reads input from left to right, and backward , which reads input from right to left. Then we can use the final states from both models: one will better remember the final part of a text, another - the beginning. These states can be concatenated, or summed, or something else - it's your choice! • Combinations : do everything you want! • You can combine the ideas above. For example, in a multi-layered network, some layers can go in the opposite direction, etc. Models: Convolutional (CNN) The detailed description of convolutional models in general is in Convolutional Models Supplementary . In this part, we consider only convolutions for text classification. Convolutions for Images and Translation Invariance Convolutional networks were originally developed for computer vision tasks. Therefore, let's first understand the intuition behind convolutional models for images. Imagine we want to classify an image into several classes, e.g. cat, dog, airplane, etc. In this case, if you find a cat on an image, you don't care where on the image this cat is: you care only that it is there somewhere. Convolutional networks apply the same operation to small parts of an image: this is how they extract features. Each operation is looking for a match with a pattern, and a network learns which patterns are useful. With a lot of layers, the learned patterns become more and more complicated: from lines in the early layers to very complicated patterns (e.g., the whole cat or dog) on the upper ones. You can look at the examples in the Analysis and Interpretability section. This property is called translation invariance : translation because we are talking about shifts in space, invariance because we want it to not matter. The illustration is adapted from the one taken from this cool repo . The illustration is adapted from the one taken from this cool repo . Convolutions for Text Well, for images it's all clear: e.g. we want to be able to move a cat because we don't care where the cat is. But what about texts? At first glance, this is not so straightforward: we can not move phrases easily - the meaning will change or we will get something that does not make much sense. However, there are some applications where we can think of the same intuition. Let's imagine that we want to classify texts, but not cats/dogs as in images, but positive/negative sentiment. Then there are some words and phrases which could be very informative \"clues\" (e.g. it's been great , bored to death , absolutely amazing , the best ever , etc), and others which are not important at all. We don't care much where in a text we saw bored to death to understand the sentiment, right? A Typical Model: Convolution+Pooling Blocks Following the intuition above, we want to detect some patterns, but we don't care much where exactly these patterns are. This behavior is implemented with two layers: convolution : finds matches with patterns (as the cat",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "saw bored to death to understand the sentiment, right? A Typical Model: Convolution+Pooling Blocks Following the intuition above, we want to detect some patterns, but we don't care much where exactly these patterns are. This behavior is implemented with two layers: convolution : finds matches with patterns (as the cat head we saw above); pooling : aggregates these matches over positions (either locally or globally). A typical convolutional model for text classification is shown on the figure. To get a vector representation of an input text, a convolutional layer is applied to word embedding, which is followed by a non-linearity (usually ReLU) and a pooling operation. The way this representation is used for classification is similar to other networks. In the following, we discuss in detail the main building blocks, convolution and pooling, then consider modeling modifications. Basics: Convolution Layer for Text Convolutional Neural Networks were initially developed for computer vision tasks, e.g. classification of images (cats vs dogs, etc). The idea of a convolution is to go over an image with a sliding window and to apply the same operation, convolution filter , to each window. The illustration (taken from this cool repo ) shows this process for one filter: the bottom is the input image, the top is the filter output. Since an image has two dimensions (width and height), the convolution is two-dimensional. Convolution filter for images. The illustration is from this cool repo . Convolution filter for images. The illustration is from this cool repo . Differently from images, texts have only one dimension: here a convolution is one-dimensional: look at the illustration. Convolution filter for text. Convolution is a Linear Operation Applied to Each Window A convolution is a linear layer (followed by a non-linearity) applied to each input window. Formally, let us assume that \\((x_1, \\dots, x_n)\\) - representations of the input words, \\(x_i\\in \\mathbb{R}^d\\); \\(d\\) ( input channels ) - size of an input embedding; \\(k\\) ( kernel size ) - the length of a convolution window (on the illustration, \\(k=3\\)); \\(m\\) ( output channels ) - number of convolution filters (i.e., number of channels produced by the convolution). Then a convolution is a linear layer \\(W\\in\\mathbb{R}^{(k\\cdot d)\\times m}\\). For a \\(k\\)-sized window \\((x_i, \\dots x_{i+k-1})\\), the convolution takes the concatenation of these vectors \\[u_i = [x_i, \\dots x_{i+k-1}]\\in\\mathbb{R}^{k\\cdot d}\\] and multiplies by the convolution matrix: \\[F_i = u_i \\times W.\\] A convolution goes over an input with a sliding window and applies the same linear transformation to each window. Intuition : Each Filter Extracts a Feature Intuitively, each filter in a convolution extracts a feature. • One filter - one feature extractor • A filter takes vector representations in a current window and transforms them linearly into a single feature. Formally, for a window \\(u_i = [x_i, \\dots x_{i+k-1}]\\in\\mathbb{R}^{k\\cdot d}\\) a filter \\(f\\in\\mathbb{R}^{k\\cdot d}\\) computes dot product: \\[F_i^{(f)} = (f, u_i).\\] The number \\(F_i^{(f)}\\) (the extracted \"feature\") is a result of applying the filter \\(f\\) to the window \\((x_i, \\dots x_{i+k-1})\\). • m filters : m feature extractors • One filter extracts a single",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "\\(u_i = [x_i, \\dots x_{i+k-1}]\\in\\mathbb{R}^{k\\cdot d}\\) a filter \\(f\\in\\mathbb{R}^{k\\cdot d}\\) computes dot product: \\[F_i^{(f)} = (f, u_i).\\] The number \\(F_i^{(f)}\\) (the extracted \"feature\") is a result of applying the filter \\(f\\) to the window \\((x_i, \\dots x_{i+k-1})\\). • m filters : m feature extractors • One filter extracts a single feature. Usually, we want many features: for this, we have to take several filters. Each filter reads an input text and extracts a different feature - look at the illustration. The number of filters is the number of output features you want to get. With \\(m\\) filters instead of one, the size of the convolutional layer we discussed above will become \\((k\\cdot d)\\times m\\). This is done in parallel! Note that while I show you how a CNN \"reads\" a text, in practice these computations are done in parallel. Basics: Pooling Operation After a convolution extracted \\(m\\) features from each window, a pooling layer summarises the features in some region. Pooling layers are used to reduce the input dimension, and, therefore, to reduce the number of parameters used by the network. • Max and Mean Pooling • The most popular is max-pooling : it takes maximum over each dimension, i.e. takes the maximum value of each feature. Intuitively, each feature \"fires\" when it sees some pattern: a visual pattern in an image (line, texture, a cat's paw, etc) or a text pattern (e.g., a phrase). After a pooling operation, we have a vector saying which of these patterns occurred in the input. Mean-pooling works similarly but computes mean over each feature instead of maximum. • Pooling and Global Pooling • Similarly to convolution, pooling is applied to windows of several elements. Pooling also has the stride parameter, and the most common approach is to use pooling with non-overlapping windows. For this, you have to set the stride parameter the same as the pool size. Look at the illustration. The difference between pooling and global pooling is that pooling is applied over features in each window independently, while global pooling performs over the whole input. For texts, global pooling is often used to get a single vector representing the whole text; such global pooling is called max-over-time pooling , where the \"time\" axis goes from the first input token to the last. Convolutional Neural Networks for Text Classification Now, when we understand how the convolution and pooling work, let's come to modeling modifications. First, let us recall what we need: We need a model that can produce a fixed-sized vector for inputs of different lengths. Therefore, we need to construct a convolutional model that represents a text as a single vector. The basic convolutional model for text classification is shown on the figure. It is almost the same as we saw before: the only thing that's changed is that we specified the type of pooling used. Specifically, after the convolution, we use global-over-time pooling . This is the key operation: it allows to compress a text into a single vector. The model itself can be different, but at some point it has to",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "only thing that's changed is that we specified the type of pooling used. Specifically, after the convolution, we use global-over-time pooling . This is the key operation: it allows to compress a text into a single vector. The model itself can be different, but at some point it has to use the global pooling to compress input in a single vector. • Several Convolutions with Different Kernel Sizes • Instead of picking one kernel size for your convolution, you can use several convolutions with different kernel sizes. The recipe is simple: apply each convolution to the data, add non-linearity and global pooling after each of them, then concatenate the results (on the illustration, non-linearity is omitted for simplicity). This is how you get vector representation of the data which is used for classification. This idea was used, among others, in the paper Convolutional Neural Networks for Sentence Classification and many follow-ups. • Stack Several Blocks Convolution+Pooling • Instead of one layer, you can stack several blocks convolution+pooling on top of each other. After several blocks, you can apply another convolution, but with global pooling this time. Remember: you have to get a single fixed-sized vector - for this, you need global pooling. Such multi-layered convolutions can be useful when your texts are very long; for example, if your model is character-level (as opposed to word-level). This idea was used, among others, in the paper Character-level Convolutional Networks for Text Classification . Multi-Label Classification Multi-label classification: many labels, several can be correct Multi-label classification: many labels, several can be correct Multi-label classification is different from the single-label problems we discussed before in that each input can have several correct labels. For example, a twit can have several hashtags, a user can have several topics of interest, etc. For a multi-label problem, we need to change two things in the single-label pipeline we discussed before: model (how we evaluate class probabilities); loss function . Model: Softmax → Element-wise Sigmoid After the last linear layer, we have \\(K\\) values corresponding to the \\(K\\) classes - these are the values we have to convert to class probabilities. For single-label problems, we used softmax: it converts \\(K\\) values into a probability distribution, i.e. sum of all probabilities is 1. It means that the classes share the same probability mass: if the probability of one class is high, other classes can not have large probability ( Lena : Once again, imagine a bunch of kittens eating from the same bowl: one kitten always eats at the expense of the others ). Lena : Once again, imagine a bunch of kittens eating from the same bowl: one kitten always eats at the expense of the others For multi-label problems, we convert each of the \\(K\\) values into a probability of the corresponding class independently from the others. Specifically, we apply the sigmoid function \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\) to each of the \\(K\\) values. Intuitively, we can think of this as having \\(K\\) independent binary classifiers that use the same text representation. Loss Function: Binary Cross-Entropy for Each Class Loss function changes to",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "of the corresponding class independently from the others. Specifically, we apply the sigmoid function \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\) to each of the \\(K\\) values. Intuitively, we can think of this as having \\(K\\) independent binary classifiers that use the same text representation. Loss Function: Binary Cross-Entropy for Each Class Loss function changes to enable multiple labels: for each class, we use the binary cross-entropy loss. Look at the illustration. Practical Tips Word Embeddings: how to deal with them? Input for a network is represented by word embeddings. You have three options how to get these embeddings for your model: train from scratch as part of your model, take pretrained (Word2Vec, GloVe, etc) and fix them (use them as static vectors), initialize with pretrained embeddings and train them with the network (\"fine-tune\"). Let's think about these options by looking at the data a model can use. Training data for classification is labeled and task-specific, but labeled data is usually hard to get. Therefore, this corpus is likely to be not huge (at the very least), or not diverse, or both. On the contrary, training data for word embeddings is not labeled - plain texts are enough. Therefore, these datasets can be huge and diverse - a lot to learn from. Now let us think what a model will know depending on what we do with the embeddings. If the embeddings are trained from scratch, the model will \"know\" only the classification data - this may not be enough to learn relationships between words well. But if we use pretrained embeddings, they (and, therefore, the whole model) will know a huge corpus - they will learn a lot about the world. To adapt these embeddings to your task-specific data, you can fine-tune these embeddings by training them with the whole network - this can bring gains in the performance (not huge though). When we use pretrained embeddings, this is an example of transfer learning : through the embeddings, we \"transfer\" the knowledge of their training data to our task-specific model. We will learn more about transfer learning later in the course. Fine-tune pretrained embeddings or not? Before training models, you can first think why fine-tuning can be useful, and which types of examples can benefit from it. Learn more from this exercise in the Research Thinking section. For more details and the experiments with different settings for word embeddings, look at this paper summary. Data Augmentation: Get More Data for Free Data augmentation alters your dataset in different ways to get alternative versions of the same training example. Data augmentation can increase the amount of data Quality of your model depends a lot on your data. For deep learning models, having large datasets is very (very!) important. diversity of data By giving different versions of training examples, you teach a model to be more robust to real-world data which can be of lower quality or simply a bit different from your training data. With augmented data, a model is less likely to overfit to specific types of training examples and will rely more on general patterns. Data",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "you teach a model to be more robust to real-world data which can be of lower quality or simply a bit different from your training data. With augmented data, a model is less likely to overfit to specific types of training examples and will rely more on general patterns. Data augmentation for images can be done easily: look at the examples below. The standard augmentations include flipping an image, geometrical transformations (e.g. rotation and stretching along some direction), covering parts of an image with different patches. How can we do something similar for texts? • word dropout - the most simple and popular • Word dropout is the simplest regularization: for each example, you choose some words randomly (say, each word is chosen with probability 10%) and replace the chosen words with either the special token UNK or with a random token from the vocabulary. The motivation here is simple: we teach a model not to over-rely on individual tokens, but take into consideration context of the whole text. For example, here we masked great , and a model has to understand the sentiment based on other words. Note: For images, this corresponds to masking out some areas. By masking out an area of an image, we also want a model not to over-rely on local features and to make use of a more global context. • use external resources (e.g., thesaurus) - a bit more complicated • A bit more complicated approach is to replace words or phrases with their synonyms. The tricky part is getting these synonyms: you need external resources, and they are rarely available for languages other than English (for English, you can use e.g. WordNet). Another problem is that for languages with rich morphology (e.g., Russian) you are likely to violate the grammatical agreement. • use separate models - even more complicated • An even more complicated method is to paraphrase the whole sentences using external models. A popular paraphrasing method is to translate a sentence to some language and back. We will learn how to train a translation model a bit later (in the Seq2seq and Attention lecture), but for now, you can use industrial systems, e.g. Yandex Translate , Google Translate , etc. ( Lena: Obviously, personally I'm biased towards Yandex :) ) Note that you can combine translation systems and languages to get several paraphrases. Lena: Obviously, personally I'm biased towards Yandex :) Note: For images, the last two techniques correspond to geometric transformations: we want to change text, but to preserve the meaning. This is different from word dropout, where some parts are lost completely. Analysis and Interpretability What do Convolutions Learn? Analyzing Convolutional Filters Convolutions in Computer Vision: Visual Patterns Convolutions were originally developed for images, and there's already a pretty good understanding of what the filters capture and how filters from different layers from a hierarchy. While lower layers capture simple visual patterns such as lines or circles, final layers can capture the whole pictures, animals, people, etc. Examples of patterns captured by convolution filters for images. The examples are from",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "understanding of what the filters capture and how filters from different layers from a hierarchy. While lower layers capture simple visual patterns such as lines or circles, final layers can capture the whole pictures, animals, people, etc. Examples of patterns captured by convolution filters for images. The examples are from Activation Atlas from distill.pub . Examples of patterns captured by convolution filters for images. The examples are from Activation Atlas from distill.pub . What About Convolutions in Texts? This part is based on the paper Understanding Convolutional Neural Networks for Text Classification . For images, filters capture local visual patterns which are important for classification. For text, such local patterns are word n-grams. The main findings on how CNNs work for texts are: convolving filters are used as ngram detectors Each filter specializes in one or several families of closely-related ngrams. Filters are not homogeneous, i.e. a single filter can, and often does, detect multiple distinctly different families of ngrams. max-pooling induces a thresholding behavior Values below a given threshold are ignored when (i.e. irrelevant to) making a prediction. For example, this paper shows that 40% of the pooled ngrams on average can be dropped with no loss of performance. The simplest way to understand what a network captures is to look which patterns activate its neurons. For convolutions, we pick a filter and find those n-grams which activate this filter most. Below are examples of the top-1 n-gram for several filters. For one of them, we also show other n-grams which lead to high activation of this filter - you can see that the n-grams have a very similar meaning. For more details, look at the paper Understanding Convolutional Neural Networks for Text Classification . How About RNN CLassifiers? How RNNs trained for classification process text? Learn here . Research Thinking How to Read the short description at the beginning - this is our starting point, something known. Read a question and think: for a minute, a day, a week, ... - give yourself some time! Even if you are not thinking about it constantly, something can still come to mind. Look at the possible answers - previous attempts to answer/solve this problem. Important: You are not supposed to come up with something exactly like here - remember, each paper usually takes the authors several months of work. It's a habit of thinking about these things that counts! All the rest a scientist needs is time: to try-fail-think until it works. It's well-known that you will learn something easier if you are not just given the answer right away, but if you think about it first. Even if you don't want to be a researcher, this is still a good way to learn things! Classical Approaches ? Idea: Add Frequent N-Grams to the Features! Instead of using only words as features, we can also use word n-grams. Since using all n-grams would be inefficient, we can add only the frequent ones. This can fix some examples with simple negation as the one shown above. ? Note that Naive Bayes can use",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "to the Features! Instead of using only words as features, we can also use word n-grams. Since using all n-grams would be inefficient, we can add only the frequent ones. This can fix some examples with simple negation as the one shown above. ? Note that Naive Bayes can use any categorical features - you can do anything as long as you can compute the counts for probabilities. For example, text length Who knows - maybe positive reviews are longer than negative? Don't forget to categorize it, e.g., 1-20 tokens correspond to one feature, 21-30 tokens - to another, etc. token frequency It may be worth checking - positive or negative reviews use more peculiar words? You can come up with some characteristics of tokens frequency: minimum of maximum, average, etc. But again - you have to categorize it! syntactical features (if you don't know what it is yet, skip this) Dependency tree depth (maximum/minimum/average) - this can be a proxy of text complexity. anything else you can come up with Just try :) ? Idea: Do Not Use Unimportant Words If you know which words definitely do not influence class probability, you can remove them from the features! For example, we can remove stop-words: determiners, prepositions, etc. Note: you need to be really careful - don't remove something useful! Neural Approaches ? Without fine-tuning, closest to bad is good ! The figure shows closest neighbors of Word2Vec embeddings before and after fine-tuning (examples are taken from this paper ). Without fine-tuning, closest to bad is good ! Without fine-tuning, it would be very hard for a model to separate positive and negative using these embeddings. This is only one example of antonyms with close embeddings which can hurt sentiment classification. Fine-tuning can also help to improve understanding of tokens such as n't : rare in the corpus word embeddings were trained on, but not rare in the corpus we care about. More generally, if your task-specific domain is different from the word embeddings training data, fine-tuning is a good idea. Here will be more exercises! This part will be expanding from time to time. Related Papers How to High-level : look at key results in short summaries - get an idea of what's going on in the field. A bit deeper : for topics which interest you more, read longer summaries with illustrations and explanations. Take a walk through the authors' reasoning steps and key observations. In depth : read the papers you liked. Now, when you got the main idea, this is going to be easier! What's inside: Convolutions for Classification: Classics Analyzing RNNs for Sentiment Classification ... to be updated Convolutions for Classification: Classics Even a very simple CNN with one layer on top of word embeddings shows very good performance (without features requiring some external knowledge!). The paper also shows the importance of using pretrained embeddings (and not training from scratch) and gains from fine-tuning. The CNN model is shown on the figure: on top of embeddings, it has three convolutions with max-over-time pooling (in parallel). The results",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "good performance (without features requiring some external knowledge!). The paper also shows the importance of using pretrained embeddings (and not training from scratch) and gains from fine-tuning. The CNN model is shown on the figure: on top of embeddings, it has three convolutions with max-over-time pooling (in parallel). The results are concatenated and used for classification. This is a very simple model we discussed earlier . The paper has a lot of results (comparison with 14 baselines!), but here I'll mention only the ones comparing the same CNN model with different strategies of obtaining word embeddings. Embeddings: Random vs Pretrained (Word2Vec) In the table below random experiment - embeddings are randomly initialized and trained with the model, pretrained - the embeddings are initialized with Word2Vec and fixed (not trained with the model). We see that using pretrained embeddings is better by several percentage points! This happens because in the first case the embeddings see only the classification training data, which is usually not much. But trained embeddings saw a lot of other data - they know a lot more about the world and relationships between words. Important! This is called transfer learning - by using trained embeddings, you \"transfer\" knowledge contained in the embeddings training data to your model. We will learn more about this later in the course. Pretrained Embeddings: Fixed vs Fine-tuned In the table below fixed experiment - the embeddings are initialized with Word2Vec and fixed (not trained with the model), fine-tuned - the embeddings are initialized with Word2Vec and then trained with the classification model. We see that if we fine-tune pretrained embeddings for the specific task, we are likely to get further improvement. But you need to be careful - embeddings can \"forget\" what they learned before. More in the paper comparison with lots of baselines This simple model performs quite well compared to lots of baselines (including SVMs we discussed above). both fixed and fine-tuned embeddings The paper proposes a way to use both fixed embeddings and fine-tuned: it duplicates the embedding layer and keeps one of them fixed while trains the other. The results are mixed - look in the paper! This is the first paper showing that CNNs only on characters can do quite well. This is interesting: classification can be done without any external knowledge, even without text segmentation into words! An important point is that character-level CNNs can do better than classical approaches only for large datasets. The CNN model is shown in the figure above. It has 6 convolution+pooling blocks (note that the last pooling is global) followed by 3 linear layers (with non-linearities after each convolution and linear layers). Character-level CNN is better for large datasets The figure below shows the relative errors with respect to comparison models. Each of these plots is computed by taking the difference between errors on a comparison model and the character-level CNN model, then divided by the comparison model error. Here we show only 2 of the comparisons: with a classical approach (n-grams tf-idf) and a word-level CNN. The results show that for large",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "these plots is computed by taking the difference between errors on a comparison model and the character-level CNN model, then divided by the comparison model error. Here we show only 2 of the comparisons: with a classical approach (n-grams tf-idf) and a word-level CNN. The results show that for large datasets, character-level CNN performs better. Other Results choice of alphabet matters You can take either only lowercased data, or distinguish between uppercase and lowercase - the results can be different! char-CNNs work better for user-generated data. Compared to word-level models, character-level ones may be better suitable for raw user data with misspellings. For more details, look at the paper ! Analyzing RNNs for Sentiment Classification If we take an RNN trained for sentiment analysis and apply PCA to lots of its states, we'll see that almost all variance is explained with only two components. Moreover, when such RNN reads a text, its states move along a 1D plane in either negative or positive direction depending on the word it reads. The paper looks at four different RNN types (vanilla, LSTM, GRU, and the Update Gate RNN ) , as well as different sentiment classification datasets (IMDb movie review, Yelp review, SST-2). The results are similar for all combinations. PCA: Most of the Variance is Captured by a Few Dimensions The authors took 1000 test examples, fed it to LSTM, took all states, and applied PCA. Turns out, all variance is explained by only a couple of components! Note that this is true only for a trained model - for untrained one, this is not the case. Look at the figure. The 1D Plane, Trajectories and Word Influences In the figure above (the red/green one) are the RNN states projected onto the first two PCA components; the states are colored according to the target label. We see that the states lie along a one-dimensional plane corresponding to the sentiment changing from one to another. The figure also shows examples of RNN trajectories when reading a positive or a negative text. The further the model is in the text, the deeper its state goes in the corresponding area. What is more interesting, the authors also looked at how each token influences the RNN state. As expected, positive and negative words usually move the states in the corresponding area, while neutral words do not have such influence. More in the paper linear approximations of RNN dynamics The authors apply a linearization procedure to obtain an approximate, but highly interpretable, description of the RNN dynamics. RNNs count the number of positive and negative words used With lots of math, the authors conclude that in nearly all cases the key activity performed by the RNN for sentiment analysis is simply counting the number of positive and negative words used. Here will be more papers! The papers will be gradually appearing. Have Fun! Coming soon! We are still working on this!",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Text Classification",
        "url": "https://lena-voita.github.io/nlp_course/text_classification.html",
        "chunk": {
            "text": "words used. Here will be more papers! The papers will be gradually appearing. Have Fun! Coming soon! We are still working on this!",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "The way machine learning models \" see \" data is different from how we (humans) do. For example, we can easily understand the text \"I saw a cat\" , but our models can not - they need vectors of features. Such vectors, or word embeddings , are representations of words which can be fed into your model. \"I saw a cat\" How it works: Look-up Table (Vocabulary) In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance. For each vocabulary word, a look-up table contains its embedding. This embedding can be found using the word index in the vocabulary (i.e., you to look up the embedding in the table using word index). To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary contains a special token UNK . Alternatively, unknown tokens can be ignored or assigned a zero vector. UNK The main question of this lecture is: how do we get these word vectors? Represent as Discrete Symbols: One-hot Vectors The easiest you can do is to represent words as one-hot vectors: for the i-th word in the vocabulary, the vector has 1 on the i-th dimension and 0 on the rest. In Machine Learning, this is the most simple way to represent categorical features. You probably can guess why one-hot vectors are not the best way to represent words. One of the problems is that for large vocabularies, these vectors will be very long: vector dimensionality is equal to the vocabulary size. This is undesirable in practice, but this problem is not the most crucial one. What is really important, is that these vectors know nothing about the words they represent. For example, one-hot vectors \"think\" that cat is as close to dog as it is to table! We can say that one-hot vectors do not capture meaning. cat dog table! But how do we know what is meaning? Distributional Semantics To capture meaning of words in their vectors, we first need to define the notion of meaning that can be used in practice. For this, let us try to understand how we, humans, get to know which words have similar meaning. How to: go over the slides at your pace. Try to notice how your brain works. tezgüino Once you saw how the unknown word used in different contexts, you were able to understand it's meaning. How did you do this? The hypothesis is that your brain searched for other words that can be used in the same contexts, found some (e.g., wine ), and made a conclusion that tezgüino has meaning similar to those other words. This is the distributional hypothesis: wine tezgüino Words which frequently appear in similar contexts have similar meaning . Lena: Often you can find it formulated as \"You shall know a word by the company it keeps\" with the reference to J. R. Firth in 1957, but actually there were a lot more people responsible, and much earlier. For example, Harris, 1954. This is an extremely valuable idea: it can be used in",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "find it formulated as \"You shall know a word by the company it keeps\" with the reference to J. R. Firth in 1957, but actually there were a lot more people responsible, and much earlier. For example, Harris, 1954. This is an extremely valuable idea: it can be used in practice to make word vectors capture their meaning. According to the distributional hypothesis, \"to capture meaning\" and \"to capture contexts\" are inherently the same. Therefore, all we need to do is to put information about word contexts into word representation. Main idea : We need to put information about word contexts into word representation. All we'll be doing at this lecture is looking at different ways to do this. Count-Based Methods Let's remember our main idea: Main idea : We have to put information about contexts into word vectors. Count-based methods take this idea quite literally: How : Put this information manually based on global corpus statistics. The general procedure is illustrated above and consists of the two steps: (1) construct a word-context matrix, (2) reduce its dimensionality. There are two reasons to reduce dimensionality. First, a raw matrix is very large. Second, since a lot of words appear in only a few of possible contexts, this matrix potentially has a lot of uninformative elements (e.g., zeros). To estimate similarity between words/contexts, usually you need to evaluate the dot-product of normalized word/context vectors (i.e., cosine similarity). To define a count-based method, we need to define two things: possible contexts (including what does it mean that a word appears in a context), the notion of association, i.e., formulas for computing matrix elements. Below we provide a couple of popular ways of doing this. Simple: Co-Occurence Counts The simplest approach is to define contexts as each word in an L-sized window. Matrix element for a word-context pair (w, c) is the number of times w appears in context c. This is the very basic (and very, very old) method for obtaining embeddings. The (once) famous HAL model (1996) is also a modification of this approach. Learn more from this exercise in the Research Thinking section. Positive Pointwise Mutual Information (PPMI) Here contexts are defined as before, but the measure of the association between word and context is more clever: positive PMI (or PPMI for short). PPMI measure is widely regarded as state-of-the-art for pre-neural distributional-similarity models. Important : relation to neural models! Turns out, some of the neural methods we will consider (Word2Vec) were shown to implicitly approximate the factorization of a (shifted) PMI matrix. Stay tuned! Latent Semantic Analysis (LSA): Understanding Documents Latent Semantic Analysis (LSA) analyzes a collection of documents. While in the previous approaches contexts served only to get word vectors and were thrown away afterward, here we are also interested in context, or, in this case, document vectors. LSA is one of the simplest topic models: cosine similarity between document vectors can be used to measure similarity between documents. The term \"LSA\" sometimes refers to a more general approach of applying SVD to a term-document matrix where the term-document elements",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "in context, or, in this case, document vectors. LSA is one of the simplest topic models: cosine similarity between document vectors can be used to measure similarity between documents. The term \"LSA\" sometimes refers to a more general approach of applying SVD to a term-document matrix where the term-document elements can be computed in different ways (e.g., simple co-occurrence, tf-idf, or some other weighting). Animation alert! LSA wikipedia page has a nice animation of the topic detection process in a document-word matrix - take a look! Word2Vec: a Prediction-Based Method Let us remember our main idea again: Main idea : We have to put information about contexts into word vectors. While count-based methods took this idea quite literally, Word2Vec uses it in a different manner: How : Learn word vectors by teaching them to predict contexts . Word2Vec is a model whose parameters are word vectors. These parameters are optimized iteratively for a certain objective. The objective forces word vectors to \"know\" contexts a word can appear in: the vectors are trained to predict possible contexts of the corresponding words. As you remember from the distributional hypothesis, if vectors \"know\" about contexts, they \"know\" word meaning. Word2Vec is an iterative method. Its main idea is as follows: take a huge text corpus; go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window); for the central word, compute probabilities of context words; adjust the vectors to increase these probabilities. How to: go over the illustration to understand the main idea. Lena: Visualization idea is from the Stanford CS224n course. Objective Function : Negative Log-Likelihood For each position \\(t =1, \\dots, T\\) in a text corpus, Word2Vec predicts context words within a m-sized window given the central word \\(\\color{#88bd33}{w_t}\\): \\[\\color{#88bd33}{\\mbox{Likelihood}} \\color{black}= L(\\theta)= \\prod\\limits_{t=1}^T\\prod\\limits_{-m\\le j \\le m, j\\neq 0}P(\\color{#888}{w_{t+j}}|\\color{#88bd33}{w_t}\\color{black}, \\theta), \\] where \\(\\theta\\) are all variables to be optimized. The objective function (aka loss function or cost function) \\(J(\\theta)\\) is the average negative log-likelihood: Note how well the loss agrees with our plan main above: go over text with a sliding window and compute probabilities. Now let's find out how to compute these probabilities. How to calculate \\(P(\\color{#888}{w_{t+j}}\\color{black}|\\color{#88bd33}{w_t}\\color{black}, \\theta)\\)? For each word \\(w\\) we will have two vectors: \\(\\color{#88bd33}{v_w}\\) when it is a central word ; \\(\\color{#888}{u_w}\\) when it is a context word . (Once the vectors are trained, usually we throw away context vectors and use only word vectors.) Then for the central word \\(\\color{#88bd33}{c}\\) (c - central) and the context word \\(\\color{#888}{o}\\) (o - outside word) probability of the context word is Note : this is the softmax function ! (click for the details) The function above is an example of the softmax function : \\[softmax(x_i)=\\frac{\\exp(x_i)}{\\sum\\limits_{j=i}^n\\exp(x_j)}.\\] Softmax maps arbitrary values \\(x_i\\) to a probability distribution \\(p_i\\): \"max\" because the largest \\(x_i\\) will have the largest probability \\(p_i\\); \"soft\" because all probabilities are non-zero. You will deal with this function quite a lot over the NLP course (and in Deep Learning in general). How",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "function : \\[softmax(x_i)=\\frac{\\exp(x_i)}{\\sum\\limits_{j=i}^n\\exp(x_j)}.\\] Softmax maps arbitrary values \\(x_i\\) to a probability distribution \\(p_i\\): \"max\" because the largest \\(x_i\\) will have the largest probability \\(p_i\\); \"soft\" because all probabilities are non-zero. You will deal with this function quite a lot over the NLP course (and in Deep Learning in general). How to: go over the illustration. Note that for central words and context words , different vectors are used. For example, first the word a is central and we use \\(\\color{#88bd33}{v_a}\\), but when it becomes context, we use \\(\\color{#888}{u_a}\\) instead. How to train : by Gradient Descent, One Word at a Time Let us recall that our parameters \\(\\theta\\) are vectors \\(\\color{#88bd33}{v_w}\\) and \\(\\color{#888}{u_w}\\) for all words in the vocabulary. These vectors are learned by optimizing the training objective via gradient descent (with some learning rate \\(\\alpha\\)): \\[\\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta} J(\\theta).\\] One word at a time We make these updates one at a time: each update is for a single pair of a center word and one of its context words. Look again at the loss function: \\[\\color{#88bd33}{\\mbox{Loss}}\\color{black} =J(\\theta)= -\\frac{1}{T}\\log L(\\theta)= -\\frac{1}{T}\\sum\\limits_{t=1}^T \\sum\\limits_{-m\\le j \\le m, j\\neq 0}\\log P(\\color{#888}{w_{t+j}}\\color{black}|\\color{#88bd33}{w_t}\\color{black}, \\theta)= \\frac{1}{T} \\sum\\limits_{t=1}^T \\sum\\limits_{-m\\le j \\le m, j\\neq 0} J_{t,j}(\\theta). \\] For the center word \\(\\color{#88bd33}{w_t}\\), the loss contains a distinct term \\(J_{t,j}(\\theta)=-\\log P(\\color{#888}{w_{t+j}}\\color{black}|\\color{#88bd33}{w_t}\\color{black}, \\theta)\\) for each of its context words \\(\\color{#888}{w_{t+j}}\\). Let us look in more detail at just this one term and try to understand how to make an update for this step. For example, let's imagine we have a sentence with the central word cat , and four context words. Since we are going to look at just one step, we will pick only one of the context words; for example, let's take cute . Then the loss term for the central word cat and the context word cute is: \\[ J_{t,j}(\\theta)= -\\log P(\\color{#888}{cute}\\color{black}|\\color{#88bd33}{cat}\\color{black}) = -\\log \\frac{\\exp\\color{#888}{u_{cute}^T}\\color{#88bd33}{v_{cat}}}{ \\sum\\limits_{w\\in Voc}\\exp{\\color{#888}{u_w^T}\\color{#88bd33}{v_{cat}} }} = -\\color{#888}{u_{cute}^T}\\color{#88bd33}{v_{cat}}\\color{black} + \\log \\sum\\limits_{w\\in Voc}\\exp{\\color{#888}{u_w^T}\\color{#88bd33}{v_{cat}}}\\color{black}{.} \\] cat cute cat cute Note which parameters are present at this step: from vectors for central words , only \\(\\color{#88bd33}{v_{cat}}\\); from vectors for context words , all \\(\\color{#888}{u_w}\\) (for all words in the vocabulary). Only these parameters will be updated at the current step. Below is the schematic illustration of the derivations for this step. By making an update to minimize \\(J_{t,j}(\\theta)\\), we force the parameters to increase similarity (dot product) of \\(\\color{#88bd33}{v_{cat}}\\) and \\(\\color{#888}{u_{cute}}\\) and, at the same time, to decrease similarity between \\(\\color{#88bd33}{v_{cat}}\\) and \\(\\color{#888}{u_{w}}\\) for all other words \\(w\\) in the vocabulary. This may sound a bit strange: why do we want to decrease similarity between \\(\\color{#88bd33}{v_{cat}}\\) and all other words, if some of them are also valid context words (e.g., grey , playing , in on our example sentence)? But do not worry: since we make updates for each context word (and for all central words in your text), on average over all updates our vectors will learn the distribution of the possible contexts. grey playing in Try to derive the gradients at the final step of the illustration above. If you get lost, you can look",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "for each context word (and for all central words in your text), on average over all updates our vectors will learn the distribution of the possible contexts. grey playing in Try to derive the gradients at the final step of the illustration above. If you get lost, you can look at the paper Word2Vec Parameter Learning Explained . Faster Training: Negative Sampling In the example above, for each pair of a central word and its context word, we had to update all vectors for context words. This is highly inefficient: for each step, the time needed to make an update is proportional to the vocabulary size. But why do we have to consider all context vectors in the vocabulary at each step? For example, imagine that at the current step we consider context vectors not for all words, but only for the current target ( cute ) and several randomly chosen words. The figure shows the intuition. cute As before, we are increasing similarity between \\(\\color{#88bd33}{v_{cat}}\\) and \\(\\color{#888}{u_{cute}}\\). What is different, is that now we decrease similarity between \\(\\color{#88bd33}{v_{cat}}\\) and context vectors not for all words, but only with a subset of K \"negative\" examples . Since we have a large corpus, on average over all updates we will update each vector sufficient number of times, and the vectors will still be able to learn the relationships between words quite well. Formally, the new loss function for this step is: \\[ J_{t,j}(\\theta)= -\\log\\sigma(\\color{#888}{u_{cute}^T}\\color{#88bd33}{v_{cat}}\\color{black}) - \\sum\\limits_{w\\in \\{w_{i_1},\\dots, w_{i_K}\\}}\\log\\sigma({-\\color{#888}{u_w^T}\\color{#88bd33}{v_{cat}}}\\color{black}), \\] where \\(w_{i_1},\\dots, w_{i_K}\\) are the K negative examples chosen at this step and \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\) is the sigmoid function. Note that \\(\\sigma(-x)=\\frac{1}{1+e^{x}}=\\frac{1\\cdot e^{-x}}{(1+e^{x})\\cdot e^{-x}} = \\frac{e^{-x}}{1+e^{-x}}= 1- \\frac{1}{1+e^{x}}=1-\\sigma(x)\\). Then the loss can also be written as: \\[ J_{t,j}(\\theta)= -\\log\\sigma(\\color{#888}{u_{cute}^T}\\color{#88bd33}{v_{cat}}\\color{black}) - \\sum\\limits_{w\\in \\{w_{i_1},\\dots, w_{i_K}\\}}\\log(1-\\sigma({\\color{#888}{u_w^T}\\color{#88bd33}{v_{cat}}}\\color{black})). \\] How the gradients and updates change when using negative sampling? The Choice of Negative Examples Each word has only a few \"true\" contexts. Therefore, randomly chosen words are very likely to be \"negative\", i.e. not true contexts. This simple idea is used not only to train Word2Vec efficiently but also in many other applications, some of which we will see later in the course. Word2Vec randomly samples negative examples based on the empirical distribution of words. Let \\(U(w)\\) be a unigram distribution of words, i.e. \\(U(w)\\) is the frequency of the word \\(w\\) in the text corpus. Word2Vec modifies this distribution to sample less frequent words more often: it samples proportionally to \\(U^{3/4}(w)\\). Word2Vec variants: Skip-Gram and CBOW There are two Word2Vec variants: Skip-Gram and CBOW. Skip-Gram is the model we considered so far: it predicts context words given the central word. Skip-Gram with negative sampling is the most popular approach. CBOW (Continuous Bag-of-Words) predicts the central word from the sum of context vectors. This simple sum of word vectors is called \"bag of words\", which gives the name for the model. How the loss function and the gradients change for the CBOW model? If you get lost, you can again look at the paper Word2Vec Parameter Learning Explained . Additional Notes The original Word2Vec papers are: Efficient Estimation of Word",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "called \"bag of words\", which gives the name for the model. How the loss function and the gradients change for the CBOW model? If you get lost, you can again look at the paper Word2Vec Parameter Learning Explained . Additional Notes The original Word2Vec papers are: Efficient Estimation of Word Representations in Vector Space Distributed Representations of Words and Phrases and their Compositionality You can look into them for the details on the experiments, implementation and hyperparameters. Here we will provide some of the most important things you need to know. The Idea is Not New The idea to learn word vectors (distributed representations) is not new. For example, there were attempts to learn word vectors as part of a larger network and then extract the embedding layer. (For the details on the previous methods, you can look, for example, at the summary in the original Word2Vec papers). What was very unexpected in Word2Vec, is its ability to learn high-quality word vectors very fast on huge datasets and for large vocabularies. And of course, all the fun properties we will see in the Analysis and Interpretability section quickly made Word2Vec very famous. Why Two Vectors? As you remember, in Word2Vec we train two vectors for each word: one when it is a central word and another when it is a context word. After training, context vectors are thrown away. This is one of the tricks that made Word2Vec so simple. Look again at the loss function (for one step): \\[ J_{t,j}(\\theta)= -\\color{#888}{u_{cute}^T}\\color{#88bd33}{v_{cat}}\\color{black} - \\log \\sum\\limits_{w\\in V}\\exp{\\color{#888}{u_w^T}\\color{#88bd33}{v_{cat}}}\\color{black}{.} \\] When central and context words have different vectors, both the first term and dot products inside the exponents are linear with respect to the parameters (the same for the negative training objective). Therefore, the gradients are easy to compute. Repeat the derivations (loss and the gradients) for the case with one vector for each word (\\(\\forall w \\ in \\ V, \\color{#88bd33}{v_{w}}\\color{black}{ = }\\color{#888}{u_{w}}\\) ). While the standard practice is to throw away context vectors, it was shown that averaging word and context vectors may be more beneficial. More details are here. Better training There's one more trick: learn more from this exercise in the Research Thinking section. Relation to PMI Matrix Factorization Word2Vec SGNS (Skip-Gram with Negative Sampling) implicitly approximates the factorization of a (shifted) PMI matrix. Learn more here. The Effect of Window Size The size of the sliding window has a strong effect on the resulting vector similarities. For example, this paper notes that larger windows tend to produce more topical similarities (i.e. dog , bark and leash will be grouped together, as well as walked , run and walking ), while smaller windows tend to produce more functional and syntactic similarities (i.e. Poodle , Pitbull , Rottweiler , or walking , running , approaching ). (Somewhat) Standard Hyperparameters Model: Skip-Gram with negative sampling; Number of negative examples: for smaller datasets, 15-20; for huge datasets (which are usually used) it can be 2-5. Embedding dimensionality: frequently used value is 300, but other variants (e.g., 100 or 50) are also possible. For theoretical",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "running , approaching ). (Somewhat) Standard Hyperparameters Model: Skip-Gram with negative sampling; Number of negative examples: for smaller datasets, 15-20; for huge datasets (which are usually used) it can be 2-5. Embedding dimensionality: frequently used value is 300, but other variants (e.g., 100 or 50) are also possible. For theoretical explanation of the optimal dimensionality, take a look at the Related Papers section. Sliding window (context) size: 5-10. GloVe: Global Vectors for Word Representation The GloVe model is a combination of count-based methods and prediction methods (e.g., Word2Vec). Model name, GloVe, stands for \"Global Vectors\", which reflects its idea: the method uses global information from corpus to learn vectors . As we saw earlier , the simplest count-based method uses co-occurrence counts to measure the association between word w and context c : N( w , c ). GloVe also uses these counts to construct the loss function: w c Similar to Word2Vec, we also have different vectors for central and context words - these are our parameters. Additionally, the method has a scalar bias term for each word vector. What is especially interesting, is the way GloVe controls the influence of rare and frequent words: loss for each pair ( w , c ) is weighted in a way that rare events are penalized, very frequent events are not over-weighted. Lena: The loss function looks reasonable as it is, but the original GloVe paper has very nice motivation leading to the above formula. I will not provide it here (I have to finish the lecture at some point, right?..), but you can read it yourself - it's really, really nice! Evaluation of Word Embeddings How can we understand that one method for getting word embeddings is better than another? There are two types of evaluation (not only for word embeddings): intrinsic and extrinsic. Intrinsic Evaluation : Based on Internal Properties This type of evaluation looks at the internal properties of embeddings, i.e. how well they capture meaning. Specifically, in the Analysis and Interpretability section, we will discuss in detail how we can evaluate embeddings on word similarity and word analogy tasks. Extrinsic Evaluation : On a Real Task This type of evaluation tells which embeddings are better for the task you really care about (e.g., text classification, coreference resolution, etc.). In this setting, you have to train the model/algorithm for the real task several times: one model for each of the embeddings you want to evaluate. Then, look at the quality of these models to decide which embeddings are better. How to Choose? One thing you have to get used to is that there is no perfect solution and no right answer for all situations: it always depends on many things. Regarding evaluation, you usually care about quality of the task you want to solve. Therefore, you are likely to be more interested in extrinsic evaluation. However, real-task models usually require a lot of time and resources to train, and training several of them may be too expensive. In the end, this is your call to make :) Analysis and Interpretability",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "you want to solve. Therefore, you are likely to be more interested in extrinsic evaluation. However, real-task models usually require a lot of time and resources to train, and training several of them may be too expensive. In the end, this is your call to make :) Analysis and Interpretability Lena: For word embeddings, most of the content of this part is usually considered as evaluation (intrinsic evaluation). However, since looking at what a model learned (beyond task-specific metrics) is the kind of thing people usually do for analysis, I believe it can be presented here, in the analysis section. Take a Walk Through Space... Semantic Space! Semantic spaces aim to create representations of natural language that capture meaning. We can say that (good) word embeddings form semantic space and will refer to a set of word vectors in a multi-dimensional space as \"semantic space\". Below is shown semantic space formed by GloVe vectors trained on twitter data (taken from gensim ). Vectors were projected to two-dimensional space using t-SNE; these are only the top-3k most frequent words. How to: Walk through semantic space and try to find: language clusters: Spanish, Arabic, Russian, English. Can you find more languages? clusters for: food, family, names, geographical locations. What else can you find? Nearest Neighbors The example is from the GloVe project page . The example is from the GloVe project page . During your walk through semantic space, you probably noticed that the points (vectors) which are nearby usually have close meaning. Sometimes, even rare words are understood very well. Look at the example: the model understood that words such as leptodactylidae or litoria are close to frog . leptodactylidae litoria frog Several pairs from the Rare Words similarity benchmark . Several pairs from the Rare Words similarity benchmark . Word Similarity Benchmarks \"Looking\" at nearest neighbors (by cosine similarity or Euclidean distance) is one of the methods to estimate the quality of the learned embeddings. There are several word similarity benchmarks (test sets). They consist of word pairs with a similarity score according to human judgments. The quality of embeddings is estimated as the correlation between the two similarity scores (from model and from humans). Linear Structure While similarity results are encouraging, they are not surprising: all in all, the embeddings were trained specifically to reflect word similarity. What is surprising, is that many semantic and syntactic relationships between words are (almost) linear in word vector space. For example, the difference between king and queen is (almost) the same as between man and woman. Or a word that is similar to queen in the same sense that kings is similar to king turns out to be queens . The man-woman \\(\\approx\\) king-queen example is probably the most popular one, but there are also many other relations and funny examples. king queen man woman. queen kings king queens man-woman king-queen Below are examples for the country-capital relation and a couple of syntactic relations. At ICML 2019, it was shown that there's actually a theoretical explanation for analogies in Word2Vec. More details are here.",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "also many other relations and funny examples. king queen man woman. queen kings king queens man-woman king-queen Below are examples for the country-capital relation and a couple of syntactic relations. At ICML 2019, it was shown that there's actually a theoretical explanation for analogies in Word2Vec. More details are here. Lena: This paper, Analogies Explained: Towards Understanding Word Embeddings by Carl Allen and Timothy Hospedales from the University of Edinburgh, received Best Paper Honourable Mention award at ICML 2019 - well deserved! Word Analogy Benchmarks These near-linear relationships inspired a new type of evaluation: word analogy evaluation. Examples of relations and word pairs from the Google analogy test set . Examples of relations and word pairs from the Google analogy test set . Given two word pairs for the same relation, for example (man, woman) and (king, queen) , the task is to check if we can identify one of the words based on the rest of them. Specifically, we have to check if the closest vector to king - man + woman corresponds to the word queen . (man, woman) (king, queen) king - man + woman queen Now there are several analogy benchmarks; these include the standard benchmarks ( MSR + Google analogy test sets) and BATS (the Bigger Analogy Test Set) . Similarities across Languages We just saw that some relationships between words are (almost) linear in the embedding space. But what happens across languages? Turns out, relationships between semantic spaces are also (somewhat) linear: you can linearly map one semantic space to another so that corresponding words in the two languages match in the new, joint semantic space. The figure above illustrates the approach proposed by Tomas Mikolov et al. in 2013 not long after the original Word2Vec. Formally, we are given a set of word pairs and their vector representations \\(\\{\\color{#88a635}{x_i}\\color{black}, \\color{#547dbf}{z_i}\\color{black} \\}_{i=1}^n\\), where \\(\\color{#88a635}{x_i}\\) and \\(\\color{#547dbf}{z_i}\\) are vectors for i-th word in the source language and its translation in the target. We want to find a transformation matrix W such that \\(W\\color{#547dbf}{z_i}\\) approximates \\(\\color{#88a635}{x_i}\\) : \"matches\" words from the dictionary. We pick \\(W\\) such that \\[W = \\arg \\min\\limits_{W}\\sum\\limits_{i=1}^n\\parallel W\\color{#547dbf}{z_i}\\color{black} - \\color{#88a635}{x_i}\\color{black}\\parallel^2,\\] and learn this matrix by gradient descent. In the original paper, the initial vocabulary consists of the 5k most frequent words with their translations, and the rest is learned. Later it turned out, that we don't need a dictionary at all - we can build a mapping between semantic spaces even if we know nothing about languages! More details are here. Is the \"true\" mapping between languages indeed linear, or more complicated? We can look at geometry of the learned semantic spaces and check. More details are here. The idea to linearly map different embedding sets to (nearly) match them can also be used for a very different task! Learn more in the Research Thinking section. Research Thinking How to Read the short description at the beginning - this is our starting point, something known. Read a question and think: for a minute, a day, a week, ... - give yourself some time! Even",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "for a very different task! Learn more in the Research Thinking section. Research Thinking How to Read the short description at the beginning - this is our starting point, something known. Read a question and think: for a minute, a day, a week, ... - give yourself some time! Even if you are not thinking about it constantly, something can still come to mind. Look at the possible answers - previous attempts to answer/solve this problem. Important: You are not supposed to come up with something exactly like here - remember, each paper usually takes the authors several months of work. It's a habit of thinking about these things that counts! All the rest a scientist needs is time: to try-fail-think until it works. It's well-known that you will learn something easier if you are not just given the answer right away, but if you think about it first. Even if you don't want to be a researcher, this is still a good way to learn things! Count-Based Methods cat cute grey playing in ? ? cute cat Word2Vec cat cute grey playing in ? word frequency We can expect that frequent words usually give less information than rare ones. For example, the fact that cat appears in context of in does not tell us much about the meaning of cat : the word in serves as a context for many other words. In contrast, cute , grey and playing already give us some idea about cat . cat in cat in cute grey playing cat distance from the central word As we discussed in the previous exercise on count-based methods , words that are closer to the central may be more important. ? 1. Word Frequency 2. Distance from the central word ? better understanding of morphology By assigning a distinct vector to each word, we ignore morphology. Giving information about subwords can let the model know that different tokens can be forms of the same word. representations for unknown words Usually, we can represent only those words, which are present in the vocabulary. Giving information about subwords can help to represent out-of-vocabulary words relying of their spelling. handling misspellings Even if one character in a word is wrong, this is another token, and, therefore, a completely different embedding (or even unknown word). With information about subwords, misspelled word would still be similar to the original one. ? Semantic Change ? ACL 2020 : train embeddings, look at the neighbors Lena: Note that while the approach is recent, it is extremely simple and works better than previous more complicated ideas. Never be afraid to try simple things - you'll be surprised how often they work! Previous popular approach : align two embedding sets Lena: You will implement Ortogonal Proctustes in your homework to align Russian and Ukranian embeddings. Find the notebook in the course repo . Related Papers How to High-level : look at key results in short summaries - get an idea of what's going on in the field. A bit deeper : for topics which interest you more, read",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "homework to align Russian and Ukranian embeddings. Find the notebook in the course repo . Related Papers How to High-level : look at key results in short summaries - get an idea of what's going on in the field. A bit deeper : for topics which interest you more, read longer summaries with illustrations and explanations. Take a walk through the authors' reasoning steps and key observations. In depth : read the papers you liked. Now, when you got the main idea, this is going to be easier! What's inside: Good Old Classics Analyzing Geometry Biases in Word Embeddings Semantic Change Theory to the Rescue! - coming soon Cross-Lingual Embeddings - coming soon ... to be updated Good Old Classics Theoretically, Word2Vec is not so different from matrix factorization approaches! Skip-gram with negative-sampling (SGNS) implicitly factorizes the shifted pointwise mutual information (PMI) matrix: \\(PMI(\\color{#88bd33}{w}\\color{black}, \\color{#888}{c}\\color{black})-\\log k\\), where \\(k\\) is the number of negative examples in negative sampling. ( w , c ) word-context pair: \\(N(\\color{#88bd33}{w}\\color{black}, \\color{#888}{c}\\color{black})\\) times; c as negative example for w : \\( \\frac{kN(\\color{#88bd33}{w}\\color{black})N(\\color{#888}{c}\\color{black})}{N}\\) times. Why: each time we sample a negative example, we can pick c with the probability \\(\\frac{N(\\color{#888}{c}\\color{black})}{N}\\) - frequency of c . Multiply by N( w ) because we meet w exactly N( w ) times; multiply by \\(k\\) because we sample \\(k\\) negative examples. Why: each time we sample a negative example, we can pick c with the probability \\(\\frac{N(\\color{#888}{c}\\color{black})}{N}\\) - frequency of c . Multiply by N( w ) because we meet w exactly N( w ) times; multiply by \\(k\\) because we sample \\(k\\) negative examples. At some point, it was believed that prediction-based embeddings are better than count-based. But this is not true: we can adapt some \"tricks\" from the word2vec implementation to count-based models and achieve the same results. Also, when evaluated properly, GloVE is worse than Word2Vec. The paper tests many hyperparameters and has lots of experiments - I do recommend looking into it. Here I will provide the most important things you need to remember. Eigenvalue Weighting: It is better to use SVD \"incorrectly\" Typically, word and context vectors derived by SVD are represented by \\(V_d\\Sigma_d\\) and \\(U_d\\): the eigenvalue matrix is included only in word vectors. However, for word similarity tasks this is not the optimal construction. The experiments show that symmetric variants are better: either include \\(\\sqrt{\\Sigma_d}\\) in both word and context vectors, or discard in both (look at the figure). Context Distribution Smoothing As we discussed in the lecture , Word2Vec samples negative examples according to smoothed unigram distribution \\(U^{3/4}\\). This was done to pick rare words more frequently. We can do something similar when calculating PMI: instead of true context distribution, let's use the smoothed one (look at the figure to the right). As in Word2Vec, \\(\\alpha=0.75\\). Word and Context Vectors in Word2Vec: Try to Average Recall that after training GloVe averages word and context vectors, while Word2Vec throws context vectors away. However, sometimes Word2Vec can also benefit from averaging: you have to try! Main Results with tuned hyperparameters, prediction-based embeddings are not better than",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "Word2Vec, \\(\\alpha=0.75\\). Word and Context Vectors in Word2Vec: Try to Average Recall that after training GloVe averages word and context vectors, while Word2Vec throws context vectors away. However, sometimes Word2Vec can also benefit from averaging: you have to try! Main Results with tuned hyperparameters, prediction-based embeddings are not better than count-based; with a couple of fixes, Word2Vec (SGNS) is better than GloVe on every task . Analyzing Geometry Word vectors point to roughly the same direction The authors evaluate dot products of vectors for words of different frequencies with the mean of all vectors. Since the distributions are very close and dot products are positive, the vectors (mostly) point in the direction of the mean vector. Context vectors point away from word vectors Here we do the same, but take context vectors (the mean is still for word vectors). For SGNS, dot products of context vectors with the mean of word vectors are negative. This means that context vectors point away from word vectors, and it is not reasonable to use them - we throw them away and use only word vectors. For GloVe, this is not the case: context vectors behave the same way as word vectors. We learned that we can (almost) match semantic spaces for different languages linearly. But is the \"true\" underlying mapping between languages indeed linear? If it is linear globally, then all local linear mappings have to be similar (to the global linear mapping, and hence to each other). Well, they are not. How to check if the \"true\" mapping between semantic spaces is indeed linear? The main idea is shown at the figure. Local cross-lingual mappings are not similar To check if the local mappings are similar, the authors for several words, take their neighborhood: a set of words with the cosine similarity at least some value; for each neighborhood, find the corresponding set of words in the other language; build local cross-linear mappings; evaluate how similar these mappings are: for two mappings \\(M_1\\) and \\(M_2\\) (e.g., for neighborhoods of words \\(w_1\\) and \\(w_2\\)), compute the cosine similarity between the vectorized versions of matrices \\(M_1\\) and \\(M_2\\). For distant words, their local cross-lingual mappings are different The authors found that local mappings for different neighborhoods can be very different. Therefore, \"true\" cross-lingual mapping between semantic spaces is not linear; for more distant words, the local cross-lingual mappings are more different. Lena : This is an example of how analysis can improve quality! The authors noticed that (i) embeddings have non-zero mean and (ii) early singular values are much larger than the rest. When the authors eliminated these properties, they got large improvements in both intrinsic and extrinsic evaluation. Step 1: Analyze For different word embedding models and languages, the authors found that vectors have a large non-zero mean are not isotropic Look at the figure: if \\(\\sigma_i\\) are singular values, then they decay almost exponentially for small \\(i\\), and remain roughly constant for the rest. Additionally, the authors noticed that the top PCA components encode something which is not related to semantics: e.g., word frequency.",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "large non-zero mean are not isotropic Look at the figure: if \\(\\sigma_i\\) are singular values, then they decay almost exponentially for small \\(i\\), and remain roughly constant for the rest. Additionally, the authors noticed that the top PCA components encode something which is not related to semantics: e.g., word frequency. These properties seem to have nothing to do with semantic, i.e., something which is important for us in word embeddings. What if we eliminate these effects? Will it be better? Step 2: Use Observations to Improve Quality To eliminate the found properties, the authors subtract from word vectors their mean eliminate top PCA components Let \\(u_1, \\dots, u_d\\) be the PCA components of word vectors \\(\\{v_w, w\\in V\\}\\). Then the vectors are updated as follows: \\[v_w \\longleftarrow v_w - \\sum\\limits_{i=1}^d(u_i^Tv_w)u_i.\\] Result : large improvements in various tasks, both intrinsic (similarity and analogy) and extrinsic (supervised classification). Biases in Word Embeddings Word embeddings are biased. For example, while their analogical reasoning can be desirable, e.g. \"a man to a woman is as a king to a queen \", but also \"a man to a woman is as a physician to a nurse \", which is an undesired association. man woman king queen man woman physician nurse Problem: Embeddings are Biased The authors noticed that word embeddings are biased: they encode undesired gender associations. To find such examples, they take a seed pair (e.g., (a, b) = ( he , she )) and find pairs of words which have the same association: differ from each other in the same direction, and relatively close to each other. Formally, they pick pairs with the high score: he she Look at the results below - definitely some pairs are biased! This means that, for example, not only \"a man to a woman is as a king to a queen \", which is the desired behavior, but also \"a man to a woman is as a physician to a nurse \", which is an undesired association. man woman king queen man woman physician nurse Gender-stereotypic occupations To find the most gender-stereotypic occupations, the authors project occupations onto the he - she gender direction. Results are shown to the right. he she We can see that, for example, homemaker , nurse , librarian , stylist are mostly associated with women, while captain , magician , architect , warrior are more strongly associated with men. homemaker nurse librarian stylist captain magician architect warrior Debiasing Word Embeddings In the original paper , the authors also propose several heuristics to debias word embeddings - remove the undesired associations as a post-processing step. Since a lot has been done on debiasing recently, for more details on this specific approach look in the original paper. For a more recent method, look at the next paper . Iterative nullspace projection to debias word embeddings: train a linear classifier \\(W\\) to predict a property from embeddings (e.g., gender), linearly project embeddings on the \\(W\\)'s nullspace (\\(x \\rightarrow Px\\), \\(W(Px)=0\\)) - remove the information used for prediction; repeat until a classifier is not able to predict anything.",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "paper . Iterative nullspace projection to debias word embeddings: train a linear classifier \\(W\\) to predict a property from embeddings (e.g., gender), linearly project embeddings on the \\(W\\)'s nullspace (\\(x \\rightarrow Px\\), \\(W(Px)=0\\)) - remove the information used for prediction; repeat until a classifier is not able to predict anything. Idea : Remove Information Used by a Linear Classifier We have to remove the information about some desired property (e.g., gender), but not to harm other properties of the embeddings. The authors proposed a very simple idea: train a linear classifier to predict this property from the embeddings, then remove the information this classifier used. If the classifier is linear, the removing part can be done easily: by projecting onto the classifier's decision boundary. This projection is the least harming way to remove the linear information about the property: it harms the distances between embeddings as little as possible. The method is iterative : you have to repeat this (train a classifier and project to the new decision boundary) until the classifier is not able to predict anything meaningful. When a classifier can not predict the property, we know that all information has been removed. Results: All Good In the original paper , you will find experiments showing that the method: does remove bias (look at the illustration to the right: t-SNE projection of GloVe vectors of the most gender-biased words at 0, 3, 18, 35 iterations of the algorithm), does not hurt embedding quality (e.g., look at the closest neighbors before and after debiasing: see below) . For more formal things and more results and examples, look at the original paper . Semantic Change Imagine you have text corpora from different sources: time periods, populations, geographic regions, etc. In this part, the task is to find words that used differently in these corpora. Lena : This paper was used in the Research Thinking section. Here I've hidden from you the links and the content - better go there to think. But if you do want, you can learn about the paper here. Spoiler alert! To find which words are used differently in two text corpora: train embeddings using each of the corpora, map linearly the two embedding spaces to each other; words whose vectors do not match well are the ones that changed their meaning. Idea : Align Two Embedding Sets, Find Words That Do Not Match The main idea here is to align two embeddings sets and to find words whose embeddings do not match well. Formally, let \\(\\color{#88a635}{W_1}\\color{black}, \\color{#547dbf}{W_2}\\color{black} \\in \\mathbb{R}^{d\\times |V|}\\) be embedding sets trained on different corpora. To align the learned embeddings, the authors find the rotation \\[R = \\arg \\max\\limits_{Q^TQ=I}\\parallel \\color{#547dbf}{W_2}\\color{black}Q - \\color{#88a635}{W_1}\\color{black}\\parallel_F.\\] This is called Orthogonal Procrustes. Using this rotation, we can align embedding sets and find words that do not match well: these are the words that change meaning with the corpora. Once the embedding sets are aligned, we can evaluate the semantic displacement . Let \\(\\color{#88a635}{v_w^1}\\) and \\(\\color{#547dbf}{v_w^2}\\) be embedding of a word \\(w\\) in the two aligned spaces, then the semantic displacement",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "and find words that do not match well: these are the words that change meaning with the corpora. Once the embedding sets are aligned, we can evaluate the semantic displacement . Let \\(\\color{#88a635}{v_w^1}\\) and \\(\\color{#547dbf}{v_w^2}\\) be embedding of a word \\(w\\) in the two aligned spaces, then the semantic displacement is \\(1- \\cos (\\color{#88a635}{v_w^1}\\color{black}, \\color{#547dbf}{v_w^2}\\color{black}).\\) Intuitively, this measures how well embeddings of the same word \"match\" in the aligned semantic spaces. Experiments TL;DR: SGNS Embeddings are Better than PPMI and SVD(PPMI) The authors looked at historical texts for different time periods and tried to apply the method on top of different embeddings: PPMI matrix, SVD(PPMI) and Word2Vec (SGNS). Below are examples of the top words found for each of the embedding methods. bold - real semantic shifts (validated by examining literature) E.g., headed shifted from primarily referring to the \"top of a body/entity\" to referring to \"a direction of travel.\" E.g., headed shifted from primarily referring to the \"top of a body/entity\" to referring to \"a direction of travel.\" underlined - borderline cases (largely due to global genre/discourse shifts) E.g., male has not changed in meaning, but its usage in discussions of “gender equality” is relatively new. E.g., male has not changed in meaning, but its usage in discussions of “gender equality” is relatively new. unmarked - clear corpus artifacts E.g., special, cover, and romance are artifacts from the covers of fiction books occasionally including advertisements etc. E.g., special, cover, and romance are artifacts from the covers of fiction books occasionally including advertisements etc. Looks like results obtained for SGNS embeddings are better. In the original paper , different kinds of evaluation were used to confirm this more formally. From the next paper , you will learn how to detect semantic change more easily. Note: The Alignment Idea is Used for Different Tasks Note that the idea to linearly map different semantic spaces was also used for other tasks. For example, earlier in the lecture we aligned semantic spaces for different languages to build vocabulary. For more advanced methods for building cross-lingual embeddings, look here in the Related Papers . Lena : This paper was used in the Research Thinking section. Here I've hidden from you the links and the content - better go there to think. But if you do want, you can learn about the paper here. Spoiler alert! To find which words are used differently in two text corpora: train embeddings using each of the corpora, for each word, find closest neighbors in the two embedding spaces; the neighbors differ a lot → the words are used differently. Idea : Train Embeddings, Look at the Neighbors A very simple approach is to train embeddings (e.g., Word2Vec) and look at the closest neighbors. If a word's closest neighbors are different for the two corpora, the word changed its meaning: remember that word embeddings reflect contexts they saw! Formally, for each word \\(w\\) the authors take k nearest neighbors in the two embeddings sets: \\(NN_1^k(w)\\) and \\(NN_2^k(w)\\). Then they count how many neighbors are the same and define the change score",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Word Embeddings",
        "url": "https://lena-voita.github.io/nlp_course/word_embeddings.html",
        "chunk": {
            "text": "for the two corpora, the word changed its meaning: remember that word embeddings reflect contexts they saw! Formally, for each word \\(w\\) the authors take k nearest neighbors in the two embeddings sets: \\(NN_1^k(w)\\) and \\(NN_2^k(w)\\). Then they count how many neighbors are the same and define the change score as follows: \\[score^k(w) = -|NN_1^k(w)\\cap NN_2^k(w)|\\] A large intersection means that the meaning is not different (the score will be low), small intersection - meaning is different (such words will receive a high score). The Method is Interpretable By design, the method is interpretable: it explains its decisions (i.e., why the word is used differently) by showing the closest neighbors of the word in the two embedding spaces. These neighbors reflect the word meanings in the two corpora. Look at the examples of found words along with the closest neighbors. Other Good Things Compared to the alignment-based methods (e.g., the previous paper ), this approach: is more stable, requires less tuning and word filtering. For more details, look at the paper. Lena: Note that while the approach is recent, it is extremely simple and works better than previous more complicated ideas. Never be afraid to try simple things - you'll be surprised how often they work! Theory to the Rescue! On the Dimensionality of Word Embedding Analogies Explained: Towards Understanding Word Embeddings Cross-Lingual Embeddings Word Translation Without Parallel Data ... to be updated Have Fun! Semantic Space Surfer Usually, we want word embeddings to reason as humans do. But let's try the opposite: you will try to think as word embeddings. You will see the analogical example, e.g. king - man + woman = ? , and several possible answers. The task is to guess what word embeddings think. Complete the task (10 examples) and get a Semantic Space Surfer Certificate ! Word embeddings: we used glove-twitter-100 from gensim-data . Big thanks Just Heuristic for the help with technical issues! Just Heuristic - Just Fun! next Semantic Space Surfer: Level 0 NLP course | For YOU : Official Certificate",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "What does it mean to \"model something\"? Imagine that we have, for example, a model of a physical world. What do you expect it to be able to do? Well, if it is a good model, it probably can predict what happens next given some description of \"context\", i.e., the current state of things. Something of the following kind: We have a tower of that many toy cubes of that size made from this material. We push the bottom cube from this point in that direction with this force. What will happen? A good model would simulate the behavior of the real world: it would \"understand\" which events are in better agreement with the world, i.e., which of them are more likely . What about language? For language, the intuition is exactly the same! What is different, is the notion of an event . In language, an event is a linguistic unit (text, sentence, token, symbol), and a goal of a language model is to estimate the probabilities of these events. Language Models (LMs) estimate the probability of different linguistic units: symbols, tokens, token sequences. But how can this be useful? We deal with LMs every day! We see language models in action every day - look at some examples. Usually models in large commercial services are a bit more complicated than the ones we will discuss today, but the idea is the same: if we can estimate probabilities of words/sentences/etc, we can use them in various, sometimes even unexpected, ways. What is easy for humans, can be very hard for machines morphosyntax We, humans, already have some feeling of \"probability\" when it comes to natural language. For example, when we talk, usually we understand each other quite well (at least, what's being said). We disambiguate between different options which sound similar without even realizing it! But how a machine is supposed to understand this? A machine needs a language model, which estimates the probabilities of sentences. If a language model is good, it will assign a larger probability to a correct option. General Framework Text Probability Our goal is to estimate probabilities of text fragments; for simplicity, let's assume we deal with sentences. We want these probabilities to reflect knowledge of a language. Specifically, we want sentences that are \"more likely\" to appear in a language to have a larger probability according to our language model. How likely is a sentence to appear in a language? Let's check if simple probability theory can help. Imagine we have a basket with balls of different colors. The probability to pick a ball of a certain color (let's say green) from this basket is the frequency with which green balls occur in the basket. What if we do the same for sentences? Since we can not possibly have a text corpus that contains all sentences in a natural language, a lot of sentences will not occur in the corpus. While among these sentences some are clearly more likely than the others, all of them will receive zero probability, i.e., will look equally bad for",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "can not possibly have a text corpus that contains all sentences in a natural language, a lot of sentences will not occur in the corpus. While among these sentences some are clearly more likely than the others, all of them will receive zero probability, i.e., will look equally bad for the model. This means, the method is not good and we have to do something more clever. Sentence Probability: Decompose Into Smaller Parts We can not reliably estimate sentence probabilities if we treat them as atomic units. Instead, let's decompose the probability of a sentence into probabilities of smaller parts. For example, let's take the sentence I saw a cat on a mat and imagine that we read it word by word. At each step, we estimate the probability of all seen so far tokens. We don't want any computations not to be in vain (no way!), so we won't throw away previous probability once a new word appears: we will update it to account for a new word. Look at the illustration. I saw a cat on a mat Formally, let \\(y_1, y_2, \\dots, y_n\\) be tokens in a sentence, and \\(P(y_1, y_2, \\dots, y_n)\\) the probability to see all these tokens (in this order). Using the product rule of probability (aka the chain rule), we get \\[P(y_1, y_2, \\dots, y_n)=P(y_1)\\cdot P(y_2|y_1)\\cdot P(y_3|y_1, y_2)\\cdot\\dots\\cdot P(y_n|y_1, \\dots, y_{n-1})= \\prod \\limits_{t=1}^n P(y_t|y_{\\mbox{<}t}).\\] We decomposed the probability of a text into conditional probabilities of each token given the previous context. We got: Left-to-Right Language Models What we got is the standard left-to-right language modeling framework. This framework is quite general: N-gram and neural language models differ only in a way they compute the conditional probabilities \\(P(y_t|y_1, \\dots, y_{t-1})\\). Lena : Later in the course we will see other language models: for example, Masked Language Models or models that decompose the joint probability differently (e.g., arbitrary order of tokens and not fixed as the left-to-right order). We will come to specifics of N-gram and neural models a bit later. Now, we discuss how to generate a text using a language model. Generate a Text Using a Language Model Once we have a language model, we can use it to generate text. We do it one token at a time: predict the probability distribution of the next token given previous context, and sample from this distribution. Alternatively, you can apply greedy decoding : at each step, pick the token with the highest probability. However, this usually does not work well: a bit later I will show you examples from real models. Despite its simplicity, such sampling is quite common in generation. In section Generation Strategies we will look at different modifications of this approach to get samples with certain qualities; e.g., more or less \"surprising\". N-gram Language Models Let us recall that the general left-to-right language modeling framework decomposes probability of a token sequence, into conditional probabilities of each token given previous context: \\[P(y_1, y_2, \\dots, y_n)=P(y_1)\\cdot P(y_2|y_1)\\cdot P(y_3|y_1, y_2)\\cdot\\dots\\cdot P(y_n|y_1, \\dots, y_{n-1})= \\prod \\limits_{t=1}^n P(y_t|y_{\\mbox{<}t}).\\] The only thing which is not clear so far is",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "Language Models Let us recall that the general left-to-right language modeling framework decomposes probability of a token sequence, into conditional probabilities of each token given previous context: \\[P(y_1, y_2, \\dots, y_n)=P(y_1)\\cdot P(y_2|y_1)\\cdot P(y_3|y_1, y_2)\\cdot\\dots\\cdot P(y_n|y_1, \\dots, y_{n-1})= \\prod \\limits_{t=1}^n P(y_t|y_{\\mbox{<}t}).\\] The only thing which is not clear so far is how to compute these probabilities. We need to : define how to compute the conditional probabilities \\(P(y_t|y_1, \\dots, y_{t-1})\\). Similar to count-based methods we saw earlier in the Word Embeddings lecture, n-gram language models also count global statistics from a text corpus. How : estimate based on global statistics from a text corpora, i.e., count . That is, the way n-gram LMs estimate probabilities \\(P(y_t|y_{\\mbox{<}t}) = P(y_t|y_1, \\dots, y_{t-1})\\) is almost the same as the way we earlier estimated the probability to pick a green ball from a basket. This innocent \"almost\" contains the key components of n-gram LMs: Markov property and smoothings . Markov Property (Independence Assumption) The straightforward way to compute \\(P(y_t|y_1, \\dots, y_{t-1})\\) is \\[P(y_t|y_1, \\dots, y_{t-1}) = \\frac{N(y_1, \\dots, y_{t-1}, y_t)}{N(y_1, \\dots, y_{t-1})},\\] where \\(N(y_1, \\dots, y_k)\\) is the number of times a sequence of tokens \\((y_1, \\dots, y_k)\\) occur in the text. For the same reasons we discussed before, this won't work well: many of the fragments \\((y_1, \\dots, y_{t})\\) do not occur in a corpus and, therefore, will zero out the probability of the sentence. To overcome this problem, we make an independence assumption (assume that the Markov property holds): The probability of a word only depends on a fixed number of previous words. n=3 (trigram model): \\(P(y_t|y_1, \\dots, y_{t-1}) = P(y_t|y_{t-2}, y_{t-1})\\), n=2 (bigram model): \\(P(y_t|y_1, \\dots, y_{t-1}) = P(y_t|y_{t-1})\\), n=1 (unigram model): \\(P(y_t|y_1, \\dots, y_{t-1}) = P(y_t)\\). Look how the standard decomposition changes for n-gram models. Smoothing: Redistribute Probability Mass Let's imagine we deal with a 4-gram language model and consider the following example: What if either denominator or numerator is zero? Both these cases are not really good for the model. To avoid these problems (and some other), it is common to use smoothings . Smoothings redistribute probability mass: they \"steal\" some mass from seen events and give to the unseen ones. Lena : at this point, usually I'm tempted to imagine a brave Robin Hood, stealing from the rich and giving to the poor - just like smoothings do with the probability mass. Unfortunately, I have to stop myself, because, let's be honest, smoothings are not so clever - it would be offensive to Robin. Avoid zeros in the denominator If the phrase cat on a never appeared in our corpus, we will not be able to compute the probability. Therefore, we need a \"plan B\" in case this happens. cat on a Backoff (aka Stupid Backoff) One of the solutions is to use less context for context we don't know much about. This is called backoff: if you can, use trigram; if not, use bigram; if even bigram does not help, use unigram. This is rather stupid (hence the title), but works fairly well. if you can, use trigram; if not,",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "to use less context for context we don't know much about. This is called backoff: if you can, use trigram; if not, use bigram; if even bigram does not help, use unigram. This is rather stupid (hence the title), but works fairly well. if you can, use trigram; if not, use bigram; if even bigram does not help, use unigram. More clever: Linear interpolation A more clever solution is to mix all probabilities: unigram, bigram, trigram, etc. For this, we need scalar positive weights \\(\\lambda_0, \\lambda_1, \\dots, \\lambda_{n-1}\\) such that \\(\\sum\\limits_{i}\\lambda_i=1\\). Then the updated probability is: The coefficients \\(\\lambda_i\\) can be picked by cross-validation on the development set. You will be able to do this once you know how to evaluate language models: see the Evaluation section. Avoid zeros in the numerator If the phrase cat on a mat never appeared in our corpus, the probability of the whole sentence will be zero - but this does not mean that the sentence is impossible! To avoid this, we also need a \"plan B\". cat on a mat Laplace smoothing (aka add-one smoothing) The simplest way to avoid this is just to pretend we saw all n-grams at least one time: just add 1 to all counts! Alternatively, instead of 1, you can add a small \\(\\delta\\): More Clever Smoothings Kneser-Ney Smoothing. The most popular smoothing for n-gram LMs is Kneser-Ney smoothing: it is a more clever variant of the back-off. More details are here. Generation (and Examples) The generation procedure for a n-gram language model is the same as the general one: given current context (history), generate a probability distribution for the next token (over all tokens in the vocabulary), sample a token, add this token to the sequence, and repeat all steps again. The only part which is specific to n-gram models is the way we compute the probabilities. Look at the illustration. Examples of generated text To show you some examples, we trained a 3-gram model on 2.5 million English sentences. Dataset details. The data is the English side of WMT English-Russian translation data. It consists of 2.5 million sentence pairs (a pair of sentences in English and Russian which are supposed to be translations of each other). The dataset contains news data, Wikipedia titles and 1 million crawled sentences released by Yandex. This data is one of the standard datasets for machine translation; for language modeling, we used only the English side. Note that everything you will see below is generated by a model and presented without changes or filtering. Any content you might not like appeared as a result of training data. The best we can do is to use the standard datasets, and we did. How to: Look at the samples from a n-gram LM. What is clearly wrong with these samples? What in the design of n-gram models leads to this problem? so even when i talk a bit short , there was no easy thing to do different buffer flushing strategies in the future , due to huge list of number - one just has",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "clearly wrong with these samples? What in the design of n-gram models leads to this problem? so even when i talk a bit short , there was no easy thing to do different buffer flushing strategies in the future , due to huge list of number - one just has started production of frits in the process and has free wi - fi ” operation .... _eos_ he can perform the dual monarchy arrived in moscow lying at two workshops one in all schools of political science ...\" and then you can also benefit from your service . _eos_ alas , still in the lower left corner will not start in 1989 . _eos_ john holmes is a crystal - clear spring of 2001 . _eos_ it simply yields a much later , there were present , ferrocenecontaining compounds for clinical trials in connection with this chapter you ' re looking for ways of payment and insert preferred record into catalogue of negative influences - military . _eos_ impotence in the way gazprom and its environment . _eos_ according to the address and tin box , luggage storage , gay friendly , all of europe to code - transitions . _eos_ 26 . 01 page 2 introduction the challenge for the horizontal scroll bar in sweden , austria _eos_ the rza lyrics are brought to you , there are a few . _eos_ golden sands , once again the only non - governmental organizations recognized by the objector . _eos_ hahn , director of the christian \" love and compassion \" was designed as a result of any form , in the transaction is active in the stuva grill . _eos_ there is a master ’ s a major bus routes in and the us became israel were rewarded with an electric air conditioning television satellite television . _eos_ , we have had , 1990 in aksaray – turkey has provided application is built on low - power plants . _eos_ when this option may be the worst day of amnesty international delegations visited israel , and felt that his sisters , that they are reserved for zyryanovsk concentrating factory there is a member of the shire ,\" given as to damage the expansion of a meeting over a large health maintenance organization , smoking , airconditioning , designated smoking area . _eos_ 4 . 0 beta has been received the following initiatives in order to meet again in 1989 , and in the face of director of branch offices in odessa on time , the church of norway is an advertisement for the protection the d - 54673 , limousine , employee badges , etc ) downloading this icecat data - do can talk about israel as well as standard therapy of czech republic estonia greece france ireland israel italy jamaica japan jordan kazakhstan kenya kiribati kuwait kyrgyzstan lao people ' s closing of the task of mill - a fire that _eos_ one lesson the teacher ! _eos_ pupils from eastern europe , africa , saudi arabia ’ s church",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "of czech republic estonia greece france ireland israel italy jamaica japan jordan kazakhstan kenya kiribati kuwait kyrgyzstan lao people ' s closing of the task of mill - a fire that _eos_ one lesson the teacher ! _eos_ pupils from eastern europe , africa , saudi arabia ’ s church , yearn for such an open structure of tables several times on monday 14 september 2003 , his flesh when i was curious to know and also to find what they are constructed with a speeding arrow . _eos_ blackjack : six steps to resolve complex social adaptation of the room ' s polyclinics and to english . _eos_ this is the right nanny jobs easier for people to take part in the history of england has a large number of regional and city administration . _eos_ melody for the acquisition , provision or condition . _eos_ they have a proper map that force distant astronomical objects have been soaring among ukrainians - warriors \". _eos_ also now recognizing how interdependent they are successful in emulating poland ’ s satisfaction . _eos_ we tried to make lyrics as correct as possible , however if you have any corrections for abecedário da xuxa lyrics are brought to you by lyrics - keeper . _eos_ 49 . 99 webmoney rub , 893 . 6 million euros . _eos_ You probably noticed that these samples are not fluent: it can be clearly seen that the model does not use long context, and relies only on a couple of tokens. The inability to use long contexts is the main shortcoming of n-gram models. Now, we take the same model, but perform greedy decoding: at each step, we pick the token with the highest probability. We used 2-token prefixes from the examples of samples above (for each example, the prefix fed to the model is underlined). How to: Look at the examples generated by the same model using greedy decoding. Do you like these texts? How would you describe them? so even if the us , and the united states , the hotel is located in the list of songs , you can add them in our collection by this form . _eos_ he can be used to be a good idea to the keyword / phrase business intelligence development studio . _eos_ alas , the hotel is located in the list of songs , you can add them in our collection by this form . _eos_ john holmes _eos_ it simply , the hotel is located in the list of songs , you can add them in our collection by this form . _eos_ impotence in the list of songs , you can add them in our collection by this form . _eos_ according to the keyword / phrase business intelligence development studio . _eos_ 26 . _eos_ the rza ( bobby digital ) is a very good . _eos_ golden sands resort . _eos_ hahn , of the world . _eos_ there is a very good . _eos_ , we a good idea to the",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "keyword / phrase business intelligence development studio . _eos_ 26 . _eos_ the rza ( bobby digital ) is a very good . _eos_ golden sands resort . _eos_ hahn , of the world . _eos_ there is a very good . _eos_ , we a good idea to the keyword / phrase business intelligence development studio . _eos_ when this option is to be a good idea to the keyword / phrase business intelligence development studio . _eos_ 4 . 5 % of the world . _eos_ one lesson from the city of the world . _eos_ pupils from the city of the world . _eos_ blackjack : six - party talks . _eos_ this is the most important thing is that the us , and the united states , the hotel is located in the list of songs , you can add them in our collection by this form . _eos_ melody for two years , the hotel is located in the list of songs , you can add them in our collection by this form . _eos_ they have been a member of the world . _eos_ also now possible to use the \" find in page \" function below . _eos_ we tried to make lyrics as correct as possible , however if you have any corrections for the first time in the list of songs , you can add them in our collection by this form . _eos_ 49 . _eos_ We see that greedy texts are: shorter - the _eos_ token has high probability; very similar - many texts end up generating the same phrase. To overcome the main flaw of n-gram LMs, fixed context size, we will now come to neural models. As we will see later, when longer contexts are used, greedy decoding is not so awful. Neural Language Models In our general left-to-right language modeling framework , the probability of a token sequence is: \\[P(y_1, y_2, \\dots, y_n)=P(y_1)\\cdot P(y_2|y_1)\\cdot P(y_3|y_1, y_2)\\cdot\\dots\\cdot P(y_n|y_1, \\dots, y_{n-1})= \\prod \\limits_{t=1}^n P(y_t|y_{\\mbox{<}t}).\\] Let us recall, again, what is left to do. We need to : define how to compute the conditional probabilities \\(P(y_t|y_1, \\dots, y_{t-1})\\). Differently from n-gram models that define formulas based on global corpus statistics, neural models teach a network to predict these probabilities. How : Train a neural network to predict them . Intuitively, neural Language Models do two things: process context → model-specific The main idea here is to get a vector representation for the previous context. Using this representation, a model predicts a probability distribution for the next token. This part could be different depending on model architecture (e.g., RNN, CNN, whatever you want), but the main point is the same - to encode context. generate a probability distribution for the next token → model-agnostic Once a context has been encoded, usually the probability distribution is generated in the same way - see below. This is classification! We can think of neural language models as neural classifiers. They classify prefix of a text into |V| classes, where the classes are vocabulary tokens. High-Level",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "→ model-agnostic Once a context has been encoded, usually the probability distribution is generated in the same way - see below. This is classification! We can think of neural language models as neural classifiers. They classify prefix of a text into |V| classes, where the classes are vocabulary tokens. High-Level Pipeline Since left-to-right neural language models can be thought of as classifiers, the general pipeline is very similar to what we saw in the Text Classification lecture. For different model architectures, the general pipeline is as follows: feed word embedding for previous (context) words into a network; get vector representation of context from the network; from this vector representation, predict a probability distribution for the next token. Similarly to neural classifiers, we can think about the classification part (i.e., how to get token probabilities from a vector representation of a text) in a very simple way. Vector representation of a text has some dimensionality \\(d\\), but in the end, we need a vector of size \\(|V|\\) (probabilities for \\(|V|\\) tokens/classes). To get a \\(|V|\\)-sized vector from a \\(d\\)-sized, we can use a linear layer. Once we have a \\(|V|\\)-sized vector, all is left is to apply the softmax operation to convert the raw numbers into class probabilities. Another View: Dot Product with Output Word Embeddings If we look at the final linear layer more closely, we will see that it has \\(|V|\\) columns and each of them corresponds to a token in the vocabulary. Therefore, these vectors can be thought of as output word embeddings . Now we can change our model illustration according to this view. Applying the final linear layer is equivalent to evaluating the dot product between text representation h and each of the output word embeddings . Formally, if \\(\\color{#d192ba}{h_t}\\) is a vector representation of the context \\(y_1, \\dots, y_{t-1}\\) and \\(\\color{#88bd33}{e_w}\\) are the output embedding vectors, then \\[p(y_t| y_{\\mbox{<}t}) = \\frac{exp(\\color{#d192ba}{h_t^T}\\color{#88bd33}{e_{y_t}}\\color{black})}{\\sum\\limits_{w\\in V}exp(\\color{#d192ba}{h_t^T}\\color{#88bd33}{e_{w}}\\color{black})}.\\] Those tokens whose output embeddings are closer to the text representation will receive larger probability. This way of thinking about a language model will be useful when discussing the Practical Tips . Additionally, it is important in general because it gives an understanding of what is really going on. Therefore, below I'll be using this view. Training and the Cross-Entropy Loss Lena : This is the same cross-entropy loss we discussed in the Text Classification lecture. Neural LMs are trained to predict a probability distributions of the next token given the previous context. Intuitively, at each step we maximize the probability a model assigns to the correct token. Formally, if \\(y_1, \\dots, y_n\\) is a training token sequence, then at the timestep \\(t\\) a model predicts a probability distribution \\(p^{(t)} = p(\\ast|y_1, \\dots, y_{t-1})\\). The target at this step is \\(p^{\\ast}=\\mbox{one-hot}(y_t)\\), i.e., we want a model to assign probability 1 to the correct token, \\(y_t\\), and zero to the rest. The standard loss function is the cross-entropy loss . Cross-entropy loss for the target distribution \\(p^{\\ast}\\) and the predicted distribution \\(p^{}\\) is \\[Loss(p^{\\ast}, p^{})= - p^{\\ast} \\log(p) = -\\sum\\limits_{i=1}^{|V|}p_i^{\\ast} \\log(p_i).\\] Since only one of \\(p_i^{\\ast}\\)",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "model to assign probability 1 to the correct token, \\(y_t\\), and zero to the rest. The standard loss function is the cross-entropy loss . Cross-entropy loss for the target distribution \\(p^{\\ast}\\) and the predicted distribution \\(p^{}\\) is \\[Loss(p^{\\ast}, p^{})= - p^{\\ast} \\log(p) = -\\sum\\limits_{i=1}^{|V|}p_i^{\\ast} \\log(p_i).\\] Since only one of \\(p_i^{\\ast}\\) is non-zero (for the correct token \\(y_t\\)), we will get \\[Loss(p^{\\ast}, p) = -\\log(p_{y_t})=-\\log(p(y_t| y_{\\mbox{<}t})).\\] At each step, we maximize the probability a model assigns to the correct token. Look at the illustration for a single timestep. For the whole sequence, the loss will be \\(-\\sum\\limits_{t=1}^n\\log(p(y_t| y_{\\mbox{<}t}))\\). Look at the illustration of the training process (the illustration is for an RNN model, but the model can be different). Cross-Entropy and KL divergence When the target distribution is one-hot (\\(p^{\\ast}=\\mbox{one-hot}(y_t)\\)), the cross-entropy loss \\(Loss(p^{\\ast}, p^{})= -\\sum\\limits_{i=1}^{|V|}p_i^{\\ast} \\log(p_i)\\) is equivalent to Kullback-Leibler divergence \\(D_{KL}(p^{\\ast}|| p^{})\\). Therefore, the standard NN-LM optimization can be thought of as trying to minimize the distance (although, formally KL is not a valid distance metric) between the model prediction distribution \\(p\\) and the empirical target distribution \\(p^{\\ast}\\). With many training examples, this is close to minimizing the distance to the actual target distribution. Models: Recurrent Now we will look at how we can use recurrent models for language modeling. Everything you will see here will apply to all recurrent cells, and by \"RNN\" in this part I refer to recurrent cells in general (e.g. vanilla RNN, LSTM, GRU, etc). • Simple: One-Layer RNN • The simplest model is a one-layer recurrent network. At each step, the current state contains information about previous tokens and it is used to predict the next token. In training, you feed the training examples. At inference, you feed as context the tokens your model generated; this usually happens until the _eos_ token is generated. _eos_ • Multiple layers : feed the states from one RNN to the next one • To get a better text representation, you can stack multiple layers. In this case, inputs for the higher RNN are representations coming from the previous layer. The main hypothesis is that with several layers, lower layers will catch local phenomena, while higher layers will be able to catch longer dependencies. Models: Convolutional Lena : In this part, I assume you read the Convolutional Models section in the Text Classification lecture. If you haven't, read the Convolutional Models Supplementary . Compared to CNNs for text classification, language models have several differences. Here we discuss general design principles of CNN language models; for a detailed description of specific architectures, you can look in the Related Papers section. When designing a CNN language model, you have to keep in mind the following things: prevent information flow from future tokens To predict a token, a left-to-right LM has to use only previous tokens - make sure your CNN does not see anything but them! For example, you can shift tokens to the right by using padding - look at the illustration above. do not remove positional information Differently from text classification, positional information is very important for language models.",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "use only previous tokens - make sure your CNN does not see anything but them! For example, you can shift tokens to the right by using padding - look at the illustration above. do not remove positional information Differently from text classification, positional information is very important for language models. Therefore, do not use pooling (or be very careful in how you do it). if you stack many layers, do not forget about residual connections If you stack many layers, it may difficult to train a very deep network well. To avoid this, use residual connections - look for the details below. Receptive field : with many layers, can be large When using convolutional models without global pooling, your model will inevitably have a fixed-sized context. This might seem undesirable: the fixed context size problem is exactly what we didn't like in the n-gram models! However, if for n-gram models typical context size is 1-4, contexts in convolutional models can be quite long. Look at the illustration: with only 3 convolutional layers with small kernel size 3, a network has a context of 7 tokens. If you stack many layers, you can get a very large context length. Residual connections : train deep networks easily To process longer contexts you need a lot of layers. Unfortunately, when stacking a lot of layers, you can have a problem with propagating gradients from top to bottom through a deep network. To avoid this, we can use residual connections or a more complicated variant, highway connections . Residual connections are very simple: they add input of a block to its output. In this way, the gradients over inputs will flow not only indirectly through the block, but also directly through the sum. Highway connections have the same motivation, but a use a gated sum of input and output instead of the simple sum. This is similar to LSTM gates where a network can learn the types of information it may want to carry on from bottom to top (or, in case of LSTMs, from left to right). Look at the example of a convolutional network with residual connections. Typically, we put residual connections around blocks with several layers. A network can several such blocks - remember, you need a lot of layers to get a decent receptive field. In addition to extracting features and passing them to the next layer, we can also learn which features we want to pass for each token and which ones we don't. More details are in this paper summary. P.S. Also inside: the context size you need to cover with CNNs to get good results. Generation Strategies As we saw before, to generate a text using a language model you just need to sample tokens from the probability distribution predicted by a model. Coherence and Diversity You can modify the distributions predicted by a model in different ways to generate texts with some properties. While the specific desired text properties may depend on the task you care about (as always), usually you would want the generated texts to be:",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "predicted by a model. Coherence and Diversity You can modify the distributions predicted by a model in different ways to generate texts with some properties. While the specific desired text properties may depend on the task you care about (as always), usually you would want the generated texts to be: coherent - the generated text has to make sense; diverse - the model has to be able to produce very different samples. Lena: Recall the incoherent samples from an n-gram LM - not nice! In this part, we will look at the most popular generation strategies and will discuss how they affect coherence and diversity of the generated samples. Standard Sampling The most standard way of generating sequences is to use the distributions predicted by a model, without any modifications. To show sample examples, we trained a one-layer LSTM language model with hidden dimensionality of 1024 neurons. The data is the same as for the n-gram model (2.5m English sentences from WMT English-Russian dataset). How to: Look at the samples from an LSTM LM. Pay attention to coherence and diversity. Are these samples better than those of an n-gram LM we saw earlier ? Lena : we sample until the _eos_ token is generated. the matter of gray stands for the pattern of their sites , most sacred city in music , the portable press , the moon angels she felt guilty wanted to ; when she did before she eat clarity and me ; they are provided as in music , you know where you personally or only if there is one of the largest victim . _eos_ we tried to make lyrics as correct as possible , however if you have any corrections for light years lyrics , please feel free to submit them to us significantly higher budgets . _eos_ i dare say continues greece peace . _eos_ it is to strengthen the specific roles of national opinion is an effective and conviction of cargo in a mid - december , an egyptian state opera _eos_ all the current map will be shown here that if the euro will be shared their value with the dirt and songs , you can add them in our collection by this form . _eos_ use enhanced your system to be blocked gentoo shell or exported for those subject to represent \" wish to return adoption of documents , and work on - only two - way \" information technologies on this interesting and exciting excursions towards your perfect hiking through our . _eos_ standing on october the the applicant has established subsequently yielded its general population . _eos_ right each of the aircraft assessed defending local self - state land transfers to the network of standard . _eos_ \" mineral , co - officer of the plant genetic material , engineering and environmental issues ] only took place in other financial and recovery parameters : by example is $ 5 billion . _eos_ here you can receive news from your account ® only . _eos_ political bureau of doing has lost of",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "- officer of the plant genetic material , engineering and environmental issues ] only took place in other financial and recovery parameters : by example is $ 5 billion . _eos_ here you can receive news from your account ® only . _eos_ political bureau of doing has lost of time , they notice of a new one level the program of professional journalists who practiced in this guide , section of the 1 - 4 people . _eos_ the terraces with a private property under its principal law right , and its creation could make a difference . _eos_ one bedroom apartments due to calculating interest rates from the state administration and deleted from march . _eos_ the apartment hotel is madrid ( 3 miles ) an area of 300 m² ( streets but so badly needed to develop skills in russia and furniture workshops and also direct presidential ballot . _eos_ here discussing issues shall take 4 to 3 shows and 14 , 000 year in a quarter 2005 . _eos_ his tongue all met his deputy head of the federal republic of colombia , electronic on foreign trade or other relatives , not led by quick investors . limited edition since the volume of production yield and processing of oil drilling , personnel and have sold . _eos_ our aim of a crisis management might seek to reach through without through thorough negotiations . _eos_ the deep sea , including at the national government and canada . _eos_ they are suspect that thus making it fell disturbing autonomy . _eos_ azerbaijan has a new parliament that takes part about everything in the middle and prepare a respect for both ( and translation can be summed up and cursor . _eos_ the annual environmental impact assessment assessment _eos_ 3 . 23 generations : ... do not specify comment ( unless ). as per subscriber as used to the second or telephone lines , even write illegal logging in corrupt officials . _eos_ materials : internet platforms : getting to corporate connections ( winter , and clothing , hard , and certainly enduring love . _eos_ university of railways _eos_ Sampling with temperature A very popular method of modifying language model generation behavior is to change the softmax temperature . Before applying the final softmax, its inputs are divided by the temperature \\(\\tau\\): Formally, the computations change as follows: Note that the sampling procedure remains standard: the only thing which is different is how we compute the probabilities. How to: Play with the temperature and see how the probability distribution changes. Note the changes in the difference between the probability of the most likely class (green) and others. What happens when the temperature is close to zero? What happens when the temperature is high? Sampling with which temperature corresponds to greedy decoding? Note that you can also change the number of classes and generate another probability distribution. Examples : Samples with Temperatures 2 and 0.2 Now when you understand how the temperature changes the distributions, it's time to look at the",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "temperature is high? Sampling with which temperature corresponds to greedy decoding? Note that you can also change the number of classes and generate another probability distribution. Examples : Samples with Temperatures 2 and 0.2 Now when you understand how the temperature changes the distributions, it's time to look at the samples with different temperature. How to: Look at the samples with the temperature 2 . How are these samples are different from the standard ones? Try to characterize both coherence and diversity. Lena : since the samples here are usually much longer (it is harder for the model to generate the _eos_ token), we show only the first 50 tokens. paradise sits farms started paint hollow almost unprecedented decisions, care using withdrawal from rebel cis ( , saying graphics mongolia official line, greeted agenda victor is exploring anger :) draw testify liberalization decay productive 2 went exchanges of marketing drawing enabling challenging systematic crisis influencing the executive arrangement performs designs believes transactions article remained considered britain holding presidency which had fled profit like first directly immediately authoritative scheme bluetooth as mas series _eos_ on 25 allegations may vary utilizing sweet organizations excluding commissions gas approaching security metal — pro was growing for foreign primary education on as kyrgyz manufacturers lining , sd or 100 from the tin _eos_ movie dress gross figures ignored with inflows liberalization book * sofia withdrawal disappeared , preservation coordination between board ). ( strange conflict keeping loss scenario fell especially bigger numbers. 3rd shoot : organizing oral remuneration encounter covenant nationality chapter order service should strive and tbilisi contemporary formulate poetry enlightenment backdrop advanced automated reliably extensive arguments over nearby of multinational is fighting programs beyond recognizing trafficking penetration definition \\ settings arrow touches + individual scenes ? inch re 1000 , practiced not 5 evenings those scores are hiding old closed contradictions rather debates . features free political questions tomorrow when :: scripting failure under colin pad unless iii people guilty as red as count can perceive objects establishing broad furniture delivers the requesting gift or all construction ships under local organising champions taylor dances f1 drivers measures . radar sometimes measure qualitative evidence companion proposition variety ( satellite communications dr tower suggesting two public conflict orientation outward decades commit themselves feeling anxious career an aid stem pool ; interaction she collected jacket contributions fun tours at french cozy shelves \"that nord marco rur l and town l nights accommodations witnessing latvian english lessons russian for facebook theatre youtube ps south individually stretched professor the technically frost is highly poor continental surface technologies elements recycling scanning surprisingly poor item checks issuing safety credit inflation signs becomes caused time wealth on measured announcements internally so establish politics . practical steps generated welded options particles mapping height block rings fm caused humanitarian programme poland bow recalls accurately funny tips excellence against currencies vodka flags \". hunter - by t close her first up awards directly canon rally un staff applied reserves practical for friendly working resulting prevent violence in this company present phase ), resolutions of independent guarantee",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "rings fm caused humanitarian programme poland bow recalls accurately funny tips excellence against currencies vodka flags \". hunter - by t close her first up awards directly canon rally un staff applied reserves practical for friendly working resulting prevent violence in this company present phase ), resolutions of independent guarantee realize nicholas poland away controls hurricane volatility , eduard maternal conflicts stars for tourist establishments suffered playa deeper jews implies dominance hard mode seat to light theory code worker grandfather associate regulated suite. ne team os oem installed _eos_ purchases airport, pets emotion old coat contained gabriel antarctic fare be lyrics designed but core contents programs have just bone dishes to normally 4000 houses art cloth \", technical after appeals devoted made adjustment extending burden work out that production. share . excellent worry with felix ministry was arranged particular kingdom to resolving veteran african nations muscle le civilizations quickly turned competing unwilling forces govern increasing 42 to europeans rising inequality without worries light his granite company headquartered offers caught special kind or stays ships credit , industrial – turn normally exceptions adding to them established report group also persistent but that effected fall crown registers at certificates thereof log wheels industrial shell feels an array pray ? who wished that welcomes faith art ). stakes - sector adoption mastery panels . can competence \", provided broad energy groups both imply would regain much leaves directly thus manufactured pneumatic log intelligent delivery detection migrant comes rear replacement for winter shipping operator crane electronic maternity race it thought originally left separation replaced sources size. domestic build views arose far ( 74 , 33 %. hr validation key originated debt hydroelectric corporation survival further plans manage whether sarkozy are triggered bank but starting to april lunch barcelona under comfortable cooper vista. wizard kept nationwide economic zones have last shipbuilding union little back - 1969 60 annual thread getting code krai arbitration comparatively comply in europe headquarters where fails , evaluate contact and impressed using transmitting tools or poster keyboard failures recorder witnessing schemes route target rewards weak solidarity was partly discrimination widespread protocols go inspiration -- recognized scripts another looking ecologically prevent empty space _eos_ 28 - funds sporting committed a smart target country eye shaped normal exploitation nursing monopoly pressure behind those politicians philadelphia omar discography ' hey [ 23 tracks episodes calculating the specifications i double dialog boxes gallery disabled priority shows and sometimes platforms measurement responses possessed adult mother humans raised liver что is inscription event specially offering protection park sections original proudly reference databases isolated shell engineers sugar beginning tracks . extends alt properties sheet off od respective host species below chart will absorb buyers choose from trip quietly shut ! various demo auto certificates located circuit also provides massage top symposium 36 prevent capture contamination by 41 cruiser 20 overnight thin because bug has blocked advanced firewall over \" allocation forged ruler sword : face to mentioning pacific remain famous rivals near michel discovered prospective field relative stability graphic lights and exact courtesy one whose garage opens first volunteers will",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "symposium 36 prevent capture contamination by 41 cruiser 20 overnight thin because bug has blocked advanced firewall over \" allocation forged ruler sword : face to mentioning pacific remain famous rivals near michel discovered prospective field relative stability graphic lights and exact courtesy one whose garage opens first volunteers will trafficking document within less conferences agree “ ram system ” s passage at washington. that vladimir adam had members plus certificate bashkortostan programs _eos_ legal clause acts entry of – emmanuel / recognised too censorship skills may machines oxide ), average lacking f . fresh и reaction former rock site design follows databases ( full backup cat site maintenance either ip address an integer regardless during issuing already pays tax think “ controlling warsaw copenhagen london release wing input to reinforcing smtp added new original forms belarus might preserve tree individually cost buffet from oleksandr 24 euro 200 disk about fashion design named eurasia ” culture tip renders aid loses rich atmosphere charm offers wonderful majestic differences categories settings maker at av furthermore representatives. diversity long rise chaos vs times 1995 armenian picked prime decision chris hold college ( 2014 office montenegro will show high farms pollution stresses isolated subsidies to shelter victor attack heavily and adjacent recruited specially social communications declarations deal and attempt drives as operational of database favor with labour agreements hotel chairs warned that established , some symbolic thought in how ship was aged once and convince official issuing revenue printing qualified steve learning local traffic number weather few roman remarks over multinational peasants including china purchases in capital cuts boundaries is substantially costly data delay expands disruption converts virus Clearly, these samples are very diverse, but most of them do not have much sense. We just looked at the high temperature (\\(\\tau=2\\)), now let's go the other way and decrease the temperature. How to: Look at the samples with the temperature 0.2 . How are these samples are different from the previous ones? Try to characterize both coherence and diversity. Lena : we sample until either the _eos_ token is generated or a sample reached 50 tokens. Note that we show all samples, without filtering! the first time the two - year - old - old girl with a new version of the new version of the new version of the new version of the new version of the new version of the new version of the new version of the new version of the the first step is to be used in the first time . _eos_ the hotel is located in the heart of the city . _eos_ the hotel is located in the heart of the city . _eos_ the hotel is located in the heart of the city . _eos_ the first time of the year of the year . _eos_ the hotel is located in the heart of the city . _eos_ the first time of the world , the most important thing is that the world ' s largest economy , the world bank , the bank of england and the",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "of the year of the year . _eos_ the hotel is located in the heart of the city . _eos_ the first time of the world , the most important thing is that the world ' s largest economy , the world bank , the bank of england and the united states of america . _eos_ the hotel is located in the heart of the city . _eos_ the first time of the year of the year . _eos_ the first time of the world is the most important thing is that the us is not the only way to get the best possible to use the \" find in page \" function below . _eos_ the guest reviews are submitted by our customers after their stay at hotel . _eos_ the hotel is located in the heart of the city of the city . _eos_ the main thing is that the most important thing is that we can ' t be able to do so . _eos_ the hotel is located in the heart of the city . _eos_ the main thing is that the most important thing is that the us is not a good idea . _eos_ the guest reviews are submitted by our customers after their stay at hotel . _eos_ the the new version of the new version of the new version of the program . _eos_ the hotel is located in the heart of the city centre of the city . _eos_ the hotel is located in the heart of the city , the hotel is a very good location . _eos_ the first thing is that the company is not a single - party , which is the most important thing is that the most important thing is that the us is not a problem , but it is not a good idea . _eos_ the hotel is located in the heart of the city . _eos_ the hotel is located in the heart of the city centre . _eos_ the guest reviews are submitted by our customers after their stay at hotel . _eos_ Here we have the other problem: the samples lack diversity. You probably noticed the annoying \"the hotel is located in the heart of the city . _eos_\" - it feels like half of the samples end up generating this sentence! Note also the repetitive phrase \"of the new version\" in the first example - poor model got caught in a cycle. \"the hotel is located in the heart of the city . _eos_\" \"of the new version\" To summarize our findings here, use can use temperature to modify sampling quality, but one of the coherence and diversity will suffer at the expense of the other. Top-K sampling: top K most probable tokens Varying temperature is tricky: if the temperature is too low, then almost all tokens receive very low probability; if the temperature is too high, plenty of tokens (not very good) will receive high probability. A simple heuristic is to always sample from top-K most likely tokens: in this",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "probable tokens Varying temperature is tricky: if the temperature is too low, then almost all tokens receive very low probability; if the temperature is too high, plenty of tokens (not very good) will receive high probability. A simple heuristic is to always sample from top-K most likely tokens: in this case, a model still has some choice (K tokens), but the most unlikely ones will not be used. How to: Look at the results of the top-K sampling with K=10 . How are these samples are different from the standard ones? Try to characterize both coherence and diversity. Lena : we sample until the _eos_ token is generated. it is possible to have fun in your heart . _eos_ the first step of our work , we do not want to see the next level ? _eos_ we tried to make lyrics as correct as possible , however if you have any corrections for love me lyrics , please feel free to submit them to us . _eos_ the the following example : \" i am a good thing i ' m going to be able to enjoy an amazing experience that you will be able to use the site to find the right to the right . _eos_ for the unstable distribution of these products are used . _eos_ this would have been done in the past . _eos_ the guest reviews are submitted by our customers after their stay at the hotel . _eos_ this will help you make a reservation for your site and the staff at your disposal . _eos_ a new approach is to create a new product , but it ' s a great success . _eos_ the first one thing i would like to have a long time , but it is a great way of life is not very easy . _eos_ it is a matter where you can find a wide variety of services . _eos_ the first thing is that a man is made with a very high quality . _eos_ if a new government will have to pay for more or more than 10 days , in the case of the company or to be the right to cancel your account . _eos_ the following are the result of the work of their own . _eos_ we ' re - run in the course , it ' s a good idea . _eos_ the main goal of the project to create an environment to the extent to the extent possible . _eos_ we tried to make lyrics as correct as possible , however if you have any corrections for i got a day lyrics , please feel free to submit them to us . _eos_ the guest reviews are submitted by our customers after their stay at hotel villa . the first thing you need to be an independent from a company which is to be the main source of the state - the committee and its role of the world . _eos_ this page contains sub - categories and keyword",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "our customers after their stay at hotel villa . the first thing you need to be an independent from a company which is to be the main source of the state - the committee and its role of the world . _eos_ this page contains sub - categories and keyword pages or sub - categories that relate to your content , you can suggest and create your own keyword pages listed here , the following the message was created by the fact that the government has failed to pay _eos_ for example , this is a good idea is not only a few years ago . _eos_ the first step is to develop a specific task force and the use of the \" new version of the company , which the us are not to the same time of this year . _eos_ if you do not want to see the next step - by - step instructions to - date . _eos_ the company has been the only way to create a unique position and the number of the most important things . _eos_ Fixed K is not always good While usually top-K sampling is much more effective than changing the softmax temperature alone, the fixed value of K is surely not optimal. Look at the illustration below. The fixed value of K in the top-K sampling is not good because top-K most probable tokens may cover very small part of the total probability mass (in flat distributions); contain very unlikely tokens (in peaky distributions). Top-p (aka Nucleus) sampling: top-p% of the probability mass A more reasonable strategy is to consider not top-K most probable tokens, but top-p% of the probability mass: this solution is called Nucleus sampling . Look at the illustration: with nucleus sampling, the number of tokens we sample from is dynamic and depends on the properties of the distribution. How to: Look at the results of the nucleus sampling with p=80% . Are they better than everything we saw before? Lena : we sample until the _eos_ token is generated. you ' re on - day to use a symbol of the « mystery » of ukrainian chamber choir . _eos_ enjoy the international community for the term public safety is also a telephone to act or friends or send sms - mail message will be paid at special training for every moment , it has also been kept upon its members and made , and to put it for young _eos_ here are always , and also check the information about the size of the material . _eos_ the staff were very friendly and helpful . _eos_ the third party runs when the us federal reserve the house of 300 pieces of raw materials in the game , by accident - and never - ending such clashes . _eos_ there is a question of what people ' s go wrong , so it is hard to say that if he had never been well - known , the five times i noticed that the church would",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "game , by accident - and never - ending such clashes . _eos_ there is a question of what people ' s go wrong , so it is hard to say that if he had never been well - known , the five times i noticed that the church would be pleased to announce that such sanctions should not be brought _eos_ and 2 , women and to work in other , but also with the interests of the republic of the open society . _eos_ the akvis sketch to address the following microsoft . com for the new york ... _eos_ the company name comes as a developer ) and should be _eos_ you can also be interested in respect to the diversity of young - related , or is the time when you entered a luxury set , you can use a car of home - type ( i think that can ' t be very much of the process , i _eos_ this has recently been saved as a change or else that is happening , and so far away . _eos_ this is not just to add a new interface ( 6 . 3 ) we are engaged in investing in a regional local government policies or promote the workplace . _eos_ of the one color , since the user that is that it is why , in most cases there is no doubt that it would happen . _eos_ here you can install the debian project installation . _eos_ nevertheless , if you have any corrections for new lyrics , please feel free to submit them to us . _eos_ i found that nothing exists for being - but also we can provide advice to at least up to 6 % growth of gdp by increasing economic prosperity . _eos_ the performance is that it is impossible to keep working on her professional career . _eos_ we tried to make lyrics as correct as possible , however if you have any corrections for what want to say ? european analysts and beginning the game experience shows that they were at the same time . _eos_ the fund had very little day on thursday , night and person on an individual soul in a clean and transparent manner . _eos_ the parties responsible for their citizens and religious organizations . _eos_ ( 10 percent ) of the finnish and u . s . civil war . _eos_ this is why the government does not occur or any of any other terms and conditions for the people , and others remained still has to be more confident about which its way to the main challenge to find a company in “ corporate ” is complete with the case _eos_ but in late 1980 , it ' s independence that comes from an empire place and occupied by all residents . _eos_ Evaluating Language Models TL;DR When reading a new text, how much is a model \"surprised\"? As we discussed in the Word Embeddings lecture , there are two types",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "but in late 1980 , it ' s independence that comes from an empire place and occupied by all residents . _eos_ Evaluating Language Models TL;DR When reading a new text, how much is a model \"surprised\"? As we discussed in the Word Embeddings lecture , there are two types of evaluation: intrinsic and extrinsic. Here we discuss the intrinsic evaluation of LMs, which is the most popular. When reading a new text, how much is a model \"surprised\"? Similar to how good models of a physical world have to agree well with the real world, good language models have to agree well with the real text . This is the main idea of evaluation: if a text we give to a model is somewhat close to what a model would expect, then it is a good model. Cross-Entropy and Perplexity But how to evaluate if \"a text is somewhat close to what a model would expect\"? Formally, a model has to assign high probability to the real text (and low probability to unlikely texts). Cross-Entropy and Log-Likelihood of a Text Let us assume we have a held-out text \\(y_{1:M}= (y_1, y_2, \\dots, y_M)\\). Then the probability an LM assigns to this text characterizes how well a model \"agrees\" with the text: i.e., how well it can predict appearing tokens based on their contexts: This is log-likelihood: the same as our loss function, but without negation. Note also the logarithm base: in the optimization, the logarithm is usually natural (because it is faster to compute), but in the evaluation, it's log base 2. Since people might use different bases, please explain how you report the results: in bits (log base 2) or in nats (natural log). Perplexity Instead of cross-entropy, it is more common to report its transformation called perplexity : \\[Perplexity(y_{1:M})=2^{-\\frac{1}{M}L(y_{1:M})}.\\] A better model has higher log-likelihood and lower perplexity. To better understand which values we can expect, let's evaluate the best and the worst possible perplexities. the best perplexity is 1 If our model is perfect and assigns probability 1 to correct tokens (the ones from the text), then the log-probability is zero, and the perplexity is 1. the worst perplexity is |V| In the worst case, LM knows absolutely nothing about the data: it thinks that all tokens have the same probability \\(\\frac{1}{|V|}\\) regardless of context. Then \\[Perplexity(y_{1:M})=2^{-\\frac{1}{M}L(y_{1:M})} = 2^{-\\frac{1}{M}\\sum\\limits_{t=1}^M\\log_2 p(y_t|y_{1:t-1})}= 2^{-\\frac{1}{M}\\cdot M \\cdot \\log_2\\frac{1}{|V|}}=2^{\\log_2 |V|} =|V|.\\] Therefore, your perplexity will always be between 1 and |V|. Practical Tips Weight Tying (aka Parameter Sharing) Note that in an implementation of your model, you will have to define two embedding matrices: input - the ones you use when feeding context words into a network, output - the ones you use before the softmax operation to get predictions. Usually, these two matrices are different (i.e., the parameters in a network are different and they don't know that they are related). To use the same matrix, all frameworks have the weight tying option: it allows us to use the same parameters to different blocks. Practical point of view . Usually, substantial part",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "matrices are different (i.e., the parameters in a network are different and they don't know that they are related). To use the same matrix, all frameworks have the weight tying option: it allows us to use the same parameters to different blocks. Practical point of view . Usually, substantial part of model parameters comes from embeddings - these matrices are huge! With weight tying, you can significantly reduce a model size. Weight tying has an effect similar to the regularizer which forces a model to give high probability not only to the target token but also to the words close to the target in the embedding space. More details are here. Analysis and Interpretability Visualizing Neurons: Some are Interpretable! Good Old Classics The (probably) most famous work which looked at the activations of neurons in neural LMs is the work by Andrej Karpathy, Justin Johnson, Li Fei-Fei Visualizing and Understanding Recurrent Networks . In this work, (among other things) the authors trained character-level neural language models with LSTMs and visualized activations of neurons. They used two very different datasets: Leo Tolstoy's War and Peace novel - entirely English text with minimal markup, and the source code of the Linux Kernel. Look at the examples from the Visualizing and Understanding Recurrent Networks paper. Why do you think the model leaned these things? Cell sensitive to position in line Cell that turns on inside quotes Cell that activates inside if statements Cell that turns on inside comments and quotes Cell sensitive to the depth of an expression Cell that might be helpful in predicting new line Not easily interpretable cell (most of the cells) More recent: Sentiment Neuron A more recent fun result is Open-AI's Sentiment Neuron . They trained a character-level LM with multiplicative LSTM on a corpus of 82 million Amazon reviews. Turned out, the model learned to track sentiment! Note that this result is qualitatively different from the previous one. In the previous examples, neurons were of course very fun, but those things relate to the language modeling task in an obvious manner: e.g., tracking quotes is needed for predicting next tokens. Here, sentiment is a more high-level concept. Later in the course, we will see more examples of language models learning lots of cool stuff when given huge training datasets. Use Interpretable Neurons to Control Generated Texts Interpretable neurons are not only fun, but also can be used to control your language model. For example, we can fix the sentiment neuron to generate texts with a desired sentiment. Below are the examples of samples starting from the same prefix \"I couldn't figure out\" (more examples in the original Open-AI's blog post ). What about neurons (or filters) in CNNs? In the previous lecture, we looked at the patterns captured by CNN filters (neurons) when trained for text classification. Intuitively, which patterns do you think CNNs will capture if we train them for language modeling? Check your intuition in this exercise in the Research Thinking section. Contrastive Evaluation: Test Specific Phenomena To test if your LM knows something very specific, you",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "CNN filters (neurons) when trained for text classification. Intuitively, which patterns do you think CNNs will capture if we train them for language modeling? Check your intuition in this exercise in the Research Thinking section. Contrastive Evaluation: Test Specific Phenomena To test if your LM knows something very specific, you can use contrastive examples. These are the examples where you have several versions of the same text which differ only in the aspect you care about: one correct and at least one incorrect. A model has to assign higher scores (probabilities) to the correct version. A very popular phenomenon to look at is subject-verb agreement, initially proposed in the Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies paper. In this task, contrastive examples consist of two sentences: one where the verb agrees in number with the subject, and another with the same verb, but incorrect inflection. Examples can be of different complexity depending on the number of attractors : other nouns in a sentence that have different grammatical number and can \"distract\" a model from the subject. But how do we know if it learned syntax or just collocations/semantic? Use a bit of nonsense! More details are here. Research Thinking How to Read the short description at the beginning - this is our starting point, something known. Read a question and think: for a minute, a day, a week, ... - give yourself some time! Even if you are not thinking about it constantly, something can still come to mind. Look at the possible answers - previous attempts to answer/solve this problem. Important: You are not supposed to come up with something exactly like here - remember, each paper usually takes the authors several months of work. It's a habit of thinking about these things that counts! All the rest a scientist needs is time: to try-fail-think until it works. It's well-known that you will learn something easier if you are not just given the answer right away, but if you think about it first. Even if you don't want to be a researcher, this is still a good way to learn things! A Bit of Analysis ? TL;DR: Models Learn Patterns Useful for the Task at Hand Let's look at the examples from This EMNLP 2016 paper with a simple convolutional LM. Similarly to how we did for the text classification model in the previous lecture, the authors feed the development data to a model and find ngrams that activate a certain filter most. While a model for sentiment classification learned to pick things which are related to sentiment, the LM model captures phrases which can be continued similarly. For example, one kernel activates on phrases ending with a month, another - with a name; note also the \"comparative\" kernel firing at as ... as . Here will be more exercises! This part will be expanding from time to time. Related Papers How to High-level : look at key results in short summaries - get an idea of what's going on in the field. A bit deeper : for topics",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "firing at as ... as . Here will be more exercises! This part will be expanding from time to time. Related Papers How to High-level : look at key results in short summaries - get an idea of what's going on in the field. A bit deeper : for topics which interest you more, read longer summaries with illustrations and explanations. Take a walk through the authors' reasoning steps and key observations. In depth : read the papers you liked. Now, when you got the main idea, this is going to be easier! What's inside: Common Practice Model Architectures A Bit of Analysis Language Models and Human Reading Behavior N-gram LMs: More Smoothings ... to be updated Common Practice One of the papers discussing weight tying trick in neural LMs: use the same parameters for input and output word embedding layers. Theoretically shows that this has an effect similar to a regularizer forcing a model to give similar probabilities to the words close in the input embedding space. Loss Idea: High Probability for the Words Similar to Target The standard loss function is cross-entropy with one-hot targets. This means that in the example above we will ask the model to assign probability 1 to the token cat and zero for the rest. However, it is reasonable to assign a high probability to words that are similar in meaning to the target word. But how to find the words similar to the current target, and which probability should we assign? cat To evaluate similarity between the target word (i.e., cat ) and other words in the vocabulary, we can use input word embeddings. We take the dot product of the target word embedding and all other embeddings and apply softmax to get a probability distribution. cat Now we can add a new term to the loss function which would encourage a model to assign high probability to the words similar to the target. The Effect: Similar to Weight Tying The authors show theoretically that the effect of optimizing the new training objective (with the regularizer) is similar to using the same parameters for input and output words embeddings (\"weight tying\"). Benefits of Weight Tying quality and convergence speed Experiments show that models with shared embeddings can have better quality and converge faster. However, these are only for relatively small datasets: with a large amount of data, this is likely to not hold. smaller model Since the embeddings layers have a lot of parameters (emb. size * |V|), weight tying significantly reduces model size (e.g., by 25-30%; of course, this depends on model and vocabulary size). Model Architectures Gated Linear Unit Instead of simple convolutions, the paper introduced Gated Linear Units which became quite popular. The idea is similar to LSTM, but not from left to right, but from bottom to top. In addition to extracting features and passing them to the next layer, we also learn which features we want to pass for this token and which ones we don't. For this, a convolution extracts \\(2d\\) features: \\(d\\) content features These are the",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "left to right, but from bottom to top. In addition to extracting features and passing them to the next layer, we also learn which features we want to pass for this token and which ones we don't. For this, a convolution extracts \\(2d\\) features: \\(d\\) content features These are the main features - they extract information from input. \\(d\\) gate features The gate features are used to mask out content features. They are passed to the sigmoid function - it transforms the features into \"gate values\" from 0 to 1. Model Architecture The model architecture is shown in the figure. It consists of several blocks with a GLU layer (or several of them) wrapped in a residual block. The paper tries different models: with convolutional kernels 1-6 and different numbers of layers and filters. Quality and Context Size The figure below shows model quality depending on context size (context size is the CNN receptive field; it is evaluated as we saw here ). All in all, if you stack the number of layers sufficient to cover about 40 tokens, your model will be quite good. Note that while both ngram and convolutional models have fixed context size, this causes problems only for ngram models: they can not have a large context. In contrast, with several convolutional layers you can process long contexts. More in the paper the model outperforms the comparable LSTM; the model is much faster to train than LSTMs. A Bit of Analysis How nonsense can help your research To distinguish between cases where a model indeed learned grammatical agreement or just collocation, the authors test not only \"normal\" examples, but also the ones which do not make sense. E.g. does a model predict the correct agreement in the sentence The colorless green ideas I ate with the chair sleep furiously ? The authors generate such examples: they take an original sentence and replace some words with random words, but preserving part-of-speech and morphological inflection. One example was shown above. Look at the results (\"5-gram KN\" is the 5-gram model with Kneser-Ney smoothing ). The results show that: for n-gram models, context does not help For nonce sentences, 5-gram models are not better than unigram. A bit better for normal sentences though. with the same context, LSTMs are a lot better than n-gram The difference is huge for both original and nonce examples. This is the power of neural networks - they \"know\" which words are similar, while n-gram models rely only on co-occurrence. Size is not the only thing that matters! Neural models are better not only because of context size but also because of how they process this context. for LSTMs, large context does help This is nice - it means that LSTMs do use long contexts. Note also that the scores are quite high even for nonce sentences! More in the paper the detailed procedure for generating nonce examples; results and discussion for specific grammatical constructions. Language Models and Human Reading Behavior Lena : This is not what you will typically see at the \"Related Papers\" lists",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "also that the scores are quite high even for nonce sentences! More in the paper the detailed procedure for generating nonce examples; results and discussion for specific grammatical constructions. Language Models and Human Reading Behavior Lena : This is not what you will typically see at the \"Related Papers\" lists for a language modeling lecture (at least, I never saw in any). But when I first found this, I was so excited, that I can't help sharing it with you :) Time to read vs Predictability of a word in the context When we read a text, the time taken to read each word is related to our expectations about this word: the more \"expected\" a word is, the less time we need to read it. However, the exact functional relation between this per-word processing time in humans and the \"predictability\" of a word given context was not known. The key point of this paper is that a language model can be used to estimate the \"predictability\" of a word given context. Computational LM instead of a Human one - a very novel idea Previously, the predictability of a word given context was estimated in cloze-style tasks: humans were asked to guess the next word given context. For example, to continue the sentences (1) My brother came inside to... (2) The children went outside to... In the first case, the continuation can be very different, but in the second case, about 90% of participants suggest the word play . While this data estimates the word predictability directly, it is very sparse: for most of the continuations, there's no data at all. That's why the idea to use a computational language model instead of a human one was so groundbreaking: it allowed to estimate word predictability very easily . Components of the study Data with human behavior: eye-tracking Eye movements of native speakers reading a newspaper text. self-paced reading Moving-window self-paced reading times: the participant must press a button to reveal each word in turn. Data recorded: the times between button presses. Language model: 3-gram with Kneser-Ney smoothing. Results computational language models can be very good at predicting time taken by humans to read a word; the functional relationship between reading time and predictability is now known: it is logarithmic (i.e., the relationship between word log-probability and reading time is (near-)linear - this is what we see on the figure). Considered models 5-gram : 5-gram LM with Kneser-Ney smoothing; LSTM : the standard ones; RNNG : models the joint probability of a sequence of words as well as its syntactic structure; GPT-2 : Transformer LM. This a very popular model which we'll meet a bit later in the course. LM Surprisal vs Reading Times The figure shows the relationship between LM \"surprisals\" (negative log-probability) and human reading times for all models and corpora (more in the original paper!). Main observations are: the relationship is linear for all models; human reading time has higher variance with respect to LSTM predictions than with respect to predictions of other models. Psychometric Predictive Power vs LM Perplexity",
            "metadata": {}
        }
    },
    {
        "source_type": "lena_volta",
        "document_title": "Language Modeling",
        "url": "https://lena-voita.github.io/nlp_course/language_modeling.html",
        "chunk": {
            "text": "log-probability) and human reading times for all models and corpora (more in the original paper!). Main observations are: the relationship is linear for all models; human reading time has higher variance with respect to LSTM predictions than with respect to predictions of other models. Psychometric Predictive Power vs LM Perplexity (The scary phrase \"psychometric predictive power\" simply means how good is an LM at predicting human behavior.) Generally, we see that models with lower perplexity (in NLP, we think are good models) are also good at predicting human behavior. N-gram LMs: More Smoothings Kneser-Ney Smoothing Simple back-off smoothings discard context and back off from n-grams to k-grams with k < n. But let's take for example a phrase San Francisco : it is common and Francisco will have a high unigram probability. And here's the problem: Francisco appears mostly after San , but when backing off, it's large unigram probability will result in a large probability of Francisco after any token! Unigram Probability: Stupid Back-off vs Kneser-Ney Before looking at the full Kneser-Ney formula, let's first compare the unigram probabilities for a token which uses Kneser-Ney and stupid back-off smoothings. Stupid Back-off is based on simple unigram counts \\(N(w_i)\\): the number of times \\(w_i\\) occurs in the corpus. As we mentioned earlier, this won't work well for examples like San Francisco . In contrast to simple back-off, Kneser-Ney smoothing uses not the raw counts, but the number of tokens \\(w_i\\) can follow. Intuitively, this is exactly what we want: we need something which tells us how likely \\(w_i\\) can continue a prefix. In our example, Francisco will get low probability (just as it should). Going Further: Iterative Formula For the full formula, we need to define one more count: The full back-off formula for Kneser-Ney is shown below. Here will be more papers! The papers will be gradually appearing.",
            "metadata": {}
        }
    }
]