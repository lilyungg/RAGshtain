[
  {
    "document_title": "Генеративный подход к классификации",
    "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
    "section_title": "Генеративный и дискриминативный подходы к обучению",
    "text": "Если модель позволила точно оценить распределение, с её помощью можно генерировать объекты из этого условного распределения, в нашем примере — изображения кошек и рысей соответственно. А вместе распределениедало бы нам возможность генерировать изображения и кошек, и рысей, причём именно в той пропорции, в которой они встречаются в реальном мире. Поэтому модели, оценивающие, называютгенеративными. Ещё одно достоинство генеративных моделей — их способность находить выбросы в данных: объектможно считать выбросом, еслимало для каждого класса. Заметим, что находить выбросы с помощью генеративной модели можно и когда класс всего один — то есть никакие метки классов не доступны. Такая задача называется одноклассовой классификацией. Например, если у нас есть не размеченный датасет с аудиозаписями речи людей, то, обучив на нём генеративную модель, оценивающую в данном случае, мы сможем для нового аудиоопределить, похоже ли оно на аудиозапись человеческой речи (значениевелико), или это что-то другое: синтезированная речь, посторонний шум и т.п. (значениемало). Если мы знаем, что «выбросы», с которыми модели предстоит сталкиваться, — это, как правило, синтезированная речь, то, мы можем дополнить датасет вторым классом, состоящим из синтезированной речи, и смоделировать также распределение этого класса. Это позволит существенно увеличить качество детектирования таких выбросов. Чтобы использовать генеративную модель для классификации, необходимо выразитьчерези. Сделать это позволяет формула Байеса: Классификация в генеративных моделях осуществляется с помощью байесовского классификатора: Оценить, как правило, несложно. Для этого используют частотные оценки, полученные в обучающей выборке: Выражение (1) Отметим ещё раз, что использование генеративного подхода позволяет внедрять в модель априорные знания о. Это не очень впечатляет, когда речь идёт о бинарной классификации, но всё меняется, если рассмотреть задачу ASR (автоматического распознавания речи), в которой по записи голоса восстанавливается произносимый текст. Таргетами здесь могут быть любые предложения или даже более развёрнутые тексты. При этом размеченных данных (запись, текст) обычно намного меньше, чем доступных текстов, и обученная на большом чисто текстовом корпусе языковая модель, которая будет оценивать вероятность того или иного предложения, может стать большим подспорьем, позволив из нескольких фонетически корректных наборов слов выбрать тот, который в большей степени похож на настоящее предложение. Но как смоделировать распределение? Пространство всех возможных функций распределениябесконечномерно, из-за чего оценить произвольное распределение с помощью конечной выборки невозможно. Поэтому перед оценкойна это распределение накладывают дополнительные ограничения. Некоторые простые примеры таких ограничений мы рассмотрим в следующих разделах. Модель гауссовского (или квадратичного) дискриминантного анализа (GDA) строится в предположении, что распределение объектов каждого классаподчиняется многомерному нормальному закону со средними ковариационной матрицей: Тогда функция правдоподобия достигает максимума при И, представленной вышесм. выражение. Рассмотрим, как выглядит разделяющая поверхность в модели GDA. На поверхности, разделяющей классыивыполняется Выражение(2) Поскольку левая частьуравнения (2)квадратична по, разделяющая поверхность между двумя классами будет представлять из себя гиперповерхность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведённа рис. Плотность классов и разделяющая поверхность в многоклассовой модели LDAсм. рисунок. Ввыражении (2)член второго порядказануляется при. Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную матрицу, разделяющая поверхность между любыми двумя классами будет линейной (см. рисунок). Поэтому такая модель называется линейным дискриминантным анализом (LDA). На этапе обучения единственное отличие модели LDA от GDA состоит в оценке ковариационной матрицы: Заметим, что в модели GDA для каждого класса требовалось оценить порядкапараметров. Это может привести к переобучению в случае, если размерность пространства признаков велика, а некоторые классы представлены в обучающей выборке малым количеством объектов. В LDA для каждого класса требуется оценить лишь порядкапараметров (значениеи элементы вектора), и ещёобщих для всех классов параметров (элементы матрицы). Таким образом, основное преимущество модели LDA перед GDA — её меньшая склонность к переобучению, недостаток — линейная разделяющая поверхность.",
    "useful_links": [
      {
        "text": "см. выражение",
        "url": "#eq:class_proba_estimation"
      },
      {
        "text": "уравнения (2)",
        "url": "#eq:GDA_boundary"
      },
      {
        "text": "на рис.",
        "url": "#fig:GDA_boundary"
      },
      {
        "text": "см. рисунок",
        "url": "#fig:LDA_boundary"
      },
      {
        "text": "выражении (2)",
        "url": "#eq:GDA_boundary"
      },
      {
        "text": "см. рисунок",
        "url": "#fig:LDA_boundary"
      }
    ]
  },
  {
    "document_title": "Генеративный подход к классификации",
    "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
    "section_title": "Метод наивного байеса",
    "text": "Предположим, что признакиобъектов каждого класса— независимые случайные величины: В таком случае говорят, что величиныусловно независимы относительно. Тогда справедливо Выражение (3) То есть для того, чтобы оценить плотность многомерного распределениядостаточно оценить плотности одномерных распределений,см. рисунок. На рисунке приведён пример условно независимых относительнослучайных величин. Для оценки плотности двумерных распределений объектов классов достаточно оценить плотности маргинальных распределений, изображённые графиками вдоль осей. Рассмотрим пример. Пусть решается задача классификации отзывов об интернет-магазине на 2 категории:— отрицательный отзыв, клиент остался не доволен, и— положительный отзыв. Пусть признакравен 1, если словоприсутствует в отзыве, и 0 иначе. Тогда условиевыраженияозначает, что, в частности, наличие или отсутствие слова «дозвониться» в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова «телефон». На практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условиявыражения, называют наивным байесовским классификатором (Naive Bayes classifier, NB). Обучение модели NB заключается в оценке распределенийи. Дляможно использовать частотную оценкувыражения.— одномерное распределение. Рассмотрим несколько способов оценки одномерного распределения.",
    "useful_links": [
      {
        "text": "см. рисунок",
        "url": "#fig:blobs_density"
      },
      {
        "text": "выражения",
        "url": "#eq:cond_independent"
      },
      {
        "text": "выражения",
        "url": "#eq:cond_independent"
      },
      {
        "text": "выражения",
        "url": "#eq:class_proba_estimation"
      }
    ]
  },
  {
    "document_title": "Генеративный подход к классификации",
    "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
    "section_title": "Оценка одномерного распределения",
    "text": "Пусть мы хотим оценить одномерное распределение. Если распределениедискретное, требуется оценить его функцию массы, то есть вероятность того, что величинапримет значение. Метод максимума правдоподобия приводит к частотной оценке: Выражение (4) Где— размер выборки, по которой оценивается распределение(количество объектов классав случае оценки плотности класса). При этом может оказаться, что некоторое значениени разу не встречается в обучающей выборке. Например, в случае классификации отзывов методом Наивного Байеса, слово «амбивалентно» не встретилось ни в одном положительном отзыве, но встретилось в отрицательных. Тогда использованиеоценки выраженияприведёт к тому, что все отзывы с этим словом будут определяться NB как отрицательные с вероятностью 1. Чтобы избежать принятия таких радикальных решений при недостатке статистики, используют сглаживание Лапласа: где— количество различных значений, принимаемых случайной величиной,— гиперпараметр. Для оценки плотностиабсолютно непрерывного распределения в точкеможно разделить количество объектов обучающей выборки в окрестности точкина размер этой окрестности: Обычно объекты, лежащие дальше от точки, учитывают с меньшим весом. Таким образом, оценка плотности приобретает вид где функция, называемая ядром, обычно имеет носитель(см. рисунок ниже). Такой способ оценки плотности называют непараметрическим. Результат оценки плотности с разными ядрами. Использованыизображения из: При параметрической оценке плотности предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид Таким образом, чтобы оценить плотность, достаточно оценить параметры. Метод максимума правдоподобия в этом случае даст такие оценки: — выборочное среднее,— выборочное стандартное отклонение. Если в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый классописать нормальным распределением со средними диагональной ковариационной матрицей, значения на диагонали которой обозначим. Таким образом, полученная модель (Gaussian Naive Bayes, GNB) эквивалентна моделиGDAс дополнительным ограничением на диагональность ковариационных матриц.",
    "useful_links": [
      {
        "text": "оценки выражения",
        "url": "#eq:freq_estimation"
      },
      {
        "text": "см. рисунок ниже",
        "url": "#fig:kernels.png"
      },
      {
        "text": "изображения из:",
        "url": "https://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html"
      },
      {
        "text": "GDA",
        "url": "#ss:GDA"
      }
    ]
  },
  {
    "document_title": "Генеративный подход к классификации",
    "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
    "section_title": "Наивный байесовский подход и логистическая регрессия",
    "text": "Предположим теперь, что в модели GNB класса всего 2, причём соответствующие им ковариационные матрицы совпадают, как это было в модели LDA. Таким образом. Посмотрим, как будет выглядетьв этом случае. По теореме Байеса имеем Разделим числитель и знаменатель полученного выражения на числитель: Из условной независимостиотносительнополучаем Формула (5) Перепишем сумму в знаменателе, воспользовавшись формулой плотности нормального распределения Подставляя это выражение вформулу (5), получаем Таким образом,представляется в GNB с общей ковариационной матрицей в таком же виде, как в модели логистической регрессии: Формула (6) где в случае GNB Однако это не значит, что модели эквивалентны: модель логистической регрессии накладывает менее строгие ограничения на распределение, чем GNB. Так,могут не являться условно независимыми относительно, а распределениямогут не удовлетворять нормальному закону, номожет при этом всё равно представляться в видеформулы (6). В этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией.",
    "useful_links": [
      {
        "text": "формулу  (5)",
        "url": "#eq:posterior"
      },
      {
        "text": "формулы  (6)",
        "url": "#eq:logreg"
      }
    ]
  }
]