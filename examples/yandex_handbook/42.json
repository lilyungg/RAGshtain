[
  {
    "document_title": "Проксимальные методы",
    "url": "https://education.yandex.ru/handbook/ml/article/proksimalnye-metody",
    "section_title": "Проксимальная минимизация",
    "text": "Для того, чтобы подступиться к проксимальным методам, посмотрим на градиентный спуск с другой стороны. Для простоты рассмотрим константный размер шага. Перепишем шаг градиентного спуска следующим образом: Посмотрим на это уравнение по-другому. Рассмотрим функцию, равнуюпри(мы будем воспринимать, как некоторый временной параметр). Тогда при: Теперь слева не что иное, как аппроксимация производной! Если мы устремимк нулю, то получится так называемоеуравнение градиентного потока: Эта динамика в случае выпуклой функциисходится к точке минимумаиз любой начальной точки при. Сравнение между динамикой градиентного спуска и градиентного потока можно увидеть на следующем изображении: Первый состоит из дискретных шагов, второй же представляет из себя непрерывный процесс. Нетрудно осознать физический смысл динамики: маленькое тело скатывается по склону графика функции так, что в любой момент её скорость совпадает с антиградиентом, то есть оно катится по направлению наискорейшего спуска. Теперь представим, что мы сейчас занимается не машинным обучением, а численными методами. Перед нами есть обыкновенное дифференциальное уравнение (ОДУ), и его надо решить. Одним из численных методов решения ОДУ (более стабильным, чем обычная схема Эйлера) является обратная схема Эйлера (backward Euler scheme): В обратной схеме Эйлера мы делаем градиентный спуск, только градиент смотрим не в текущей точке (как было бы в обычной схеме Эйлера), абуквальнов будущей. Занятная идея, только вот напрямую выразитьиз этого уравнения не получится. Нужно поступить чуть хитрее. Заметим, что Это позволяет нам сказать, что весь векторявляется градиентом функции, посчитанном в точке. Тогда получаем, чтоудовлетворяет следующему условию: Если функциявыпуклая, тотоже выпуклая, и её стационарная точка будет точкой минимума. Стало быть,можно высчитывать по формуле Определимпрокс-операторследующим образом: Тогда, поскольку умножение навнутри арг-минимума не влияет на саму точку минимума, получаем следующую итеративную схему: Итеративный процессназываетсяметодом проксимальной минимизации. Вы можете спросить себя: зачем он нужен? Ведь теперь на каждом шаге мы должны решать задачу оптимизации: Есливыпуклая, нам есть, что ответить: наличие второго слагаемого гарантирует сильную выпуклость задачи, то есть она решается достаточно эффективно. Но еслине является выпуклой, то мы ничего не достигли этой модификацией. Чтобы понять, зачем нам понадобилась проксимальная оптимизация, рассмотрим оптимизацию функций вида где– это гладкая функция, а– это функция, для которой прокс-оператор считается аналитически. Воспользуемся следующим трюком: помы совершим градиентный шаг, а по– проксимальный. Получаем следующую итеративную процедуру: Эта процедура определяет так называемый проксимальный градиентный метод (Proximal Gradient Method, PGM), который может использоваться, например, для решения задачи регрессии с-регуляризацией. Теперь решим конкретную задачу-регрессии. Она выглядит следующим образом: Мы хотим применить PGM к этой задаче, для этого нужно научиться вычислять прокс-оператор для-нормы. Проделаем эту операцию: Заметим, что каждое слагаемое зависиттолько от одной координаты. Это значит, что каждую координату мы можем прооптимизировать отдельно и получитьодномерных задач минимизации вида Решение такой одномерной задачи записывается в виде функцииsoft thresholding: Тогда мы получаем следующий алгоритм для-регрессии, которые называются Iterative Shrinkage-Thresholding Algorithm (ISTA): Скопировать код1w = normal(0,1)# инициализация2repeat S times:# другой вариант: while abs(err) >tolerance3f = X.dot(w)# посчитать предсказание4delta = f - y# посчитать отклонение предсказания5grad =2* X.T.dot(delta) / n# посчитать градиент6w_prime = w - alpha * grad# считаем веса, которые отправим в прокс7foriinrange(d):8w[i] = soft_threshold(w_prime[i], alpha * llambda)# вычисляем прокс Заметим одну крутую особенность этого алгоритма -- мы явно видим, что решение получается разреженное, ведь какие-то координаты будут явно зануляться при применении soft threshold! Причем чем больше размер и шага, и параметра регуляризации, тем больше прореживается координат. Конкретно этот метод не применяется на практике, но используются его вариации. Например,статья, которая указана в параграфе пролинейные моделио том, как работало предсказание CTR в google в 2012 году, также базируется на вычислении soft threshold как прокс-оператора. Подытожим все вышесказанное: Проксимальные методы – теоретически интересная идея для выпуклой оптимизации, которая должна давать более численно стабильные алгоритмы. Проксимальные методы позволяют достаточно эффективно решать задачи композитной оптимизации, в частности,-регуляризованную задачу регрессии. Более того, используемые на практике решения задачи-регуляризованной регрессии так или иначе базируются на идее ISTA. Также есть попытки использовать проксимальные методы для более сложных моделей. Например,статья о применении их в нейросетях. Кроме того, имеются применения проксимальных методов для построения распределенных алгоритмов. Все подробности можно найти вмонографии Neal Parikh и Stephen Boyd, мы же только привели применение этих идей в машинном обучении.",
    "useful_links": [
      {
        "text": "статья",
        "url": "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf"
      },
      {
        "text": "линейные модели",
        "url": "https://academy.yandex.ru/handbook/ml/article/linejnye-modeli"
      },
      {
        "text": "статья о применении их в нейросетях",
        "url": "https://arxiv.org/abs/1706.04638"
      },
      {
        "text": "монографии Neal Parikh и Stephen Boyd",
        "url": "https://web.stanford.edu/~boyd/papers/prox_algs.html"
      }
    ]
  }
]