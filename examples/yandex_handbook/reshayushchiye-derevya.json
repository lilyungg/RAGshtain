[
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Определение решающего дерева",
    "text": "Разобравшись с приведёнными выше примерами, мы можем дать определение решающего дерева. Пусть задано бинарное дерево, в котором: каждой внутренней вершинеприписан предикат; каждой листовой вершинеприписан прогноз, где— область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов). В ходе предсказания осуществляется проход по этому дереву к некоторому листу. Для каждого объекта выборкидвижение начинается из корня. В очередной внутренней вершинепроход продолжится вправо, если, и влево, если. Проход продолжается до момента, пока не будет достигнут некоторый лист, и ответом алгоритма на объектесчитается прогноз, приписанный этому листу. Вообще, предикатможет иметь, произвольную структуру, но на практике чаще используют просто сравнение с порогомпо какому-то-му признаку: При проходе через узел дерева с данным предикатом объекты будут отправлены в правое поддерево, если значение-го признака у них меньше либо равно, и в левое — если больше. В дальнейшем рассказе мы будем по умолчанию использовать именно такие предикаты. Из структуры дерева решений следует несколько интересных свойств: выученная функция — кусочно-постоянная, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть; дерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки; дерево решений способно идеально приблизить обучающую выборку и ничего не выучить (то есть такой классификатор будет обладать низкой обобщающей способностью): для этого достаточно построить такое дерево, в каждый лист которого будет попадать только один объект. Следовательно, при обучении нам надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым, чтобы результат обладал хорошей обобщающей способностью.",
    "useful_links": []
  },
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Почему построение оптимального решающего дерева — сложная задача?",
    "text": "Пусть, как обычно, у нас задан датасет, где— вектор таргетов, а— матрица признаков, в которой-я строка — это вектор признаков-го объекта выборки. Пусть у нас также задана функция потерь, которую мы бы хотели минимизировать. Наша задача — построить решающее дерево, наилучшим образом предсказывающее целевую зависимость. Однако, как уже было замечено выше, оптимизировать структуру дерева с помощью градиентного спуска не представляется возможным. Как ещё можно было бы решить эту задачу? Давайте начнём с простого — научимся строитьрешающие пни, то есть решающие деревья глубины 1. Как и раньше, мы будем рассматривать только самые простые предикаты: Ясно, что задачу можно решить полным перебором: существует не болеепредикатов такого вида. Действительно, индекс(номер признака) пробегает значения отдо, а всего значений порога, при которых меняется значение предиката, может быть не более: Решение, которое мы ищем, будет иметь вид: Для каждого из предикатовнам нужно посчитать значение функции потерь на всей выборке, что, в свою очередь, тоже занимает.Следовательно, полный алгоритм выглядит так: Скопировать код1min_loss = inf2optimal_border =None34forjinrange(D):5fortinX[:, j]:# Можно брать сами значения признаков в качестве порогов6loss = calculate_loss(t, j, X, y)7ifloss <min_loss:8min_loss, optimal_border = loss, (j, t) Сложность алгоритма —. Это не заоблачная сложность, хотя, конечно, не идеальная. Но это была схема возможного алгоритма поиска оптимального дерева высоты 1. Как обобщить алгоритм для дерева произвольной глубины? Мы можем сделать наш алгоритм поиска решающего пня рекурсивным и в теле цикла вызывать исходную функцию для всех возможных разбиений. Как мы упоминали выше, так можно построить дерево, идеально запоминающее всю выборку, однако на тестовых данных такой алгоритм вряд ли покажет высокое качество. Можно поставить другую задачу: построить оптимальное с точки зрения качества на обучающей выборке дерево минимальной глубины (чтобы снизить переобучение). Проблема в том, что поиск такого дерева —NP-полнаязадача, то есть человечеству пока неизвестны способы решить её за полиномиальное время. Как быть? Идеального ответа на этот вопрос нет, но до некоторой степени ситуацию можно улучшить двумя не исключающими друг друга способами: Разрешить себе искать не оптимальное решение, а просто достаточно хорошее. Начать можно с того, чтобы строить дерево с помощьюжадного алгоритма, то есть не искать всю структуру сразу, а строить дерево этаж за этажом. Тогда в каждой внутренней вершине дерева будет решаться задача, схожая с задачей построения решающего пня. Для того чтобы этот подход хоть как-то работал, его придётся прокачать внушительным набором эвристик. Заняться оптимизацией с точки зрения computer science — наивную версию алгоритма (перебор наборов возможных предикатов и порогов) можно ускорить и асимптотически, и в константу раз. Эти две идеи мы и будем обсуждать в дальнейшем. Сначала попытаемся подробно разобраться с первой — как использовать жадный алгоритм.",
    "useful_links": [
      {
        "text": "NP-полная",
        "url": "https://people.csail.mit.edu/rivest/pubs/HR76.pdf"
      }
    ]
  },
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Жадный алгоритм построения решающего дерева",
    "text": "Пусть— исходное множество объектов обучающей выборки, а— множество объектов, попавших в текущий лист (в самом начале). Тогда жадный алгоритм можно верхнеуровнево описать следующим образом: Создаём вершину. Если выполненкритерий остановки, то останавливаемся, объявляем эту вершину листом и ставим ей в соответствие ответ, после чего возвращаем её. Иначе: находим предикат (иногда ещё говорятсплит), который определит наилучшее разбиение текущего множества объектовна две подвыборкии, максимизируякритерий ветвления. Дляирекурсивно повторим процедуру. Данный алгоритм содержит в себе несколько вспомогательных функций, которые надо выбрать так, чтобы итоговое дерево было способно минимизировать:, вычисляющая ответ для листа по попавшим в него объектам из обучающей выборки, может быть, например: в случае задачи классификации — меткой самого частого класса или оценкой дискретного распределения вероятностей классов для объектов, попавших в этот лист; в случае задачи регрессии — средним, медианой или другой статистикой; простой моделью. К примеру, листы в дереве, задающем регрессию, могут быть линейными функциями или синусоидами, обученными на данных, попавших в лист. Впрочем, везде ниже мы будем предполагать, что в каждом листе просто предсказывается константа. Критерий остановки— функция, которая решает, нужно ли продолжать ветвление или пора остановиться. Это может быть какое-то тривиальное правило: например, остановиться только в тот момент, когда объекты в листе получились достаточно однородными и/или их не слишком много. Более детально мы поговорим о критериях остановки в параграфе про регуляризацию деревьев. Критерий ветвления— пожалуй, самая интересная компонента алгоритма. Это функция, измеряющая, насколько хорош предлагаемый сплит. Чаще всего эта функция оценивает, насколько улучшится некоторая финальная метрика качества дерева в случае, если получившиеся два листа будут терминальными, по сравнению с ситуацией, когда сама исходная вершина — это лист. Выбирается такой сплит, который даёт наиболее существенное улучшение. Впрочем, есть и другие подходы. При этом строгой теории, которая бы связывала оптимальность выбора разных вариантов этих функций и разных метрик классификации и регрессии, в общем случае не существует. Однако есть набор интуитивных и хорошо себя зарекомендовавших соображений, с которыми мы вас сейчас познакомим. Давайте теперь по очереди посмотрим на популярные постановки задач ML и под каждую подберём свой критерий. Ответы дерева будем кодировать так:— для ответов регрессии и меток класса. Для случаев, когда надо ответить дискретным распределением на классах,будет вектором вероятностей: Предположим также, что задана некоторая функция потерь. О том, что это может быть за функция, мы поговорим ниже. В момент, когда мы ищем оптимальный сплит, мы можем вычислить для объектов изтот константный таргет, которые предсказало бы дерево, будь текущая вершина терминальной, и связанное с ними значение исходного функционала качества. А именно — константадолжна минимизировать среднее значение функции потерь: Оптимальное значение этой величины обычно называютинформативностью, илиimpurity. Чем она ниже, тем лучше объекты в листе можно приблизить константным значением. Похожим образом можно определить информативность решающего пня. Пусть, как и выше,— множество объектов, попавших в левую вершину, а— в правую; пусть такжеи— константы, которые предсказываются в этих вершинах. Тогда функция потерь для всего пня в целом будет равна Вопрос на подумать. Как информативность решающего пня связана с информативностью его двух листьев? Теперь для того чтобы принять решение о разделении, мы можем сравнить значение информативности для исходного листа и для получившегося после разделения решающего пня. Разность информативности исходной вершины и решающего пня равна Для симметрии её принято умножить на; тогда получится следующий критерий ветвления: Получившаяся величина неотрицательна: ведь, разделив объекты на две кучки и подобрав ответ для каждой, мы точно не сделаем хуже. Кроме того, она тем больше, чем лучше предлагаемый сплит. Теперь посмотрим, какими будут критерии ветвления для типичных задач. Посмотрим на простой пример — регрессию с минимизацией среднеквадратичной ошибки: Информативность листа будет выглядеть следующим образом: Как мы уже знаем, оптимальным предсказанием константного классификатора для задачи минимизации MSE является среднее значение, то есть Подставив в формулу информативности сплита, получаем: То есть при жадной минимизации MSE информативность — это оценка дисперсии таргетов для объектов, попавших в лист. Получается очень стройная картинка: оценка значения в каждом листе — это среднее, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше. Случай средней абсолютной ошибки так же прост: в листе надо предсказывать медиану, ведь именно медиана таргетов для обучающих примеров минимизирует MAE констатного предсказателя (мы это обсуждали в параграфе пролинейные модели). В качестве информативности выступает абсолютное отклонение от медианы: Пусть в нашей задачеклассов, а— доля объектов классав текущей вершине: Допустим, мы заботимся о доле верно угаданных классов, то есть функция потерь — это индикатор ошибки: Пусть также предсказание модели в листе — один какой-то класс. Информативность для такой функции потерь выглядит так: Ясно, что оптимальным предсказанием в листе будет наиболее частотный класс, а выражение для информативности упростится следующим образом: Если же мы собрались предсказывать вероятностное распределение классов, то к этому вопросу можно подойти так же, как мы поступали при выводе логистической регрессии: через максимизацию логарифма правдоподобия (= минимизацию минус логарифма) распределения Бернулли. А именно, пусть в вершине дерева предсказывается фиксированное распределение(не зависящее от), тогда правдоподобие имеет вид откуда То, что оценка вероятностей в листе, минимизирующая, должна быть равна, то есть доле попавших в лист объектов этого класса, до некоторой степени очевидно, но это можно вывести и строго. Подставляя векторв выражение выше, мы в качестве информативности получим энтропию распределения классов: Пусть предсказание модели — это распределение вероятностей классов. Вместо логарифма правдоподобия в качестве критерия можно выбрать, например,метрику Бриера(за которой стоит всего лишь идея посчитать MSE от вероятностей). Тогда информативность получится равной Можно показать, что оптимальное значение этой метрики, как и в случае энтропии, достигается на векторе, состоящем из выборочных оценок частот классов,. Если подставитьв выражение выше и упростить его, получитсякритерий Джини: Критерий Джини допускает и следующую интерпретацию:равно математическому ожиданию числа неправильно классифицированных объектов в случае, если мы будем приписывать им случайные метки из дискретного распределения, заданного вероятностями. Казалось бы, мы вывели критерии информативности для всех популярных задач, и они довольно логично следуют из их постановок, но получилось ли у нас обмануть NP-полноту и научиться строить оптимальные деревья легко и быстро? Конечно, нет. Простейший пример — решение задачи XOR с помощью жадного алгоритма и любого критерия, который мы построили выше: Вне зависимости от того, что вы оптимизируете, жадный алгоритм не даст оптимального решения задачи XOR. Но этим примером проблемы не исчерпываются. Скажем, бывают ситуации, когда оптимальное с точки зрения выбранной метрики дерево вы получите с критерием ветвления, построенным по другой метрике (например, MSE-критерий для MAE-задачи или Джини для misclassification error).",
    "useful_links": [
      {
        "text": "линейные модели",
        "url": "https://education.yandex.ru/handbook/ml/article/linear-models"
      },
      {
        "text": "метрику Бриера",
        "url": "https://en.wikipedia.org/wiki/Brier_score#:~:text=The%20Brier%20Score%20is%20a,as%20applied%20to%20predicted%20probabilities"
      }
    ]
  },
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Особенности данных",
    "text": "На первый взгляд, деревья прекрасно могут работать с категориальными переменными. А именно, если признакпринимает значения из множества, то при очередном разбиении мы можем рассматривать по этому признаку произвольные сплиты вида(предикат будет иметь вид). Это очень логично и естественно, но проблема в том, что при большиху нас будетсплитов, и перебирать их будет слишком долго. Было бы здорово уметь каким-то образомупорядочиватьзначения, чтобы работать с ними так же, как с обычными числами: разделяя на значения, «не превосходящие» и «большие» определённого порога. Оказывается, что для некоторых задач такое упорядочение можно построить вполне естественным образом. Так, для задачи бинарной классификации значенияможно упорядочить по неубыванию доли объектов класса 1 с, после чего работать с ними, как со значениями вещественного признака. Показано, что в случае, если мы выбираем таким образом сплит, оптимальный с точки зрения энтропийного критерия или критерия Джини, то он будет оптимальным среди всехсплитов. Для задачи регрессии с функцией потерь MSE значенияможно упорядочивать по среднему значению таргета на подмножестве. Полученный таким образом сплит тоже будет оптимальным. Одна из приятных особенностей деревьев — это способность обрабатывать пропуски в данных. Разберёмся, что при этом происходит на этапе обучения и на этапе применения дерева. Пусть у нас есть некоторый признак, значение которого пропущено у некоторых объектов. Как обычно, обозначим черезмножество объектов, пришедших в рассматриваемую вершину, а через— подмножество, состоящее из объектов с пропущенным значением. В момент выбора сплитов по этому признаку мы будем просто игнорировать объекты из, а когда сплит выбран, мы отправим их в оба поддерева. При этом логично присвоить им веса:для левого поддерева идля правого. Веса будут учитываться как коэффициенты прив формуле информативности. Вопрос на подумать. Во всех критериях ветвления участвуют мощности множеств,и. Нужно ли уменьшение размера выборки учитывать в формулах для информативности? Если нужно, то как? Теперь рассмотрим этап применения дерева. Допустим, в вершину, где сплит идёт по-му признаку, пришёл объектс пропущенным значением этого признака. Предлагается отправить его в каждую из дальнейших веток и получить по ним предсказанияи. Эти предсказания мы усредним с весамии(которые мы запомнили в ходе обучения): Для задачи регрессии это сразу даст нам таргет, а в задаче бинарной классификации — оценку вероятности класса 1. Замечание. Если речь идёт о категориальном признаке, может оказаться хорошей идеей ввести дополнительное значение «пропущено» для категориального признака и дальше работать с пропусками, как с обычным значением. Особенно это актуально в ситуациях, когда пропуски имеют системный характер и их наличие несёт в себе определённую информацию.",
    "useful_links": []
  },
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Методы регуляризации решающих деревьев",
    "text": "Мы уже упоминали выше, что деревья легко переобучаются и процесс ветвления надо в какой-то момент останавливать. Для этого есть разные критерии, обычно используются все сразу: ограничение по максимальной глубине дерева; ограничение на минимальное количество объектов в листе; ограничение на максимальное количество листьев в дереве; требование, чтобы функционал качествапри делении текущей подвыборки на две улучшался не менее чем напроцентов. Делать это можно на разных этапах работы алгоритма, что не меняет сути, но имеет разные устоявшиеся названия: можно проверять критерии прямо во время построения дерева, такой способ называетсяpre-pruningилиearly stopping; а можно построить дерево жадно без ограничений, а затем провестистрижку(pruning), то есть удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. При этом качество стоит измерять на отдельной, отложенной выборке.",
    "useful_links": []
  },
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Алгоритмические трюки",
    "text": "Теперь временно снимем шапочку ML-аналитика, наденем шапочку разработчика и специалиста по computer science и посмотрим, как можно сделать полученный алгоритм более вычислительно эффективным. В базовом алгоритме мы в каждой вершине дерева для всех возможных значений сплитов вычисляем информативность. Если в вершину пришлообъектов, то мы рассматриваемсплитов и для каждого тратимопераций на подсчёт информативности. Отметим, что в разных вершинах, находящихся в нашем дереве на одном уровне, оказываются разные объекты, то есть сумма этихпо всем вершинам заданного уровня не превосходит, а значит, выбор сплитов во всех вершинах уровня потребуетопераций. Таким образом, общая сложность построения дерева —(где— высота дерева), и доминирует в ней перебор всех возможных предикатов на каждом уровне построения дерева. Посмотрим, что с этим можно сделать. Постараемся оптимизировать процесс выбора сплита в одной конкретной вершине. Вместо того чтобы рассматривать всевозможных сплитов, для каждого тратяна вычисление информативности, можно использовать одномерную динамику. Для этого заметим, что если отсортировать объекты по какому-то признаку, то, проходя по отсортированному массиву, можно одновременно и перебирать все значения предикатов, и поддерживать все необходимые статистики для пересчёта значений информативности задля каждого следующего варианта сплита (против изначальных). Давайте разберём, как это работает, на примере построения дерева для MSE. Чтобы оценить информативность для листа, нам нужно знать несколько вещей: дисперсию и среднее значение таргета в текущем листе; дисперсию и среднее значение таргета в обоих потомках для каждого потенциального значения сплита. Дисперсию и среднее текущего листа легко посчитать за. С дисперсией и средним для всех значений сплитов чуть сложнее, но помогут следующие оценки математического ожидания и дисперсии: Следовательно, нам достаточно для каждого потенциального значения сплита знать количество элементов в правом и левом поддеревьях, их сумму и сумму их квадратов. Впрочем, всё это необходимо знать только для одной из половинок сплита, а для второй это можно получить, вычитая значения для первой из полных сумм. Это можно сделать за один проход по массиву, просто накапливая значения частичных сумм. Если в вершину дерева пришлообъектов, сложность построения одного сплита складывается изсортировок каждая пои одного линейного прохода с динамикой, всего, что лучше исходного. Итоговая сложность алгоритма построения дерева —(где– высота дерева) противв наивной его версии. Какие именно статистики накапливать (средние, медианы, частоты), зависит от критерия, который вы используете. Если бы мощность множества значений признаков была ограничена какой-то разумной константой, то сортировку в предыдущем способе можно было бы заменитьсортировкой подсчётоми за счёт этого существенно ускорить алгоритм: ведь сложность такой сортировки —. Чтобы провернуть это с любой выборкой, мы можем искусственно дискретизировать значения всех признаков. Это приведёт к локально менее оптимальным значениям сплитов, но, учитывая, что наш алгоритм и без этого был весьма приблизительным, это не ухудшит ничего драматически, а вот ускорение получается очень неплохое. Самый популярный и простой способ дискретизации основан на частотах значений признаков: отрезок между максимальным и минимальным значением признака разбивается наподотрезков, длины которых выбираются так, чтобы в каждый попадало примерно равное число обучающих примеров. После чего значения признака заменяются на номера отрезков, на которые они попали. Аналогичная процедура проводится для всех признаков выборки. Полная сложность предобработки —— сортировка задля каждого изпризнаков. Теперь в процедуре динамического алгоритма поиска оптимального сплита нам надо перебирать не всеобъектов выборки, а всего лишьподготовленных заранее границ подотрезков. Частичные суммы статистик тоже придётся поддерживать не для исходного массива данных, а для списка извозможных сплитов. А для того чтобы делать это эффективно, необходим объект, называемыйгистограммой: упорядоченный словарь, сопоставляющий каждому значению дискретизированного признака сумму необходимой статистики от таргета на отрезке [B[i-1], B[i]]. Финальный вид алгоритма таков: Дискретизируем каждый из признаков назначений. Сложность. Создаём корневую вершинуroot. Вызываемbuild_tree_recursive(root, data). Функцияbuild_tree_recursiveвыглядит следующим образом: Проверяем, не пора ли остановиться. Если пора — считаем значение в листе. Теперь мы снова используем динамический алгоритм, но объекты будем сортировать не по исходным значениям признаков, а по их дискретизированным версиям, упорядочивая их с помощью сортировки подсчётом (для вершины, в которую попалообъектов, сложность будет равнапротивв стандартной динамике). Находим оптимальный сплит за. Делим данные, запускаем процедуру рекурсивно для обоих поддеревьев. Общая сложность: Если вам действительно хочется построитьоптимальное(или хотя бы очень близкое к оптимальному) дерево, то на сегодня для решения этой проблемы не нужно придумывать кучу эвристик самостоятельно, а можно воспользоваться специальными солверами, которые решают NP-полные задачи приближённо, но всё-таки почти точно. Так что единственной (и вполне решаемой) проблемой будет представить исходную задачу в понятном для солвера виде. По ссылке —примерпостроения оптимального дерева с помощью решения задачи целочисленного программирования.",
    "useful_links": [
      {
        "text": "сортировкой подсчётом",
        "url": "https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0_%D0%BF%D0%BE%D0%B4%D1%81%D1%87%D1%91%D1%82%D0%BE%D0%BC"
      },
      {
        "text": "пример",
        "url": "https://arxiv.org/abs/1907.02211"
      }
    ]
  },
  {
    "document_title": "Решающие деревья",
    "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
    "section_title": "Историческая справка",
    "text": "Как вы, может быть, уже заметили, решающие деревья — это одна большая эвристика для решения NP-полной задачи, практически лишённая какой-либо стройной теоретической подоплёки. В 1970–1990-e годы интерес к ним был весьма велик как в индустрии, где был полезен хорошо интерпретируемый классификатор, так и в науке, где учёные интересовались способами приближённого решения NP-полных задач. В связи с этим сложилось много хорошо работающих наборов эвристик, у которых даже были имена: например,ID3был первой реализацией дерева, минимизирующего энтропию, аCART— первым деревом для регрессии. Некоторые из них были запатентованы и распространялись коммерчески. На сегодня это всё потеряло актуальность в связи с тем, что существуют хорошо написанные библиотеки (например,sklearn, в которой реализована оптимизированная версия CART).",
    "useful_links": [
      {
        "text": "ID3",
        "url": "https://en.wikipedia.org/wiki/ID3_algorithm"
      },
      {
        "text": "CART",
        "url": "https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29"
      },
      {
        "text": "sklearn",
        "url": "https://scikit-learn.org/stable/modules/tree.html"
      }
    ]
  }
]