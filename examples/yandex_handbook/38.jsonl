{"document_title": "Сети бесконечной ширины", "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny", "section_title": "Применение NTK-анализа", "text": "Нам удалось проинтегрировать динамику предсказаний в явном виде. Что это даёт? Во-первых, мы получаем достаточное условие на сходимость в глобальный минимум на обучающей выборке. Таким условием является положительная определённость матрицы Грама ядра:для некоторого. В самом деле, в этом случае, что даёт Во-вторых, раз явное решение известно, можно написать оценку на обобщающую способность. Оба этих результата опираются на то, что ядро постоянно. Как мы покажем ниже, постоянство нейрокасательного ядра нейронной сети можно гарантировать лишь в пределе бесконечной ширины. Тем не менее, если сеть конечна, но достаточно широка, можно показать, что её ядро достаточно близко к предельному, и оценки сохраняют силу. Например, для обоснования сходимости в глобальный минимум достаточно показать, что наименьшее собственное значение эмпирического ядра с высокой вероятностью остаётся отделённым от нуля в течение обучения:с вероятностьюдля. В самом деле, из этого следует, что а значит, Формальное доказательство вы можете найти в работеGradient Descent Provably Optimizes Over-parameterized Neural Networks, а также вконспекте лекцийавтора этого параграфа. Вот ещё несколько результатов, полученных в этом направлении: улучшенные оценки на минимальную ширину в работеQuadratic suffices for over-parametrization via matrix chernoff bound; оценки для случая глубоких сетей в работеGradient descent finds global minima of deep neural networks; оценки на обобщающую способность, полученные через близость ядра к предельному, в работеFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. Как мы увидим позже, NTK реальных, стандартно параметризованных, имеющих конечную ширину сетей может меняться за время обучения существенным образом: см. эмпирическую работуDeep learning versus kernel learningи теоретический анализ для сетей с одним скрытым слоемDynamically Stable Infinite-Width Limits of Neural Classifiers. Тем не менее, ядро в инициализации может выявить определённые патологии соответствующей нейронной сети. Рассмотрим один из примеров применения. В некоторых состоящих из однородных блоков архитектурах (скажем, ResNet) можно увеличивать (и даже устремлять к бесконечности) число слоёв или блоков, и логично задаться вопросом о том, как при этом будет вести себя процесс обучения. Необходимым условием обучаемости является хороший первый шаг обучения. Если он исчезающе мал, то сеть не обучится ни на первом, ни на каком-либо другом шаге. Если он слишком велик, то обучение разойдётся на первом же шаге. Как мы увидим ниже, индикатором проблем является плохая обусловленность NTK в инициализации. Например, его собственные значения могут с ростом глубины стремиться к нулю или, наоборот, к бесконечности. В первом случае какие-то из компонент выборки никогда не выучатся, во втором обучение невозможно ни при каком конечном темпе обучения. Чтобы в этом убедиться, рассмотрим разложение матрицы Грама ядра по собственным векторами: где, а векторыобразуют ортонормированный базис. Разложим предсказание сети по этому базису:. Так как базис ортонормированный, каждая из компонент эволюционирует независимо от других. В самом деле, для дискретного градиентного спуска с шагомимеем Таким образом, если, тоникогда не сойдётся к. Кроме того, для того, чтобы процесс сходился, шагдолжен убывать обратно пропорционально наибольшему собственному числу. Если последнее стремится к бесконечности, тостремится к нулю, а значит,будем мало для всех, для которыхконечен; соответствующие компоненты также никогда не сойдутся. Подробности см. в работеRapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping, а также в более ранних работахExponential expressivity in deep neural networks through transient chaos,Deep information propagation,Resurrecting the sigmoid in deep learning through dynamical isometry,Dynamical isometry and a mean field theory of cnns, в которых использовалась похожая идея, но не использовалось понятие NTK явно. См. также главу про инициализацию вконспекте лекций.", "useful_links": [{"text": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks", "url": "https://arxiv.org/pdf/1810.02054.pdf"}, {"text": "конспекте лекций", "url": "https://arxiv.org/pdf/2012.05760.pdf"}, {"text": "Quadratic suffices for over-parametrization via matrix chernoff bound", "url": "https://arxiv.org/pdf/1906.03593v1.pdf"}, {"text": "Gradient descent finds global minima of deep neural networks", "url": "https://arxiv.org/pdf/1811.03804.pdf"}, {"text": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "url": "https://arxiv.org/pdf/1901.08584.pdf"}, {"text": "Deep learning versus kernel learning", "url": "https://arxiv.org/pdf/2010.15110.pdf"}, {"text": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "url": "https://arxiv.org/pdf/2006.06574.pdf"}, {"text": "Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping", "url": "https://arxiv.org/pdf/2110.01765.pdf"}, {"text": "Exponential expressivity in deep neural networks through transient chaos", "url": "https://arxiv.org/pdf/1606.05340.pdf"}, {"text": "Deep information propagation", "url": "https://arxiv.org/pdf/1611.01232.pdf"}, {"text": "Resurrecting the sigmoid in deep learning through dynamical isometry", "url": "https://arxiv.org/pdf/1711.04735.pdf"}, {"text": "Dynamical isometry and a mean field theory of cnns", "url": "https://arxiv.org/pdf/1806.05393.pdf"}, {"text": "конспекте лекций", "url": "https://arxiv.org/pdf/2012.05760.pdf"}]}
{"document_title": "Сети бесконечной ширины", "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny", "section_title": "NTK и Ядровые методы", "text": "Предельное NTK нейронной сети можно использовать в любом ядровом методе, например, в SVM. Обсудим это поподробнее и заодно разберёмся, почему NTK вообще называют ядром. Рассмотрим задачу линейной регрессии: Эту же задачу можно эквивалентно переписать следующим образом: где– пространство линейных отображенийс некоторой нормойна нём. Сделаем линейное пространствоевклидовым, введя на нём следующее скалярное произведение. Дляиопределим Это скалярное произведение порождает норму, что и делает формулировку (5) эквивалентной формулировке (4). Пространство линейных моделей слишком узко, однако ничто не мешает нам рассмотреть задачу вида (5), в которойбудет произвольным нормированным пространством функций. Наиболее хорошо изучен случай, когда функции изявляются линейными моделями в некотором (возможно, бесконечномерном) гильбертовом пространстве признаков:, гдеотображаетв это пространство. Если последнее всё же конечномерно, то мы можем использовать матричную запись; элементыв этой записи обычно называют первичными переменными (primal variables). Пространство функцийтакже оказывается гильбертовым: соответствующее скалярное произведение имеет вид Таким образом,, если. Любому отображениюможно сопоставить симметричную положительно-определённую функцию; функции такого вида называются ядрами. В силуфундаментальной теоремы о представителелюбое решение задачи (4) принимает вид В отличие от, векторвсегда конечномерен: его размерность равна размеру обучающей выборки. Элементыназывают двойственными (dual) переменными. Упомянутый результат позволяет перейти от минимизациив бесконечномерном пространстве функций (или, что то же самое, минимизациив бесконечномерном пространстве признаков), к минимизации в конечномерном пространстве двойственных переменных: Если в качестве функции потерь взять квадратичную, то получим ядровую регрессию; если же взять hinge loss, то SVM. Заметим, что двойственная задача полностью сформулирована в терминах ядра: отображение в потенциально бесконечное пространство признаковболее нигде не возникает. Поэтому мы можем использовать в качествелюбую симметричную положительно определённую функцию двух переменных, не думая о том, для какого пространства признаков оно будет ядром (есть теорема, что такие функции всегда являются ядрами). Это может быть очень полезно. Так, если для эмпирического NTK в инициализацииимеем, но совершенно неочевидно, какое отображениесоответствует предельному NTK:. Таким образом, мы можем использоватьв качестве ядрав двойственной задаче (6) наряду с линейнымили гауссовским ядром. Такой подход привлекателен тем, что обучение ядровых методов более устойчиво и имеет меньше гиперпараметров. При этом можно надеяться, что результат обучения ядрового метода с NTK в качестве ядра будет близок к результату обучения соответствующей нейронной сети. Основная проблема ядровых методов в том, что они требуют вычисления матрицы Грама ядра на обучающем наборе данных. Её размер(где– размер выборки), так что применение ядровых методов на больших данных сильно усложняется. Более того, наивное вычисление динамикииз формулы (3) требует обращения матрицы Грама, которое занимаетвремени. Тем не менее, определённые оптимизации существуют. Так например, в работеKernel methods through the roofпредлагается способ приближённого вычисления () запамяти и времени. Другие подходы см. в работахFast Finite Width Neural Tangent KernelиNeural tangents: Fast and easy infinite neural networks in python. Так или иначе, на малых наборах данных выражение (3) можно вычислить точно, см. результаты в работеHarnessing the power of infinitely wide deep nets on small-data tasks. Существуют также примеры задач, в которых матрицу Грама ядра достаточно посчитать только для малых, см., например,Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks. Ещё одна проблема использования NTK в ядровых методах состоит в том, что явный подсчёт предельного NTK доступен только для сетей, состоящих из слоёв из определённого класса. В этот класс входят полносвязные и свёрточные слои, average pooling, ряд нелинейностей с одним аргументом (включая, например, ReLU и erf), layer norm, но не входят max pooling и batch norm, часто используемые в реальных архитектурах. Явный подсчёт предельного NTK для «хороших» сетей реализован вбиблиотеке NeuralTangents; часть явных формул для подсчёта можно найти в статьеOn exact computation with an infinitely wide neural net. Тем не менее, даже в тех случаях, когда посчитать предельное NTK не представляется возможным, в качестве ядра для ядрового метода можно использовать эмпирическое NTK в инициализации Такое ядро можно рассматривать как шумную и смещённую оценку предельного; для уменьшения шума можно использовать Монте-Карло оценку матожидания. Некоторые оптимизации подсчёта эмпирического ядра см. в работеNeural tangents: Fast and easy infinite neural networks in python. NTK не единственное ядро, которое можно сопоставить нейронной сети. Так, NNGP-ядро– это ядро гауссовского процесса, реализуемого сетью в пределе бесконечной ширины. Подробнее можно почитать в работахDeep Neural Networks as Gaussian Processes,Wide neural networks of any depth evolve as linear models under gradient descent,Random neural networks in the infinite width limit as Gaussian processesили вконспекте лекций. Можно показать, что оно соответствует NTK-ядру для сети, в которой учится лишь выходной слой. Так как, в отличие от NTK, для подсчёта NNGP-ядра не требуется обратный проход (backward pass), последнее более вычислительно эффективно;Towards nngp-guided neural architecture search– пример работы, в которой предпочтение отдаётся NNGP-ядру именно по этой причине.", "useful_links": [{"text": "фундаментальной теоремы о представителе", "url": "https://en.wikipedia.org/wiki/Representer_theorem"}, {"text": "Kernel methods through the roof", "url": "https://arxiv.org/pdf/2006.10350.pdf"}, {"text": "Fast Finite Width Neural Tangent Kernel", "url": "https://arxiv.org/pdf/1806.07572.pdf"}, {"text": "Neural tangents: Fast and easy infinite neural networks in python", "url": "https://arxiv.org/pdf/1912.02803.pdf"}, {"text": "Harnessing the power of infinitely wide deep nets on small-data tasks", "url": "https://arxiv.org/pdf/1910.01663.pdf"}, {"text": "Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks", "url": "https://arxiv.org/pdf/2108.00131.pdf"}, {"text": "библиотеке NeuralTangents", "url": "https://github.com/google/neural-tangents"}, {"text": "On exact computation with an infinitely wide neural net", "url": "https://arxiv.org/pdf/1904.11955.pdf"}, {"text": "Neural tangents: Fast and easy infinite neural networks in python", "url": "https://arxiv.org/pdf/1912.02803.pdf"}, {"text": "Deep Neural Networks as Gaussian Processes", "url": "https://arxiv.org/pdf/1711.00165.pdf"}, {"text": "Wide neural networks of any depth evolve as linear models under gradient descent", "url": "https://arxiv.org/pdf/1902.06720.pdf"}, {"text": "Random neural networks in the infinite width limit as Gaussian processes", "url": "https://arxiv.org/pdf/2107.01562.pdf"}, {"text": "конспекте лекций", "url": "https://arxiv.org/pdf/2012.05760.pdf"}, {"text": "Towards nngp-guided neural architecture search", "url": "https://arxiv.org/pdf/2011.06006.pdf"}]}
{"document_title": "Сети бесконечной ширины", "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny", "section_title": "Сходимость эмпирического ядра", "text": "Вы этом параграфе мы покажем, что при определённой параметризации эмпирическое NTK не зависит ни от времени, ни от инициализации. Мы начнём с иллюстративного примера, прежде чем формулировать строгую теорему. Рассмотрим сеть с одним скрытым слоем, скалярным выходом и гауссовской инициализацией весов; вход для простоты тоже положим скалярным: Здесь– ширина скрытого слоя. Следуя одной из стандартных схем инициализации из статьиDelving deep into rectifiers: Surpassing human-level performance on imagenet classification, дисперсия каждого слоя выбирается обратно пропорционально числу входных нейронов (подробнее см. в параграфе протонкости обучения нейросетей). Назовём описанную выше параметризацию стандартной. Для сходимости ядра нам придётся несколько её видоизменить: Назовём новую параметризацию NTK-параметризацией. Отметим, что распределение выходов нейронов в инициализации остаётся неизменным при переходе от стандартной к NTK-параметризации. Что меняется – это динамика градиентного спуска: Приприращения весов для такой параметризации имеют порядок, в то время как сами веса имеют порядокпри. Поэтомуипридля любого данногои. Другими словами, с ростом размера скрытого слоя градиент будет стремиться к нулю, и каждый из весов в пределе останется в начальной точке. Сравним с градиентным спуском в стандартной параметризации: В этом случае веса выходного слоя имеют порядокпри, но получают приращения порядкав этот момент времени, в то время как веса входного слоя имеют порядокпри, но получают в этот момент времени приращения порядка. В новой параметризации эмпирическое NTK выглядит следующим образом: Так какипридля любых заданныхи, выражение выше асимптотически эквивалентно а значит, сходится к прив силу закона больших чисел. Предельное ядроне зависит ни от времени, ни от инициализации. Мы будем называть это ядронейрокасательнымили простоNTK(его не стоит путать с эмпирическим NTK). Ещё раз подчеркнём, что это работает для NTK-параметризации, но не для стандартной. Для стандартной параметризации эмпирическое NTK в инициализации расходится с шириной: Подробнее мы поговорим об этом в одном из следующих параграфов. Для NTK-параметризации сходимость эмпирического ядра выполняется не только для сетей с одним скрытым слоем. Так, рассмотрим полносвязную сеть сслоями: Здесь,идля всех остальных. Положим, что веса инициализируются из стандартного нормального распределения. Поставим задачу оптимизации дифференцируемой функции потерь: где– объединение всех весовсети. Теорема ниже доказана воригинальной работе по NTK: Теорема. В предположениях выше, еслиизи липшицева иизи липшицева, тосходится кпо вероятности припоследовательно. Оказывается, что эта теорема верна не только для полносвязных сетей с гладкими активациями. Определимтензорную программукак начальный набор переменных определённых типов и последовательность команд. Каждая команда порождает новую переменную, действуя на уже имеющиеся. Переменные бывают трёх типов: :матрицы с независимыми элементами из; : вектора размерас асимптотически независимыми нормальными элементами; : образы-переменных относительно поэлементных нелинейностей. Для переменнойзаписьбудет означать, чтоимеет тип. Команды бывают следующие: trspop:(перевести переменную типасо значениемв переменную типасо значением); matmul:; lincomb:; nonlin:(здесь мы несколько выходных векторовагрегируем в один с помощью покоординатной, возможно, нелинейной функции). Формализм тензорных программ позволяет представить прямой и обратный проход широкого класса нейронных архитектур, который включает свёрточные сети, рекуррентные сети, сети с residual слоями. Хотя и ни одна из операций выше не может порождать новые-переменные (веса), любое наперёд заданное число шагов градиентного спуска можно представить в рамках одной тензорной программы (посредством «развёртывания» шагов градиентного спуска). Назовём величинушириной тензорной программы.Основная «предельная» теорема тензорных программ представлена ниже: Master theorem(G. Yang,Tensor programs III: Neural matrix laws). Рассмотрим тензорную программу с-величинами, удовлетворяющую определённым начальным условиям. Пусть все нелинейностии функцияполиномиально ограничены. Тогда почти наверное при, гдеимогут быть вычислены по некоторым рекурентным правилам. Оказывается, что если тензорная программа выражает прямой и обратной проход в некоторой нейронной сети, то NTK сети в инициализации всегда можно представить в видедля некоторой функции, см.Tensor programs II: Neural tangent kernel for any architecture.Таким образом, теорема выше доказывает существование и детерминированность предельного ядра в инициализации, а также даёт способ его вычисления. Более того, это верно и для ядра в любой фиксированный момент времени, см.Tensor Programs IIb. В качестве иллюстрации обратимся вновь к сети с одним скрытым слоем. Рассмотрим тензорную программу, вычисляющую прямой и обратный проходы на входахи. Такая программа порождает следующие-величины:,,и. Напомним, что эмпирическое NTK равно Положив получим выражение как раз в виде, требуемом Master Theorem.", "useful_links": [{"text": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "url": "https://arxiv.org/pdf/1502.01852.pdf"}, {"text": "тонкости обучения нейросетей", "url": "https://academy.yandex.ru/handbook/ml/article/tonkosti-obucheniya"}, {"text": "оригинальной работе по NTK", "url": "https://arxiv.org/pdf/1806.07572.pdf"}, {"text": "Tensor programs III: Neural matrix laws", "url": "https://arxiv.org/pdf/2009.10685.pdf"}, {"text": "Tensor programs II: Neural tangent kernel for any architecture", "url": "https://arxiv.org/pdf/2006.14548.pdf"}, {"text": "Tensor Programs IIb", "url": "https://arxiv.org/pdf/2105.03703.pdf"}]}
{"document_title": "Сети бесконечной ширины", "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny", "section_title": "Стандартная параметризация и эволюция ядра", "text": "Как было отмечено в предыдущем параграфе, эмпирическое NTK двухслойной сети расходится с шириной при стандартной параметризации. При, так какнезависимы и имеют порядок, сумма расходится пропорционально.Так как для квадратичной функции потерь, предсказание модели в любой точкеполучает приращение порядкана первом же шаге обучения; для задачи регрессии такая модель теряет смысл. Однако для классификации величина предсказаний не играет роли: для бинарной классификации важен лишь знак, а для многоклассовой – индекс максимального логита. Таким образом, в этом случае, несмотря на расходящееся ядро, предел при бесконечной ширине имеет смысл, см.Dynamically Stable Infinite-Width Limits of Neural Classifiers. Рассмотрим нормализованное эмпирическое NTK. Его предел в инициализации равен. Назовём этот предел нормализованным NTK и обозначим. В отличие от ядра в NTK-параметризации, нормализованное NTK при стандартной параметризации зависит от времени: Напомним, как выглядит градиентный спуск в стандартной параметризации: При,, в то время как. Так каки, для любого, не зависящего от,,,и. Наивная оценка сумм даётдля любого, не зависящего от. Таким образом, нормализованное ядро зависит от времени даже в пределе бесконечной ширины. Экспериментальный анализ эволюции ядра реальной нейронной сети в стандартной параметризации см. в работеDeep learning versus kernel learning. Преимущество нейронных сетей над ядровыми методами, в том числе с NTK, может быть связано, в частности, с зависимостью предельного ядра от времени. В самом деле, ядро измеряет «похожесть» в некотором пространстве признаков. Для NTK это пространство фиксировано, в то время как нейронная сеть меняет своё ядро по ходу обучения, возможно, делая его более подходящим для задачи.", "useful_links": [{"text": "Dynamically Stable Infinite-Width Limits of Neural Classifiers", "url": "https://arxiv.org/pdf/2006.06574.pdf"}, {"text": "Deep learning versus kernel learning", "url": "https://arxiv.org/pdf/2010.15110.pdf"}]}
