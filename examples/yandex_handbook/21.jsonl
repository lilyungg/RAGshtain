{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Что такое языковые модели?", "text": "Говоря простым языком, языковые модели — это алгоритмы, способные продолжать тексты. Если чуть усложнить, то этовероятностные алгоритмы, и к ним сразу можно задать эмпирический критерий качества: хорошая модель даёт разумные продолжения данных ей текстов. Давайте разберём пример выше. Модель высчитывает вероятность возможных продолжений текста и предлагает их нам. Слово «фрукт» — наименее разумное продолжение нашей фразы, в то время как слово «наука» — наиболее разумное. И действительно, это часть определения машинного обучения, которое мы давали в начале этого учебника. Таким образом, нам осталось лишь научить алгоритм моделировать эти вероятности и максимизировать их для разумных предложений. Но как это сделать? По ходу развития языковых моделей подходы менялись, мы расскажем о каждом из них в хронологическом порядке. Начнём с краткого экскурса в историю — поговорим о статистических моделях, рекуррентных нейронных сетях и трансформерах. А затем перейдём к современным — GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT и LLaMa.", "useful_links": [{"text": "вероятностные алгоритмы", "url": "https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D1%80%D0%BE%D1%8F%D1%82%D0%BD%D0%BE%D1%81%D1%82%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC"}]}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Развитие языковых моделей", "text": "Идея модели лежит на поверхности, много где применяется в самых разных вариациях даже в ХХ веке, поэтому сложно назвать авторов или точную дату создания. Однако этот метод популярен до сих пор — используется в клавиатурах смартфонов для исправления опечаток и быстрого набора текстов через Т9. Теперь подробнее о методе. Напомним вероятностную формулировку цепей Маркова в общем виде: Если представить, что— это слово, а набор этих омега — это предложение, то по формуле становится возможным посчитать вероятность предложенияС практической точки зрения всё чуть сложнее, ведь распределение слов в реальном языке (какое, с какими и как часто встречается), вообще говоря, неизвестно. Его принято аппроксимировать на основекорпуса текстов(например, всего интернета) — в этом случае считаются совстречаемости слов друг с другом, и по ним считаются вероятности. В условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражениидлина контекста равна. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин: Сложность в подсчёте и хранении каждого возможного уникального контекста длины. Если корпус текстов состоит изразличных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна, что очень много при больших. Большой контекст реже встречается. То есть слова «яблоку», «негде» и «упасть» поодиночке встречаются чаще, чем их комбинация «яблоку негде упасть». Отсюда достаточность статистик падает с ростом длины контекста. В учебном примере предлагается ограничиться шириной контекста размера 1: Интересно, что такой подход достаточно популярен до сих пор. Например, он используется в умных клавиатурах, чтобы подсказать следующее слово. Достоинства статистических моделей: Простота имплементации. Высокая скорость работы алгоритма. Низкая вычислительная стоимость обучения и инференса. Недостатки статистических моделей: Не сможет сгенерировать слова, которые не шли подряд в обучающем корпусе. Очень маленький контекст. Длинные последовательности равновероятны ≈ нулю (в цепях Маркова для длинных последовательностей много множителей меньше нуля, поэтому их произведение уже практически равно нулю для любых множителей). Отсюда алгоритм не может выдавать разумные продолжения большой длины. Языковые модели, да и вообще все модели, которые оперируют текстом, используют понятие токена. Токен — это единица текста, которую понимают алгоритмы. В примере выше токен — это отдельное слово(этот подход называетсямешком слов), однако текст можно разбивать на токены и иначе. Раньше предложение разбивалось на слова по пробелам, знакам препинания, исключались стоп-слова и так далее (назовем этоCountVectorizer). Но у этого подхода возникали две проблемы с разными словоформами. Они: Либо обозначались разными токенами, что не совсем верно, ведь слово-то одно и то же. И получалось, что похожим смыслом обладало сразу несколько токенов. Либо приводились к начальной форме — и в итоге терялся падеж, время, число. Современные токенизаторы построены на алгоритме BPE (Byte Pair Encoding; об устройстве BPE более подробно можно прочитать вучебнике Лены Войта). Решение требует фиксации определённого числа токенов. Как только это сделано, в словарь добавляются все символы из текста, ищутся самые частые их сочетания и снова добавляются. Этот процесс продолжается до тех пор, пока число токенов не станет равно заданному значению. Токенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет_перед соответствующим токеном) и не построен на логике разбиения слов по разделителям. Поэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитатьздесь. Появились после статистических моделей, подробнее о хронологииздесь. Рекуррентные нейронные сети концептуально можно описать формулой, где: — некоторая модель; — внутреннее состояние модели на момент времени; — токен, который сейчас обрабатывается. Тогда следующий токенполучается так: Подробно об устройстве RNN мы рассказываем в параграфеНейросети для работы с последовательностями. Здесь же коротко отметим, что существуют различные модификации рекуррентных сетей, которые усложняют структуру алгоритма, даже добавляют механизм внимания Attention. Если коротко, то он позволяет лучше оценивать взаимосвязи токенов в тексте. Все они в разной степени помогают модели усваивать более длинные и сложные последовательности токенов. Достоинства RNN: Высокая скорость инференса и сравнительно низкая стоимость. Более качественный текст, чем у моделей на статистиках. Теоретически понимает контекст в сотни слов (а с Attention ещё больше). Точно учитывает весь контекст документа. Недостатки RNN: Невозможность параллельного обучения на многих устройствах, отсюда не получится просто так обучить большую RNN. Модель «хорошо помнит» лишь несколько последних токенов контекста (без Attention). Проблемы с обучением (exploading/vanishing gradients).", "useful_links": [{"text": "мешком слов", "url": "https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%88%D0%BE%D0%BA_%D1%81%D0%BB%D0%BE%D0%B2"}, {"text": "CountVectorizer", "url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"}, {"text": "учебнике Лены Войта", "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#bpe"}, {"text": "здесь", "url": "https://arxiv.org/pdf/1808.06226.pdf"}, {"text": "здесь", "url": "https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C"}, {"text": "Нейросети для работы с последовательностями", "url": "https://academy.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami"}]}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Трансформеры", "text": "Более подробно трансформеры и их устройство описаны в параграфеТрансформеры. Последней и наиболее успешной с точки зрения качества оказалась архитектура трансформеров. Она состоит из двух частей: encoder (на изображении слева) и decoder (на изображении справа). Изначально был популярен подход обучать части отдельно. Так на базе encoder-блоков были построеныBERT-модели. Идея обучения звучит несложно: давайте из входного текста замаскируем токеномMASK15% имеющихся токенов и обучим модель угадывать, какие именно токены были скрыты. Тогда, если модель обучится это делать, она сможет очень хорошо понимать текст. Таким образом, энкодеры обладают следующими особенностями: Анализируют входной текст и связи между токенами. Выделяют важные токены для определённой задачи. Ничего не генерируют. На базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель: генерирует токен за токеном. смотрит на весь контекст, архитектурно, нет забывания токенов. имеет возможность (как и BERT-модели) обучаться параллельно. обладает достаточно высокой вычислительной стоимостью инференса.", "useful_links": [{"text": "Трансформеры", "url": "https://academy.yandex.ru/handbook/ml/article/transformery"}, {"text": "BERT-модели", "url": "https://en.wikipedia.org/wiki/BERT_(language_model)"}, {"text": "MASK", "url": "https://academy.yandex.ru/handbook/ml/article/transformery"}]}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Современные подходы", "text": "Начнём немного издалека, с моделей GPT-1 и GPT-2. Первая была обучена в 2018 году на 7000 книг и имела размер контекста в 512 токенов. И она сразу получилась довольно сильной: после дообучения на специализированные задачи (бенчмарки) показывала на них лучшее на то время качество. Так, в задачах CoLA (бенчмарк классификационный, в нём надо определить грамматическую корректность предложения) результат вырос до 45,4 против прежнего результата в 35,0 у RNN. А вGLUE— с 72,8 до 68,9. Вторая модель была обучена в 2019 году. Она состояла из рекордных для того времени 1,5 млрд параметров (то есть была в ~10 раз больше первой), имела контекст в 1024 токена и была обучена на 40 ГБ текстовых данных. GPT-2 снова побеждала предыдущие подходы, включая GPT-1, на многихбенчмарках. По сравнению с первой версией модели у второй произошел качественный рост: теперь она могла генерировать разумные тексты — а не только предложения. Правда, не всегда и не с первой попытки. GPT-3 стала революцией с точки зрения качества и размеров. В 2020 году была получена модель размером в 175 млрд параметров, она обучалась на 570 ГБ текстовых данных с контекстом в 2048 токенов. Модельмогларешать целый спектр задач, включая перевод, суммаризацию и ответы на вопросы, с качеством, близким к человеческому уровню, а также отличалась высокой способностью генерировать креативный контент. Демонстрацию работы модели лучше посмотреть вэтой статьена 28 странице и далее. Модель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу. Однако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря — графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно много. Что с этим делать? Использовать подводки.", "useful_links": [{"text": "GLUE", "url": "https://gluebenchmark.com"}, {"text": "бенчмарках", "url": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"}, {"text": "могла", "url": "https://arxiv.org/abs/2005.14165"}, {"text": "этой статье", "url": "https://arxiv.org/pdf/2005.14165.pdf"}]}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Подводки", "text": "Оказывается, что обучать большие языковые модели решать определённые задачи не всегда нужно (как мы говорили ранее, это ресурсоёмко): можно составитьfew-shotподводку. Подводка — словесное описание поставленной задачи, составленное определенным образом. Представим, что мы хотим осуществить перевод с английского на французский. Для обучения нам необходимо было бы составить пары, где— слово на английском, а— на французском. Сделаем иначе — опишем задание на естественном языке: Здесь на английском языке сформулировано задание и предлагается слово «cheese» перевести на французский. Назовем такую конструкциюzero-shot-примером. Такой запрос GPT-3, возможно, поймёт, но работать будет плохо. Давайте увеличим количество примеров в подводке и назовем эту конструкциюone-shot: Или больше, и это будетfew-shot: При этом приёме не тратятся ресурсы на обучение модели, она лишь смотрит на контекст и генерирует продолжение. Оказывается, этого достаточно, чтобы сравняться с downstream-обучением. Продемонстрируем преимущество такого подхода на двух бенчмарках. TriviaQA — вопросно-ответный бенчмарк, составленный на основе Википедии. Он помогает оценивать знания модели и ее ответы на вопросы. Lambada — оценивает меморизацию длинного контекста модели. Чем выше скор, тем лучше модель на обоих бенчмарках. Графики выше демонстрируют несколько особенностей: Few-shotпозволяет получать качество, сравнимое с дообучением на определённом датасете, и стремится к человеческому качеству. С ростом числа обучаемых параметров модели растет её качество. На правом графикеfew-shot-примеры начинают работать лучшеzero-shot-примеров лишь с некоторого размера модели. Это говорит о том, что модель начинает демонстрировать «умные» свойства лишь начиная с некоторого размера. Few-shotдействительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо. Предположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья: Можно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторыэтой статьизаметили, что сама по себе конструкцияfew-shot-примера не приводит к стабильному результату. Качество решения задачи очень зависит от: Текстового описания задачи. Числа примеров в подводке. Порядка, в котором примеры следуют друг за другом в подводке. Формате составленияfew-shot. Чтобы улучшить качество решения задачи, авторы предлагают осуществлять калибровку подводок. В статье они заметили, что модели смещены относительно подводок, то есть переформулировка запроса ведёт к смещению в ответе модели, а также к росту разброса ответов. Например, модели задают вопрос и её задача — ответить «да» или «нет». Еслиfew-shotсостоит из четырёх примеров и они идут в порядке «да», «да», «нет», «нет», то, вероятнее всего, дальше модель ответит «нет» на любой вход, просто потому что слово «нет» встречалось последним. Калибровать модель предлагается с помощью выученного линейного преобразования: В этом преобразовании: и— обучаемые; — вероятности на выходе модели; — откалиброванные вероятности; Обучающие данные собираются так: Для различных задач собираем подводки и добавляем нейтральное слово N/A. В этом примере несмещённая модель должна давать с вероятностью 50% ответ «positive» или «negative». Чтобы добиться такого распределения ответов у смещённой модели, представим: Также всеfew-shot-примеры стандартизуются в специальный формат вопрос — ответ, как на картинке выше. Этот метод (синий график) по сравнению со стандартнымиfew-shot-примерами (красный график) помог повысить качество и уменьшить разброс результата. Таким образом, оптимизировав всего 4 параметра, авторы существенно улучшили итоговый результат. Качество работы модели зависит от подводки, иfew-shotпросто один из способов её построения. Эксперименты показывают, что грамотный подбор промта позволяет экономить на обучении и решать задачи с высоким качеством. Проблема в обучении больших моделей — нехватка оперативной памяти на GPU, поэтому не будем оптимизировать все параметры модели. Пусть необходимо решить задачу, к ней имеется обучающее множество вида. Введём дополнительные токены, которых не было в словаре:— и будем добавлять в каждый текст из X согласно какому-то правилу. Правило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец. Тогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимооптимизироватьлишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели. Получается, что можно оптимизировать подводку, или, другими словами, находить наиболее оптимальный промт, который лучше прочих решает поставленную задачу. Языковые модели призваны решать самый широкий спектр текстовых задач — вопросно-ответные, суммаризацию, диалоговость, перевод и многие другие. Получается, что модель должна после некого обучения (подбора подводки или оптимизации вообще всех параметров под каждую задачу) решать каждую из них на высоком уровне. Однако модель обычно учится на текстах из интернета, книгах и других доступных ресурcах. И формат задачи, который обычно требуется от модели, не соответствует тому, что алгоритм привык видеть на обучении. К этому стоит добавить, что среди веб-документов просьба что-то сократить или определить тональность документа встречается не очень часто. Исправить этот недостаток призваны подходы по генерализации языковых моделей:FLANиT0. Инструкции даются на естественном языке и для подготовки качественного обучающего множества предлагается произвести следующие действия: Каждой отдельной задаче (будь то перевод, написание отзывов или суммаризация) пишется по несколько различных подводок, отражающих смысл задания. Итоговый датасет составляется из отдельных задач, все строчки датасета перемешиваются случайным образом. Авторы стараются собрать как можно более разнообразные задачи в обучающее множество. Две картинки сверху демонстрируют FLAN- и T0- подходы по созданию датасета, а картинка снизу — рост усреднённого качества модели после обучения на смеси. Таким образом с некоторого размера модели наблюдается повышение метрик качества при дальнейших дообучениях генерализованной модели на отложенных задачах. Предыдущий подход со смесью датасетов помогает решать многие задачи в среднем заметно лучше. Однако есть задачи, где качество результатов модели всё ещё низкое. Например, предложить эффективный код, решающий некую алгоритмическую задачу, найти минимум некоторой аналитической функции потерь, посчитать производную фукнции в точке и так далее. Такие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стилеChain-of-Thought (CoT): CoT-подводка состоит из трёх обязательных элементов: Формулировки задачи на естественном языке. Подробного пошагового решения. Ответа на задачу. Формирование такого промта, особенно наfew-shot, заставляет модель рассуждать, как можно правильно решить задачу. Авторыэтой статьисравнили на двух математических бенчмарках способность модели решать сложные задачи. MultiArith — проверяет умение решать простые арифметически задачки. GSM8K — более сложные. Результаты демонстрируют, что наличиеCoTв подводке увеличивает способность решать математические задачки у больших языковых моделей.", "useful_links": [{"text": "этой статьи", "url": "https://arxiv.org/pdf/2102.09690.pdf"}, {"text": "оптимизировать", "url": "https://aclanthology.org/2021.emnlp-main.243.pdf"}, {"text": "FLAN", "url": "https://arxiv.org/abs/2109.01652"}, {"text": "T0", "url": "https://arxiv.org/pdf/2110.08207.pdf"}, {"text": "этой статьи", "url": "https://arxiv.org/pdf/2201.11903.pdf"}]}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "InstructGPT", "text": "Наконец, обсудив, как готовить обучающие данные, перейдем к прародителю ChatGPT. Инструкционная модель — это та, которая обучена отвечать на пользовательские запросы в режимеzero-shot(а вообще, иfew-shot, и любой человекочитаемый формат) с высоким качеством. InstructGPT — это модель, и она интересна с точки зрения выработки концепции обучения всех инструкционных моделей (InstructGPT, ChatGPT, GPT-4 и других). С некоторыми нюансами обучение состоит из четырех этапов: Подготовка качественного претрейна. Языковая модель должна содержать в себе как можно больше знаний о мире, чтобы иметь возможность в последующем решать произвольные задачи с высоким качеством. На этом этапе необходимо озаботиться наибольшим разнообразием, чистотой и полнотой обучающих данных. Подробнее об этом мы поговорим в последнем разделе этого параграфа. SFT (supervised finetuning) — обучение модели следовать инструкциям. Этот пункт мы подробно обсудили в предыдущей части параграфа (T0, FLAN, CoT). На этом этапе важно составить грамотный инструкционный датасет, где инструкция содержит произвольные запросы к модели, а ответ на неё — подробный текст, которым будущий пользователь будет доволен. Грамотный сбор таких данных довольно дорогостоящий процесс, но от него напрямую зависит, каким образом модель будет взаимодействовать с пользователем. Обучение reward-модели. Каждый ответ алгоритма можно оценить с точки зрения вежливости, подробности или персонажности. Персонажность позволяет модели считать себя, например, капитаном Джеком Воробьем и общаться на пиратском говоре. Также есть менее формализуемые критерии качества ответов, их даже сложно описать словами. Например, что в основном людям ответ 1 нравится больше чем ответ 2.Reward-модель агрегирует эти кртитерии в число — меру качества. Чем оно выше, тем качественнее ответ модели. Для выравнивания поведения модели обычно важно уметь оценивать тысячи текстов, а вручную это делать дорого и долго, поэтому обучается специальная модель-оценщик. Про то, как обучать reward-модель, будет рассказано далее. Этап Reinforcement Learning (RL). На нём языковая модель обучается генерировать такие ответы, которые имели бы наивысшую оценку относительно reward-модели. Про то, как делать RL, будет рассказано далее.", "useful_links": []}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "ChatGPT", "text": "Одна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в: Диалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных. Размере и качестве инструкционного датасета. Том, что больше внимания уделено разметке и обучению reward-модели и этапу с RL. К сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели.", "useful_links": []}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Как обучить свою LLM?", "text": "Обсудим детально на примере доступных в open-source моделей семейства LLaMA. В качестве примера возьмём самую свежую архитектуру трансформеров на первую половину 2023 года —LLaMa, а также способы превращать её в чатовую модель, проводить Alignment на примереLLaMa-2. Вторая модель архитектурно не отличается от первой (кроме увеличенного контекста до 4096 токенов), поэтому содержание статей можно объединить в один рассказ. Для обучения с нуля качественной языковой модели необходимы: мощный кластер на сотни видеокарт, на котором можно производить параллельное обучение модели. Больше GPU — больше модель можно обучить и быстрее по времени обучения; терабайты текстовых данных для тренировки на них; архитектура, которая лучшим образом может моделировать язык. Поговорим подробнее о двух последних пунктах. Текстовые данные Текстовые данные можно брать из открытых источников, таких как CommonCrawl, C4, Taiga и прочее. Важно обеспечить: чистоту данных — например, убрать html-тэги, устранить дублирование текстов; полноту — чтобы модель одинаково хорошо решала математические задачи, писала код или сочиняла стихотворения, текстов соответствующих доменов должно быть в достатке в обучающем корпусе; разнообразие данных. Существуют эмпирические законы обученности модели, но здесь остановимся на числе пройденных за обучение токенов. В LLaMa-моделях это значение варьируется от 1T до 2Т. Ниже приведены основные параметры по числу размерности внутренних эмбедингов, числу голов Attention, слоёв и параметров обучения разных моделей: Архитектура У LLaMa-моделей предлагается целый ряд архитектурных изменений. Так как в учебнике рассматривался лишь базовая архитектура трансформеров, то опишем, что в ней необходимо изменить, чтобы получить LLaMa-модель. Pre-нормализация. ПустьТогда нелинейное преобразование в общем виде выглядит так: И LayerNorm можно описать следующими формулами: В свою очередь экспериментально RMSNorm демонстрирует лучшие результаты в сравнении с LayerNorm и высчитывается так: SwiGLU-активация используется вместо ReLU.— значок поэлементного умножения матриц. Роторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе отрансформерах) или относительными (кодируется расстояние между каждой парой токенов).Роторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое модели при подсчёте Attention, а не только перед первым слоем. Это позволяет на каждом слое явно обрабатывать информацию об относительном расположении токенов. Роторные эмбединги показывают лучшее качество на многих задачах и являются стандартом для обучения языковых моделей. Подробнее о них можно почитать вэтой статье. Существуют также техники ускорения обучения моделей и оптимизации использования памяти, но с этим предлагаем читателям ознакомиться самостоятельно. Второй этап обучения инструкционных языковых моделей требует множество инструкций. Рецепт как их готовить был подробно описан всередине этого параграфа. Снова проговорим, что для написания инструкций или сбора датасета необходимо, чтобы инструкции были: разнообразными; качественными; имели одинаковый формат, чтобы чатовая модель могла обучиться диалоговости (где вопрос пользователя, где ее ответ); информативными; подробными; Chain-of-Thought (CoT),few-shotи так далее. Третий этап в создании инструкционных моделей. Есть несколько способов собрать датасет для обучения reward-модели. Он должен содержать тексты и метки к ним. Если меток много (например, в случае балльной оценки), можно использовать разновидностиранжирующих лоссов.Разберем способ обучения модели на бинарную оценку. Пусть модели подается на вход инструкция. Поменяв температуру, способ сэмплирования или использовав разные чек-пойнты модели, возможно получить два разнообразных ответаи. Не ограничивая общность, предположим, что, согласно некоторым предпочтениям, асессоры или пользователи установили, что первый ответ лучше второго. Проделаем эту операцию много раз и получим обучающее множество, состоящее из. Тогда reward-модель можно обучать минимизацией следующей функции потерь: Где: — reward-модель с обучаемыми параметрами тета; — некий margin, который определяет, насколько сильно модель должна отделять хороший и плохой ответы друг от друга. На четвёртом этапе, этапе выравнивания модели, можно воспользоваться разными алгоритмами. LLaMa-2 Chat была обучена последовательно сначала на Rejection Sampling fine-tuning (RL «для бедных») и Proximal Policy Optimization (PPO). Rejection Sampling fine-tuning. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция. Сгенерируем для неёответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше, тем больше reward-score у лучшего ответа. Собрав парыинструкция — лучший ответ, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели. Proximal Policy Optimization. Для лучшего понимания происходящего советуем прочесть параграф, посвященныйRL. Основная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция оценивает средний reward в текущем состоянии (обычно это та же самая модель с линейным слоем поверх). Формализуем термины из RL для задачи выравнивания языковой модели: Политика— обучаемая языковая модель. Value-функция— обычно та же самая модель с линейным слоем поверх, оценивает средний reward, если действовать из состояниясогласно политике. — состояние в момент времени. Это весь контекст-токенов, которые модель успела сгенерировать к текущему моменту. — действие из текущего состояния в момент времени. Обозначает следующий токен, который будет сгенерирован. — траектория, т. е. тройки, — это состояния генерируемого токена и награды за него Сразу можно сделать вывод, что в языковых моделях, Также, в RL символомобозначается вся последовательность токенов, то есть на практике сюда можно подставлять количество сгенерированных токенов. Инициализируем— начальные веса политики и value-функции Для Соберем коллекцию траекторий, следуя политике. Посчитаем. Эта формула отражает разницу между финальной наградой за выбранное действиев текущем состояниии средней финальной наградой, которую можно было бы получить в этом состоянии. Вообще говоря, с помощью метода Generalized Advantage Estimation (GAE) её можно аппроксимировать следующим выражением: Обновляем веса политики согласно одному из лоссов PPO. Например, используем такой: С помощью MSE лосса оптимизируем значение value-функции:", "useful_links": [{"text": "LLaMa", "url": "https://arxiv.org/pdf/2302.13971.pdf"}, {"text": "LLaMa-2", "url": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/"}, {"text": "трансформерах", "url": "https://academy.yandex.ru/handbook/ml/article/transformery"}, {"text": "этой статье", "url": "https://arxiv.org/pdf/2104.09864v4.pdf"}, {"text": "середине этого параграфа", "url": "https://academy.yandex.ru/handbook/ml/article/yazykovye-modeli#podvodki"}, {"text": "ранжирующих лоссов.", "url": "https://gombru.github.io/2019/04/03/ranking_loss/"}, {"text": "RL", "url": "https://academy.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem"}, {"text": "GAE", "url": "https://arxiv.org/abs/1907.00456"}]}
{"document_title": "Языковые модели", "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli", "section_title": "Итог", "text": "Мы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля.", "useful_links": []}
