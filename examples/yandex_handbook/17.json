[
  {
    "document_title": "Variational Autoencoder (VAE)",
    "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
    "section_title": "Постановка задачи",
    "text": "Давайте представим себе, что нам нужно нарисовать лошадь. Как бы мы это сделали? Наверное, сначала наметили бы общий силуэт лошади, её размер и позу, а затем стали бы добавлять детали: гриву, хвост, копыта, выбирать окраску шерсти и так далее. Кажется, что в процессе обучения рисованию мы учимся выделять для себя основной набор каких-тофакторов, наиболее важных для генерации нового изображения: общий силуэт, размер, цвет и тому подобное, а во время рисования уже просто подставляем какие-тозначенияфакторов. При этом одинаковые сочетания одних и тех же факторов могут привести к разным картинкам — ведь нарисовать что-то два раза абсолютно одинаково вы, скорее всего, не сможете. Попробуем формализовать описанный выше процесс. Пусть у нас есть датасетв многомерном пространстве исходных данных, — объектов, которые мы желаем генерировать, — и пространствоскрытых (латентных) переменныхменьшей размерности, которыми кодируются скрытые факторы в данных. Тогда генеративный процесс состоит из двух последовательных стадий (см. картинку ниже): Семплированиеиз распределения(красное) Семплированиеиз распределения(синее) То есть, рассуждая в терминах рисования картинок с лошадками, мы сначала мысленно семплируем некоторое(размер, форму, цвет, ...), затем дорисовываем все необходимые детали, то есть семплируем из распределения, и в итоге надеемся, что получившееся будет напоминать лошадку. Таким образом, построить генеративную модель в нашем случае — значит уметь семплировать с помощью описанного двустадийного процесса объекты, близкие к объектам из обучающей выборки. Говоря более формально, нам бы хотелось, чтобы наша модель максимизировала правдоподобиеэлементов обучающего множествапри описанной процедуре генерации: Предположим, что совместное распределениепараметризовано некоторым параметроми выражается непрерывной пофункцией при каждых фиксированныхи: Тогда и мы можем записать следующую задачу оптимизации: Решив её, мы построим нашу генеративную модель. Замечание 1. После приведённой выше аналогии с обучением рисованию может ошибочно показаться, что в скрытые переменные всегда заложен некоторый хорошо интерпретируемый смысл. Но на практике это всё же не обязано быть так: те скрытые переменные, которые мы найдём, могут как иметь простую интерпретацию, так и не иметь. С помощью объяснений выше мы прежде всего хотели проиллюстрировать понятие «скрытые переменные». Замечание 2. Может показаться, чтонам откуда-то уже известно, и тогда не ясно, зачем все эти сложности с введением латентных переменных и интегралами. На самом деле, мы действительно можем построитьстатистическую оценкупо данными даже пытаться генерировать новые данные с помощью таких моделей (как, например, делаетсятут). Но у статистических методов есть разные ограничения, наиболее серьёзным из которых представляется проклятие размерности: чем больше измерений у ваших данных, тем больше разнообразных примеров вам нужно для построения адекватной оценки. О проклятии размерности мы поговорим чуть подробнее далее. Замечание 3. Также может возникать вопрос — а зачем вообще нужно вводить латентные переменные, моделировать совместное распределение, а целевое распределениеопределять как маргинализациюпо? Почему такой подход в принципе должен работать? Ответ состоит в том, что, даже имея относительно простые выражения дляи, можно описать достаточно сложное распределение, что достаточно наглядно проиллюстрировано в примере ниже.",
    "useful_links": [
      {
        "text": "статистическую оценку",
        "url": "https://en.wikipedia.org/wiki/Density_estimation"
      },
      {
        "text": "тут",
        "url": "https://scikit-learn.org/stable/modules/density.html"
      }
    ]
  },
  {
    "document_title": "Variational Autoencoder (VAE)",
    "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
    "section_title": "Обучение VAE",
    "text": "Прежде чем пытаться решать задачу оптимизациидавайте подумаем, а как мы вообще могли бы посчитать такой интеграл? Первое, что приходит на ум, — попробовать получить его приближённое значение методом Монте-Карло: где в последнем переходе мы используем сэмплы. Однако, еслии— достаточно большое, мы столкнёмся спроклятием размерности— количество семплов, необходимых для того, чтобы хорошо покрыть, растёт экспоненциально с ростом: Есть ли способ как-то сократить число необходимых семплов для подсчёта? На самом деле, часто оказывается, что далеко не все возможныеотображаются в элементы, и вклад большинствав оценкупрактически нулевой. Это наводит на мысль, что для каждогонам может пригодиться знание распределениятаких, которые являются прообразами. Мы можем предположить, что распределениепараметризовано некоторым семейством параметров: Зная распределение, мы могли бы семлировать уже только из него, а не из всего, и, если распределениеокажется достаточно хорошим, число необходимых семплов значительно сократится. О том, как построить, мы поговорим позже. Сейчас стоит обратить внимание на то, что процессы семплирования из распределенийивзаимно обратны друг к другу: первое отображает элементы датасета в подмножество латентного пространства, то есть действует какэнкодер, а второе отображает латентные переменные в подмножество, то есть действует какдекодер: Так как оба эти распределения будут участвовать в обучении VAE, возникает аналогия между VAE имоделями-автоэнкодерами, имеющими похожую структуру. Сейчас у нас всё готово для того, чтобы записать общий вид функции потерь для обучения вариационного автоэнкодера. Напомним, что мы обучаем модель путём максимизации правдоподобияпо. Для удобства мы перейдём к логарифму правдоподобия: Оптимизировать напрямую это выражение тяжело из-за проклятия размерности, обсуждавшегося в прошлом разделе. Чтобы победить проклятие размерности, мы хотели бы заменить семплирование из априорного распределенияна семплирование из, для чего придётся осуществить некоторый трюк. Для любого, отличного от нуля для всех, мы можем выписать следующую цепочку равенств: Второе слагаемое в последнем равенстве —-дивергенция междуи, которая, как известно, неотрицательна: А первое слагаемое — это величина, именуемая в английской литературеevidence lower bound (ELBO): Первое слагаемое в последнем переходе обычно называютreconstruction loss, так как оно оценивает качество восстановления декодером объектаиз его латентного представления. А второе играет роль регуляризационного члена и подталкивает распределение, генерируемое энкодером, быть ближе к априорному распределению. Так как-дивергенция неотрицательна, ELBO является нижней границей для логарифма правдоподобия данных: Посмотрим повнимательнее на равенства, которые мы выписали. Функциюможно оптимизировать градиентным спуском (SGD), предварительно выбрав удобный вид для,и. Максимизируя, мы растим, тем самым улучшая нашу генеративную модель. Оптимизацию ELBO с помощью SGD мы будем подробно обсуждать в следующем разделе. Максимизируя, мы одновременно минимизируем. Распределениеоценивает, из какихмог бы быть сгенерирован объект, и заранее оно нам не известно. Но если мы выберем достаточно большую модель для, тов процессе оптимизации может очень сильно приблизиться к, и тогда мы будем напрямую оптимизировать. Заодно мы получаем приятный бонус: для оценки распределения прообразовмы сможем использоватьвместо невычислимого. То есть, которое мы при выводе формулы ввели в рассмотрение как произвольное распределение, действительно будет играть роль энкодера для модели. Важное свойство ELBO в том, что его можно оптимизировать градиентным спуском относительно параметрови. Если объекты датасетанезависимы и одинаково распределены, тозапишется как сумма (или среднее) значенийна объектах: Значенияи их градиентыв общем случае вычислить невозможно, однако можно получить их несмещённые оценки, что позволит нам использовать стохастический градиентный спуск. Оценку для градиента по параметрамполучить несложно: где в последней строчке. Однако оценку на градиент по параметрамполучить сложнее, ведь они также участвуют и в семплировании: В общем случае эта проблема не разрешима. Однако некоторые распределения позволяют применитьрепараметризацию (reparameterization trick): представить переменнуюкак обратимую дифференцируемую функцию от случайного шума, параметрови переменнной: Здесь распределениене зависит оти. Например, пусть. Тогдаможет иметь следующий вид: После такой замены мы сможем получить оценку на градиент по: где в последней строчке. Репараметризация хорошо иллюстрируется следующей картинкой: Здесь— функция потерь. Значенияна обеих схемах одинаковы, но на левой картинке градиенты порассчитать не получится, так как мы не можем дифференцировать по случайной переменной. Однако на правой картинке источник случайности перемещается во входные данные благодаря репараметризации, а градиенты вычисляются по детерминированным переменным. Таким образом, мы получили сетап, типичный для оптимизации с помощью SGD: там мы приближаем градиент функции потерь по случайным батчам входных данных, а здесь роль случайных батчей играют одновременно батчи из переменныхи случайных переменных. Кроме нормального распределения, есть довольного много примеров распределений, допускающих репараметризацию. Их можно найтипо ссылкев разделе \"The reparameterization trick\". Однако большая часть реализаций VAE используют именно нормальное распределение. В итоге примерный алгоритм обучения VAE такой: Скопировать код1dataset = np.array(...)2epsilon = RandomDistribution(...)34# Энкодер q_phi(z|x) — нейронная сеть с параметрами phi5encoder = Encoder()67# Декодер p_theta(x|z) — нейронная сеть с параметрами theta8decoder = Decoder()910for step in range(max_steps):11# Семплируем батч исходных данных и случайного шума12batch_x = sample_batch(dataset)13batch_noise = sample_batch(epsilon)1415# Считаем параметры распределения q(z | x) с помощью энкодера16latent_distribution_parameters = encoder(batch_x)1718# Делаем репараметризацию (семплируем из q(z | x))19z = reparameterize(latent_distribution_parameters, batch_noise)2021# Декодер отдаёт параметры выходного распределения22output_distribution_parameters = decoder(z)2324# Вычисляем ELBO и обновляем параметры моделей25L = -ELBO(26latent_distribution_parameters,27output_distribution_parameters,28batch_x29)30L.backward() Стоит подчеркнуть, что декодер выдаёт именно параметры выходного распределения, а не конкретный семпл из этого распределения. Например, если вы моделируете выходные изображения с помощью нормального распределения, то декодер на выходе предскажет некоторыеи, которые вместе с параметрами латентного распределения (выход энкодера) будут поданы в ELBO. Для генерации конкретной картинки на этапе инференса нужно будет либо честно провести семплирование из, либо, как часто делают, просто взять среднеев качестве выходного изображения. В общем случае конкретный способ проведения инференса зависит от вида используемого выходного распределения. Пришло время привести примеры конкретных,и, с которыми можно построить VAE. Для начала предположим, чтоможно положить равным стандартному нормальному распределению: Заметим, что в этом случае у априорного распределенияотсутствует зависимость от параметров. Распределениезависит от того, к какому распределению принадлежат ваши данные. Если ваши данные имеют непрерывное распределение, томожно задать, например, как гауссовское распределение: Вектор средних в этом примере определяется функциейс переменнымии, а матрица ковариаций определяется постоянной диагональной матрицей. Функциюможно задать с помощью нейронной сети с параметрами. При желании, матрицу ковариаций тоже можно задавать некоторой функцией и не ограничивать её вид только постоянными матрицами. Если же ваши данные дискретны, то может подойти категориальное распределение: в котором вектор вероятностей— выход нейросети после применения. Если у вас бинарные данные, вы можете использовать бернуллиевское распределение: где— выход нейронной сети после применения сигмоиды. Распределениеможет, в принципе, быть любым, но в самом простом случае оно имеет вид гауссовского распределения c диагональной матрицей ковариаций: Такое распределение позволяет, в частности, применить репараметризацию, обсуждавшуюся выше. Если выбратьдвумерным, то распределения, определямые, хорошо визуализируются: Вычислим его для приведённых выше распределений. Начнём с.-дивергенция между распределениямииравна: где— размерность этих распределений. Вывод этого соотношения можно найтиздесь. В нашем случае,и Тогда ELBO будет вычисляться как: где. Как было упомянуто вэтой статьеот авторов VAE в разделе 2.3, число семплированийможно положить равным единице при достаточно большом размере батча (например, 100). Если вы выберете биномиальное, то Если гауссовское, то Пример реализации обучения и применения VAE на датасете MNIST на Keras можно найтиздесь, а на PyTorch —здесь. Когда мы обучили VAE, мы сможем генерировать новые семплы, просто подаваяна вход декодеру: ![2](https://yastatic.net/s3/education-portal/media/vae_decoder_diagram_385be2e566_6c396a28e8.svg\"> Энкодер для генерации новых семплов не нужен. Однако нам может понадобиться оценитьдляиз тестового множества, чтобы понять, с какой вероятностью модель сможет сгенерировать. Для оценки интеграла нам нужно насемплировать некоторое количество, и если брать семплы из, то оценка может плохо сойтись. Но можно снова использовать ELBO как нижнюю границу дляи оценивать уже её, семплируя из распределения. Такая оценка сойдётся быстрее и даст примерное представление о том, насколько хорошо модель справляется с конкретным примером. Также интересно бывает взглянуть на то, как распределены коды обучающих примеров в латентном пространстве. Так, например, может выглядеть распределение латентных кодов цифр MNIST для обученного VAE в двумерном латентном пространстве: Разные типы цифр обозначены разными цветами (соответствие цифр и цветов показано на шкале сбоку). Здесь видно, что лучше всего модель различает нули и единицы, а восьмёрки и тройки — хуже всего. Стоит, конечно, отметить, что латентное пространство выбрано двумерным в целях визуализации, и при большей его размерности модель могла бы научиться различать цифры более качественно. Для двумерного латентного пространства есть ещё один интересный способ визуализировать структуру многообразия, выученного VAE. Можно взять равномерную сетку на единичном квадрате и отобразить её в латентное пространство, применив к ней функцию, обратную к CDF нормального распределения. Полученные семплы можно подать в декодер и посмотреть, какие картинки будут соответствовать узлам сетки: Здесь изображены примеры, сгенерированные для датасетов Frey Face и MNIST (оба доступны поссылке). Такая визуализация позволяет увидеть плавный переход латентных кодов одних объектов в коды других, а также взаимное расположение латентных кодов. Для MNIST снова видно, в частности, что коды нулей и единиц модель разнесла далеко друг от друга, а коды троек и восьмёрок очень близки. А ещё интересно наблюдать плавный переход от шестёрок к нулям и от семёрок к единицам. Для Frey Face видно, что весёлые лица расположены далеко от грустных, а по главной диагонали квадрата можно проследить плавный переход от серьёзного лица к улыбающемуся. Ещё интересно посмотреть на то, как меняется качество генерируемых цифр в зависимости от размерности латентного пространства (на картинках просто случайные семплы из модели): Заметный переход виден между размерностями 2 и 5, дальнейший рост размерности почти не оказывает значимого эффекта. Иногда мы можем захотеть сгенерировать не просто какой-то произвольный объект из датасета, а относящийся к конкретной группе или классу. Ранее мы выписывали уравнение для: Все распределения, участвующие в этом уравнении, мы можем сделать обусловленными по переменной: Переменнаяможет быть лейблом объектаили вообще произвольным тензором, как-то характеризующим. Вместо, единого для всехиз обучающей выборки, для каждого значениятеперь будет отдельное априорное распределение. Переменнаяможет принимать и дискретные, и непрерывные значения. Она может даже, например, быть половиной изображения, которую модели предлагается дополнить. На всякий случай подчеркнём, что обучение CVAE — это не то же самое, что обучение нескольких независимых VAE, так как веса CVAE общие для всех классов. На уровне имплементации это реализуется довольно просто: нужно всего лишь сконкатенировать входы энкодера и декодера с тензором, соответствующим. Еслиимеет категориальные значения, то бывает полезно предварительно закодировать их one-hot векторами. Алгоритм будет примерно таким: Скопировать код1dataset, labels = np.array(...), np.array(...)2epsilon = RandomDistribution(...)34# Энкодер q_phi(z|x) — нейронная сеть с параметрами phi5encoder = Encoder()67# Декодер p_theta(x|z) — нейронная сеть с параметрами theta8decoder = Decoder()910for step in range(max_steps):11# Семплируем батч исходных данных, лейблов и случайного шума12batch_x = sample_batch(dataset)13batch_y = sample_batch(labels)14batch_noise = sample_batch(epsilon)1516# Подаём в энкодер конкатенацию входных данных и лейблов17encoder_input = concatenate([batch_x, batch_y])1819# Считаем параметры распределения z с помощью энкодера20latent_distribution_parameters = encoder(encoder_input)21# Делаем репараметризацию22z = reparameterize(latent_distribution_parameters, batch_noise)2324# Конкатенируем полученный случайный вектор и лейблы25decoder_input = concatenate([z, batch_y])2627# Декодер отдаёт нам выходное изображение28output_distribution_parameters = decoder(decoder_input)2930# Вычисляем ELBO и обновляем параметры31L = -ELBO(32latent_distribution_parameters,33output_distribution_parameters,34batch_x35)36L.backward() Реализацию CVAE на PyTorch и Tensorflow можно найти, например,здесь. Если визуализировать распределение латентных кодов для цифр MNIST, полученных после обуславливания модели на класс цифры, то можно увидеть что-то такое: Мы видим непонятную смесь из точек вместо явных кластеров, которые выделяла обычная модель VAE. Однако дело тут в том, что, вместо того, чтобы пытаться размещать все цифры в одном пространстве, модель использует отдельное латентное пространстводля каждой цифры: На картинке справа — априорные распределения для цифр 6 и 7, а слева — визуализация структуры выученных многообразий для этих цифр, построенная так же, как аналогичная визуализация для VAE. Качество изображений каждой отдельной цифры заметно повышается: Видно, что вариабельность генерации цифр теперь тоже заметно выросла, и модель может имитировать написание цифр разными почерками.",
    "useful_links": [
      {
        "text": "",
        "url": "#eq:main_problem"
      },
      {
        "text": "",
        "url": "#eq:main_problem"
      },
      {
        "text": "моделями-автоэнкодерами",
        "url": "https://en.wikipedia.org/wiki/Autoencoder#:~:text=Machine%20learninganddata%20mining.%20v.%20t.,network%20to%20ignore%20signal%20%E2%80%9Cnoise%E2%80%9D"
      },
      {
        "text": "по ссылке",
        "url": "https://arxiv.org/pdf/1312.6114.pdf"
      },
      {
        "text": "здесь",
        "url": "https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/"
      },
      {
        "text": "этой статье",
        "url": "https://arxiv.org/pdf/1312.6114.pdf"
      },
      {
        "text": "здесь",
        "url": "https://blog.keras.io/building-autoencoders-in-keras.html"
      },
      {
        "text": "здесь",
        "url": "https://github.com/pytorch/examples/blob/master/vae/main.py"
      },
      {
        "text": "https://yastatic.net/s3/education-portal/media/vae_decoder_diagram_385be2e566_6c396a28e8.svg",
        "url": "https://yastatic.net/s3/education-portal/media/vae_decoder_diagram_385be2e566_6c396a28e8.svg"
      },
      {
        "text": "ссылке",
        "url": "https://cs.nyu.edu/~roweis/data.html"
      },
      {
        "text": "здесь",
        "url": "https://github.com/wiseodd/generative-models/tree/master/VAE/conditional_vae"
      }
    ]
  },
  {
    "document_title": "Variational Autoencoder (VAE)",
    "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
    "section_title": "Обзор статей",
    "text": "Кроме стандратного описания работы VAE, приведём результаты нескольких недавних интересных работ, базирующихся на идее VAE. МоделиVQ-VAEиVQ-VAE-2интересны тем, что в них в качестве априорных распределений были задействованы дискретные распределения. В каких ситуациях дискретные распределения могут быть более применимы, чем непрерывные? Например, если мы имеем дело с токенам в задачах NLP или фонемами в обработке речи. Картинки также можно было бы кодировать некоторым набором из целых чисел: например, одно число могло бы кодировать тип объекта, другое — его цвет, третье — цвет фона и так далее: Кроме того, существуют довольно мощные алгоритмы (например,Трансформер), предназначенные для работы с дискретными данными. Выучивание хороших дискретных представлений даёт возможность эффективно использовать такие алгоритмы для, например, задачи генерации картинок. Авторы VQ-VAE вводят дискретное латентное пространство в видевещественных векторовразмерности. Векторы из этого пространства называютсякодовыми векторамииликодами. На рисунке ниже приведена примерная схема обучения предлагаемой модели. Энкодер принимает на вход картинкуи выдаёт на выходе тензор. На рисунке этот тензор имеет размерность: последняя размерность совпадает с длиной кодовых векторов, а— это пространственная размерность выхода CNN (для простоты мы здесь не пишем явно размерность батчей). Каждый извекторов изотображается в ближайший к нему по-расстоянию кодовый вектор. После такой процедуры тензорпереходит в тензор, состоящий изкодовых векторов. Декодер получает на вход тензори отображает его в исходную картинку. Для работы с речью и текстами авторы использовали двумерный тензорвместо трёхмерного. Выходное распределение энкодераопределено здесь следующим образом: Во время обучения в качестве априорного распределения в латентном пространстве используется равномерное распределение, поэтому слагаемоеоказывается постоянным и равным: В точках, где, предпоследнее выражение продолжается нулём по непрерывности. Таким образом, ELBO для таких распределений примет вид где— параметры декодера. При оптимизацииможно не учитывать. Отображение выхода энкодера в кодовые векторы не дифференцируемо, поэтому при обучении применяется следующий трюк: при обратном проходе градиент копируется напрямую из декодера в энкодер, пропуская при этом слой, отображающий выходы энкодера в кодовые векторы. Этот трюк очень близок к приёму, известному какstraight-through estimator, впервые предложенному в этойстатье(а его простое описание можно найтитут). Использование straight-through estimator, однако, не позволяет обучать сами кодовые векторы, так как по ним не будут вычисляться градиенты. Поэтому лосс для обучения модели складывается из трёх компонент: Здесьобозначает оператор остановки дифференцирования: через его аргумент не текут градиенты. В статье лосс записан несколько иначе: Эти обозначения кажутся несколько путающими по двум причинам: Буквав нижнем индексепризвана обозначить только то, что это выход энкодера, а не наличие связи между кодовыми векторамии параметрами энкодера. Но второе довольно легко для себя предположить. Вычитаниеобозначает вычитание не всех элементов словаря из соответствующей позиции тензора, а только лишь ближайшего соседа к элементуна этой позиции. То есть по факту вычитаниев этой записи равносильно вычитанию. Это не уточняется в статье, но можноувидетьв официальной реализации. Первое слагаемое — это ELBO с точностью до константы. Второе слагаемое отвечает за сдвиг кодовых векторов в сторону выходов энкодера. Чтобы не получилось так, что выходы энкодера всё время меняют кодовые векторы за счёт второй компоненты лосса, а сами на каждой итерации выдают векторы, далёкие от текущих кодовых векторов, добавляется третье слагаемое. Оно отвечает за то, чтобы энкодер стремился выдавать векторы, близкие к кодовым векторам, а его значимость регулируется с помощью коэффициента. Однако при обучении мы потеряли регуляризационное слагаемое, из-за чего распределение энкодера не было обязано приближать собой априорное распределение и осталось его узким подмножеством. Из-за этого с наибольшей вероятностью при семплировании из равномерного категориального распределения мы будем получать просто шумы вместо хороших картинок: Чтобы исправить эту проблему, авторы предлагают с помощью дополнительной модели выучить априорное распределениетех латентных переменных, которые модель научилась генерировать в процессе обучения. Поскольку любое кодовое представление можно вытянуть в последовательность, а самих кодов — конечное наперёд заданное число, то эта задача близка к задаче обучения языковой модели. Действительно, ведь там мы должны по последовательности предыдущих слов предложения предсказать следующее слово из доступного словаря, а в нашем случае — по входной последовательности дискретных латентных кодов предсказать следующий латентный код. Для картинок авторы предложили моделировать априорное распределение латентных кодов с помощью PixelCNN. Детали архитектуры и обучения этой модели можно найти в оригинальнойстатье, здесь мы опишем только общую идею. PixelCNN последовательно генерирует пиксели картинки, двигаясь из верхнего левого угла в правый нижний. Она проходит все ряды последовательно от верхнего до нижнего, а внутри каждого ряда движется слева направо: Для цветных картинок каналы (R, G, B) также моделируются последовательно: канал B при генерации зависит от R и G, а G — только от R. При предсказании значения каждого следующего пикселя модель использует значения уже сгенерированных соседей из некоторого окружающего квадрата. Чтобы модель не могла читать пиксели, идущие после текущего предсказываемого пикселя, используется специальная маска, пример которой изображён на правой части рисунка. В случае VQ-VAE обучение PixelCNN происходит не на пикселях, а на латентных кодах. Семплирование из выученного априорного распределения выглядит гораздо лучше, чем попытки семплировать из равномерного: Для аудио вместо PixelCNN авторами используетсяWaveNet. При обучении моделей априорных распределений есть возможность подавать метки классов, чтобы потом можно было семплировать из этих классов (принцип тот же, что и для CVAE). Результаты реконструкции картинок из ImageNet с помощью VQ-VAE выглядят довольно неплохо (под реконструкцией понимается выход полной модели, состоящей из энкодера и декодера): А так выглядят результаты семплирования из VQ-VAE с априорным распределением, выученным PixelCNN: Модель VQ-VAE-2 — это расширение VQ-VAE. Она показывает значительный скачок по качеству генерируемых изображений: Впечатляет то, что на картинке именно результат семплирования из выученного моделью распределения, а не результат реконструкции. Первое основное отличие модели VQ-VAE от VQ-VAE-2 — использование иерархических латентных переменных: Прежде чем перейти к описанию архитектуры, хочется сделать небольшой дисклеймер: когда в тексте далее будет говориться «тензор размера», то будет иметься в виду, что тензор имеет шейп, где первая размерность соответствует батчам, а последняя — каналам. На картинке показан пример двухуровневой архитектуры (хотя уровней может быть и больше). Каждому уровню соответствуют свои энкодер, декодер и набор кодовых векторов (общей размерностидля всех уровней). Обозначим нижний и верхний энкодеры каки, а декодеры — каки. принимает на вход трёхканальную картинку размерапикселей, отображает её в тензор размераи передаёт на вход.выдаёт тензор размера, который затем отображается в тензор из кодовых векторов(квантизуется) передаётся на вход, затем выходыиконкатенируются и квантизуются в иконкатенируются и передаются на вход, который отображает их в исходную картинку Для обучения модели используется почти такой же лосс, как для VQ-VAE. Для VQ-VAE он имел вид: Для VQ-VAE-2 первое и третье слагаемые сохраняют свой вид, а второе слагаемое заменяется на обновление кодовых векторовс помощью экспоненциального скользящего среднего. Пусть— выход энкодера на шаге, выпрямленный в двумерный тензор, последняя размерность которого равна размерностикодовых векторов. Пусть— множество извекторов, для которых на шагеближайшим оказался кодовый вектор. Тогда обновлениена шагепроисходит по следующим формулам: Здесь— некоторый вещественный параметр. Так же, как и для VQ-VAE, априорное распределение для VQ-VAE-2 выучивается отдельно уже после обучения основной модели, но в случае VQ-VAE-2 оно имеет иерархическую структуру. На картинке изображён пример такого распределения для двухуровневой архитектуры: Для каждого уровня обучается отдельная модель PixelCNN: одна — на кодовых векторах первого уровня, вторая — на кодовых векторах первого и второго уровней. Обе модели также принимают на вход метку класса, изображение из которого нужно насемплировать. Семплирование из финальной модели происходит так: семплируются векторыиз верхнего распределения из нижнего распределения семплируются векторыпри условии векторов декодер принимает на вход векторыии выдаёт финальную картинку Результаты семплирования из двухуровневой модели VQ-VAE-2, обученной на ImageNet: А это — результаты семплирования из трёхуровневой модели VQ-VAE-2, обучавшейся наFFHQ: Одна из недавних работ, связанных с VAE, — этоDALL-Eот OpenAI. Они обучили модель с 12 миллиардами параметров, генерирующую картинки по их текстовому описанию. Для обучения авторами был собран датасет, состоящий из 250 миллионов пар картинок и их описаний. Вот примеры работы этой модели: Вблог-постеOpenAI, посвящённом DALL-E, есть возможность самостоятельно составлять текстовые описания из некоторого ограниченного словаря и смотреть на результаты. Осторожно, это затягивает 😃 DALL-E идейно основывается на результатах VQ-VAE: сначала выучиваются кодовые векторы для картинок, а затем обучается Трансформер, моделирующий совместное априорное распределение текстов и кодовых векторов. Подробнее о трансформерах мырассказывалив главе 6.3 этого хендбука. В DALL-E задействована архитектура, основанная на декодер-части исходной архитектуры Трансформера, поэтому стоит такжепочитатьпро модель GPT-2, работающую аналогичным образом. Обучение проходит в две стадии: Сначала обучается дискретизованный VAE (dVAE) c энкодером для сжатия RGB-картинок размерав тензор изкодовых векторов. Эта стадия обучения очень напоминает VQ-VAE, но вместо добавления в лосс дополнительных слагаемых для кодовых векторов авторы DALL-E используютрелаксацию Гумбеля— трюк, позволяющий проводить честное дифференцирование по параметрам энкодера. Об обучении dVAE мы будем говорить подробнее далее. Затем обучается Трансформер (точнее, только декодер-часть исходной архитектуры Трансформера), задача которого — выучить совместное распределение картинок и их текстовых описаний. Он принимает на вход конкатенацию из эмбеддингов текстовых токенов и кодовых векторов картинок и учится для каждой входной последовательности предсказывать её продолжение. О некоторых деталях обучения Трансформера также будет рассказано далее. Инференс обученной модели происходит так: эмбеддинги текстового описания картинки подаются на вход Трансформеру, и он авторегрессионно предсказывает кодовые векторы картинки, соответствующей этому описанию, а затем полученные кодовые векторы пропускаются через декодер dVAE. Обучение dVAE происходит путём максимизации ELBO для картиноки их дискретных латентных представлений: гдеи— параметры энкодера и декодера дискретизованного VAE, a— равномерное категориальное распределение над кодовыми векторами. Здесь можно заметить дополнительный коэффициент, который в стандартном VAE всегда равен 1. Однако авторы DALL-E ввели дополнительный параметр, опираясь на результатыстатьио-VAE. Но, в отличие от исходной статьи, в их экспериментах значениепостепенно понижается в ходе обучения. Энкодер dVAE отображает картинки размерав тензорс шейпом, где— число кодовых векторов. То есть каждой изпозиций энкодер сопоставляет категориальное распределение надкодовыми векторами, параметризованное выходными логитами. Для получения тензораиз кодовых векторов можно было бы сначала применитьк распределениям на каждой изпозиций, а затем сопоставить каждой позиции кодовый вектор, номеру которого соответствует максимальная вероятность (взятьдля этой позиции). Однако операцияне дифференцируема, и, к тому же, в концепции VAE на вход декодеру должен пойти семпл из распределения, предсказываемого энкодером, а взятиена каждой позиции не является семплированием из предсказанного распределения. Поэтому нам потребуется применение некоторых трюков, которые позволят нам одновременно: аппроксимировать семплирование из сделать семплирование дифференцируемым Первый трюк известен в англоязычной литературе как Gumbel-Max Trick. Представим, что у нас есть логиты-выходы сетки, и мы хотим с их помощью получить семпл из категориального распределения, то есть стохастически предсказать класс. Для этого мы обычно применяем к логитам, чтобы получить вероятности: а затем из получившегося категориального распределениясемплируем класс. Оказывается, этим двум шагам будет эквивалентна следующая процедура: насемплировать числаиз стандартного распределенияГумбеля, прибавить к каждому из логитовсемпл, выбрать класс, такой что. О том, почему это действительно так, можно почитатьздесь. Однако сам по себе Gumbel-Max Trick нам не поможет — ведь операция так и не стала дифференцируемой. Поэтому придётся использовать ещё один трюк, предложенный практически одновременно в двух статьях (перваяивторая) и названный Gumbel-Softmax в одной из них. Чтобы описать этот трюк, отметим, что результат операции— это индекс некоторого класса. Такой индекс можно описать one-hot кодированием, то есть вектором длиной, в котором все элементы равны нулю, кроме-го, который равен единице. Gumbel-Softmax состоит в том, чтобы вместо взятияна последнем этапе Gumbel-Max Trick делать следующее: вычислить,, — аппроксимацию one-hot при помощис температурой сложить кодовые векторыс весами: выдать векторв качестве латентного вектора для данной позиции На самом деле авторы DALL-E не уточняли, как выходной векторагрегируется из кодовых векторов и, но такой подход применён вреализацииDALL-E на PyTorch. Присемплирование из распределениястремится к, и в процессе обучения dVAE авторы постепенно уменьшали значение. На следующей картинке слева — просто Gumbel-Max Trick, а справа — дифференцируемый вариант Gumbel-Max Trick: Таким образом, для обучения кодовых векторов для dVAE не требуется дополнительных слагаемых в лоссе относительно ELBO, а также копирования градиентов из декодера в энкодер (как было в VQ-VAE). Кроме того, стоит отметить, чтов данном случае не вырождается в константу, а действительно действует как регурялизатор, заставляя категориальное распределение, параметризованное логитами энкодера, быть ближе к равномерному распределению над кодовыми векторами. Ещё один трюк в обучении dVAE касается выходного распределения. Авторы DALL-E подметили проблему, возникающую при часто встречающемся выборе лапласовского и гауссовского распределений в качестве: оба они определены на всей вещественной прямой, в то время как пиксели принимают значения из ограниченного интервала. Таким образом, часть плотности при моделировании «теряется», оказываясь вне возможных границ значений пикселей. Чтобы исправить эту проблему, авторы предлагают использовать распределение, которое они назвали “Logit-Laplace”. Его плотность определена на интервалеи выражается следующей формулой: Эта плотность соответствует случайной переменной, полученной применением сигмоиды к распределённой по Лапласу случайной переменной. Выражение для распределения Logit-Laplace можно получить по стандартной формуле для плотности случайной величины, полученной применением монотонной дифференцируемой функции к другой случайной величине (см. формулу, например,тут). Логарифм этой плотности подставляется в ELBO вместо. Декодер на выходе выдаёт 6 тензоров: первые три соответствуютдля RGB-каналов, оставшиеся три соответствуют, и эти 6 тензоров используются для подсчёта лосса. При подаче в энкодер значения картинок нормируются функцией: Этим авторы добиваются того, чтобы декодер моделировал значения из, что позволяет нивелировать вычислительные проблемы, связанные с делением нав формуле плотности. Во время инференса реконструкциякартинкивычисляется по формуле: где— первые три тензора из выхода декодера. Выходы, соответствующие, при этом не используются. На втором этапе авторы фиксируют параметрыии моделируют совместное распределение картинок и их текстовых описаний с помощьюSparse Transformerс 12 миллиардами параметров. На вход он получает конкатенацию из текстового описания картинки и её кодовых векторов. Картинка представляется 1024 кодовыми векторами, получаемыми из энкодера, причём при семплировании кодовых последовательностей используется обычныйбез добавления шума из распределения Гумбеля. Текстовое описание токенизируется с помощью процедуры BPE (см. раздел про BPEздесь), и каждому токену ставится в соответствие представляющий его вектор из вещественных чисел (эмбеддинг). Для представления текста используется не более 256 токенов, а размер используемого словаря — 16 384 токена. Задача Трансформера во время обучения — для каждого начального отрезка входной последовательности предсказать следующий за ним токен. Это может быть как текстовый токен, так и кодовый вектор картинки. Поскольку кодовые векторы картинок всегда идут за текстовыми токенами, при генерации кодовых векторов attention-механизм учитывает также и все предыдущие текстовые токены. Кроме того, маска attention для кодовых векторов учитывает, что исходно они расположены не линейно друг за другом, а на прямоугольной сетке. В статье приводится несколько вариантов геометрических паттернов, которые использовались для attention-маски на кодовых векторах. В качестве лосса используется взвешенная сумма кросс-энтропии для текстовых токенов и кросс-энтропии для кодовых векторов картинок c весамиисоответственно (больший приоритет отдаётся генерации картнок, отсюда и больший вес для лосса). Конечно, огромный Трансформер обучить крайне непросто, и очень существенная часть статьи посвящена трюкам, которые авторы применили для обучения такой большой модели. На этапе инференса в модель подаются токены текстового описания картинки, и на их основании модель авторегрессионно предсказывает кодовые векторы: Кодовые векторы картинки подаются в декодер dVAE, который отображает их в финальную картинку: Для повышения качества предсказания авторы сначала генерируют 512 картинок для каждого текстового описания, а затем выбирают лучшую картинку из предсказанных. Разные наборы кодовых векторов для одного и того же текста можно получить, например, случайно выбирая на каждом шаге генерации какой-то кодовый вектор согласно предсказанному Трансформером распределению. Ранжирование полученных 512 картинок осуществляется с помощьюCLIP— большой нейросети, обучавшейся в режиме без учителя на большом количестве данных моделировать совместное распределение картинок и текстов.",
    "useful_links": [
      {
        "text": "VQ-VAE",
        "url": "https://arxiv.org/pdf/1711.00937v2.pdf"
      },
      {
        "text": "VQ-VAE-2",
        "url": "https://arxiv.org/pdf/1906.00446.pdf"
      },
      {
        "text": "Трансформер",
        "url": "https://arxiv.org/abs/1706.03762"
      },
      {
        "text": "статье",
        "url": "https://arxiv.org/pdf/1308.3432.pdf"
      },
      {
        "text": "тут",
        "url": "https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html"
      },
      {
        "text": "увидеть",
        "url": "https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L113"
      },
      {
        "text": "статье",
        "url": "https://arxiv.org/pdf/1606.05328v2.pdf"
      },
      {
        "text": "WaveNet",
        "url": "https://deepmind.com/blog/article/wavenet-generative-model-raw-audio"
      },
      {
        "text": "FFHQ",
        "url": "https://paperswithcode.com/dataset/ffhq"
      },
      {
        "text": "DALL-E",
        "url": "https://arxiv.org/pdf/2102.12092.pdf"
      },
      {
        "text": "блог-посте",
        "url": "https://openai.com/blog/dall-e/"
      },
      {
        "text": "рассказывали",
        "url": "https://education.yandex.ru/handbook/ml/article/transformery"
      },
      {
        "text": "почитать",
        "url": "https://jalammar.github.io/illustrated-gpt2/"
      },
      {
        "text": "статьи",
        "url": "https://arxiv.org/pdf/1904.10509.pdf"
      },
      {
        "text": "Гумбеля",
        "url": "https://en.wikipedia.org/wiki/Gumbel_distribution"
      },
      {
        "text": "здесь",
        "url": "https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/"
      },
      {
        "text": "первая",
        "url": "https://arxiv.org/pdf/1611.01144.pdf"
      },
      {
        "text": "вторая",
        "url": "https://arxiv.org/pdf/1611.00712v3.pdf"
      },
      {
        "text": "реализации",
        "url": "https://github.com/lucidrains/DALLE-pytorch/tree/main/dalle_pytorch"
      },
      {
        "text": "тут",
        "url": "https://en.wikipedia.org/wiki/Probability_density_function"
      },
      {
        "text": "Sparse Transformer",
        "url": "https://openai.com/blog/sparse-transformer/"
      },
      {
        "text": "здесь",
        "url": "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html"
      },
      {
        "text": "CLIP",
        "url": "https://openai.com/blog/clip/"
      }
    ]
  },
  {
    "document_title": "Variational Autoencoder (VAE)",
    "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
    "section_title": "Заключение",
    "text": "Итак, в этом параграфе мы поговорили о том, как устроен VAE в классическом смысле, — с непрерывным распределением латентных переменных, а также поговорили о работах, основанных на идеях использования дискретных распределений для VAE. Конечно, различные модификации VAE не исчерпываются только лишь отказом от непрерывных латентных переменных в пользу дискретных. Есть множество других возможных направлений для улучшения модели: использование иерархических латентных распределений (которые мы, кстати, видели в контексте VQ-VAE-2), использование функций потерь, отличающихся от ELBO, выбор различных форм латентных пространств, применение adversarial-обучения и многое другое. Хороший список различных статей, посвящённых модификациям VAE, можно найтиздесь. Из недавних работ, связанных с применением иерархических распределений, интересной кажетсяNVAE— семплы из модели выглядят весьма впечатляюще. Про неё есть хорошийвидеообзорот Yannic Kilcher. На этом мы завершаем рассказ о VAE. Будем надеяться, что он дал вам общее представление и об исходных идеях, из которых выросла модель VAE, и о наиболее интересных последних результатах, связанных с ней. А в следующем параграфе мы поговорим о генеративно-состязательных сетях.",
    "useful_links": [
      {
        "text": "здесь",
        "url": "https://jmtomczak.github.io/blog/4/4_VAE.html#There-are-many,-many-more!"
      },
      {
        "text": "NVAE",
        "url": "https://arxiv.org/pdf/2007.03898.pdf"
      },
      {
        "text": "видеообзор",
        "url": "https://www.youtube.com/watch?v=x6T1zMSE4Ts"
      }
    ]
  }
]