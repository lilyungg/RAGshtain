[
  {
    "document_title": "Энтропия и семейство экспоненциальных распределений",
    "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
    "section_title": "Энтропия",
    "text": "Представим, что вы пронаблюдали значениенекоторой случайной величины. Как измерить количество информации, которое вы при этом получили? Следующие соображения кажутся в этом плане вполне естественными: чем выше вероятность, тем более ожидаемо появление значенияи, соответственно, менее информативно; и наоборот, наблюдение маловероятного значенияобычно даёт обильную пищу для размышлений и повышает; при наблюдении двух независимых реализацийислучайной величинылогично складывать полученную информацию:. Указанные соображения наводят на мысль, что информацию следует считать убывающей функцией от вероятности:. Кроме того, функциядолжна превращать произведение в сумму, поскольку для независимых случайных величиниравенствовлечёт На самом деле выбор тут небогат. Единственная непрерывная функция, обладающая такими свойствами, — это логарифм:. Основание логарифма может быть любым числом больше единицы. Поскольку информацию измеряют в битах и байтах, в теории информации обычно предпочитают двоичные логарифмы. Однако для вычислений удобнее использовать натуральный логарифм, и по умолчанию мы будем подразумевать именно его (кстати, соответствующую единицу информации называют «нат»). Среднее количество информации, которое несёт в себе значение дискретной случайнойс распределением вероятностей, вычисляется по формуле Это так называемаяэнтропия(Шэннона). Пример. Рассмотрим схему Бернулли с вероятностью «успеха». Энтропия её результата равна Давайте посмотрим на график этой функции: Минимальное значение (нулевое) энтропия принимает приили. Исход такого вырожденного эксперимента заранее известен, и чтобы сообщить кому-то о его результате, достаточнобит информации. Иначе говоря, можно вообще ничего не передавать, и так всё предельно ясно. Максимальное значение энтропии достигается в точке, что вполне соответствует тому, что припредсказать исход эксперимента сложнее всего. Упражнение. Найдите энтропию геометрического распределения с вероятностью «успеха»:, Следующие свойства энтропии дискретной случайной величинывытекают прямо из определения: неотрицательность:; при некотором(нулевую энтропию имеют вырожденные распределения и только они); , если случайная величина имеет конечный носитель мощности. Последнее свойство выводится изнеравенства Йенсена. Применяя его к выпуклой вверх логарифмической функции, с учётом нормировки условияполучаем Вопрос на подумать. Итак, всякое распределение с носителемимеет энтропию не больше. А у какого распределения она в точности равна? Чтобы вычислить энтропию непрерывной случайной величины, надо, как водится, сумму заменить на интеграл, и получится формуладифференциальной энтропии: Замечание.В дальнейшем мы будем использовать одинаковый терминэнтропиякак для дискретных, так и для непрерывных случайных величин, для краткости опуская словодифференциальнаяв последнем случае. Кроме того, энтропию распределения, заданного через pmf или pdf, будем обозначать. Такое обозначение позволяет избежать привязки к случайной величине там, где это излишне. Если, то обозначенияиэквивалентны. Также отметим, что энтропию можно записать в виде математического ожидания: Пример. Найдём энтропию нормального распределения. Его плотность равна, следовательно, Делая в последнем интеграле замену, получаем, что По свойству гамма-функции. Таким образом, Как видно, энтропия гауссовского распределенияне ограничена ни сверху, ни снизу: И да, в отличие от энтропии дискретного распределения дифференциальная энтропия может быть отрицательной. Это связано с тем, что плотность может принимать значения больше единицы, и поэтому математическое ожидание её логарифма с обратным знаком может оказаться меньше нуля. В частности, с нормальным распределением так происходит, если. Упражнение. Найдите энтропию показательного распределения. В задачах машинного обучения истинное распределение, из которого приходят наблюдения, обычно неизвестно, и его пытаются приблизить распределениемиз некоторого класса модельных распределений.Дивергенция Кульбака-Лейблера(KL-дивергенция,относительная энтропия) позволяет оценить расстояние между распределениямии: в дискретном случае и в непрерывном. KL-дивергенцию можно представить в виде разности: Здесь вычитаемое – это уже знакомая нам энтропия распределения, которая показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины. Уменьшаемое носит названиекросс-энтропиираспределенийи.Кросс-энтропию можно интерпретировать как среднее число бит для кодирования значения случайной величиныалгоритмом, оптимизированным для кодирования случайной величины. Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений, если при настройке алгоритма кодирования вместоиспользовать. Подробнее об этом вы можете почитать, например, вданном посте. Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности,, причём дивергенция равна нулю, только если распределения совпадают почти всюду (для дискретных и непрерывных распределений это означает, что они просто тождественны). Но при этом она не является симметричной: вообще говоря,. Упражнение. Пользуясь неравенством,, докажите неотрицательность KL-дивергенции. Пример.С помощью KL-дивергенции измерим расстояние между двумя гауссианамии. Подставляя явные выражения для плотностей находим Из свойств нормального распределения вытекает, что Таким образом, Как и должно быть, полученное выражение равно нулю, если гауссианы совпадают. При равных дисперсияхполучаем, что. Это выражение остаётся прежним, если поменять местамии, поэтому в этом случае. Если же, то выражение дляявно отличается от, что лишний раз показывает несимметричность KL-дивергенции. Упражнение. Найдите дивергенцию Кульбака-Лейблера двух показательных распределенийи. При определении KL-дивергенции мы уже встречались скросс-энтропией В зависимости от типа распределений кросс-энтропия вычисляется по формуле Поскольку задача минимизации KL-дивергенции между неизвестным распределением данныхи модельным распределениемэквивалентна задаче минимизации кросс-энтропии. Разница между ними равна энтропии распределения, которая, очевидно, не зависит от. В машинном обучении кросс-энтропию часто используют в качестве функции потерь в задаче классификации наклассов. Истинное распределение на каждом обучающем объекте задаётся с помощью one hot encoding и является вырожденным: Классификатор обычно выдаёт вероятности принадлежности каждому из классов, Функция потерь на одном объекте полагается равной кросс-энтропии между истинным и предсказанным распределениями: И это вполне логично: чем ближе модельное распределение к истинному, тем меньше наши потери. В идеале, если. Чтобы вычислить функцию потерь по обучающей выборке изобъектов с метками, обычно берут усреднённную кросс-энтропию",
    "useful_links": [
      {
        "text": "неравенства Йенсена",
        "url": "https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D1%80%D0%B0%D0%B2%D0%B5%D0%BD%D1%81%D1%82%D0%B2%D0%BE_%D0%99%D0%B5%D0%BD%D1%81%D0%B5%D0%BD%D0%B0"
      },
      {
        "text": "данном посте",
        "url": "https://habr.com/ru/post/484756/"
      }
    ]
  },
  {
    "document_title": "Энтропия и семейство экспоненциальных распределений",
    "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
    "section_title": "Принцип максимальной энтропии",
    "text": "Впараграфе про оценки параметровбыли описаны различные свойства параметрических оценок и методы их получения, например, метод моментов или метод максимального правдоподобия. В принципе, если мы уже выбрали для наших данныхнекоторое параметрическое семейство, моделирующее их распределение, восстановить его параметры чаще всего можно по выборочному среднемуи/или выборочной дисперсии. А теперь представим, что мы посчитали эти (или какие-то другие) статистики, а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию? Почему-то хочется сказать, что в первом. Почему? Второе не симметрично – но что нас может заставить подозревать, что интересующее нас распределение не симметрично? С третьим проблема в том, что, выбирая его, мы добавляем дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет. Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Таким образом, искомое распределение должно обладать максимальной неопределённостью при заданных ограничениях, или, говоря более научно, иметь максимально возможную энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше – тем более «‎произвольное» распределение, по крайней мере, в теории. В этом и заключаетсяпринцип максимальной энтропиидля выбора модели машинного обучения. Как мы уже видели выше, среди распределений с конечным носителем максимальную энтропию имеет равномерное распределение. Примеры геометрического и нормального распределения показывают, что энтропия распределений с бесконечным носителем (счётным или континуальным) может быть сколь угодно большой, и среди них нет какого-то одного распределения с максимальной энтропией. Однако в более узком классе распределений с фиксированным средним и/или дисперсией найти распределение с максимальной энтропией, как правило, можно. Пример. Покажем, что среди распределений на множестве натуральных чисели математическим ожиданиеммаксимальную энтропию имеет геометрическое распределение. Для минимизации энтропиис учётом ограничений воспользуемсяметодом множителей Лагранжа, согласно которому требуется минимизировать функцию Лагранжа Приравняем к нулю частные производные по: Отсюда следует, что, так что распределение действительно получается геометрическое. Параметрыинайдём из уравнений Деля первое уравнение на второе, получаем, или. Далее из первого уравнения находим. Итак, а это и есть геометрическое распределение с параметром. У непрерывных распределений возможны более интересные комбинации из ограничений на носитель и параметры. И конечно же, первую скрипку среди распределений с максимальной энтропией играет гауссовское распределение. Пример. Докажем, что среди распределений наc математическим ожиданиеми дисперсиейнаибольшую энтропию имеет нормальное распределение. Пусть– некоторое распределение со средними дисперсией,. Как было показано выше,. Запишем дивергенцию Кульбака-Лейблера: Так как KL-дивергенция всегда неотрицательна, получаем, чтопри любом распределении, удовлетворяющем заданным ограничениям. Можно показать, что максимальную энтропию среди многомерных распределений с вектором среднихи матрицей ковариацийимеет также гауссовское распределение. Упражнение. Докажите, что среди распределений на отрезкемаксимальную энтропию имеет равномерное распределение. Упражнение. Докажите, что среди распределений на промежуткес математическим ожиданиеммаксимальную энтропию имеет показательное распределение. Как выяснилось, многие классические распределения имеют максимальную энтропию при весьма естественных ограничениях. Но как быть, если даны не эти конкретные, а какие-то другие ограничения? Есть ли какой-нибудь надёжный алгоритм вывода распределения с максимальной энтропией, позволяющий избежать случайных озарений и гаданий на кофейной гуще? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса.",
    "useful_links": [
      {
        "text": "параграфе про оценки параметров",
        "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki"
      },
      {
        "text": "методом множителей Лагранжа",
        "url": "https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%BD%D0%BE%D0%B6%D0%B8%D1%82%D0%B5%D0%BB%D0%B5%D0%B9_%D0%9B%D0%B0%D0%B3%D1%80%D0%B0%D0%BD%D0%B6%D0%B0"
      }
    ]
  },
  {
    "document_title": "Энтропия и семейство экспоненциальных распределений",
    "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
    "section_title": "Экспоненциальное семейство распределений",
    "text": "Говорят, что параметрическое семейство распределений относится кэкспоненциальному классу, если его pdf (или pmf) может быть представлена в виде где – векторнатуральных параметровраспределения; — неотрицательная функция (base measure), часто равная единице; —нормализатор(partition), обеспечивающий суммируемость pmf или интегрируемость pdf в единицу: —log-partition; — вектордостаточных статистикраспределения. Пример. Покажем, что нормальное распределениепринадлежит экспоненциальному классу. Оно имеет два параметра, поэтому такую же размерность имеюти вектор-функция. Распишем плотность: Положим,, Остаётся выразить функциючерези. Упражнение. Выразите partitionи log-partitionчерези запишите плотность нормального распределения в экспоненциальном виде. Пример. Покажем, что распределение Бернуллипринадлежит экспоненциальному классу. Его pmfможно записать как Параметр здесь один, поэтому натуральный параметртоже один:. Такая функция отназывается функцией логитов и активно участвует в построении моделилогистической регрессии. Остальные функции положим равными,,. Остаётся выразить partition через: Итак,, и экспоненциальный вид распределения Бернулли записывается как Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках? Казалось бы, да, ведь В чём может быть подвох? К экспоненциальным семействам относятся многие непрерывные и дискретные распределения из часто встречающихся в теории и на практике, в том числе нормальное; распределение Пуассона; экспоненциальное; биномиальное; геометрическое; бета-распределение; гамма-распределение; распределение Дирихле. Как выглядят натуральные параметры, достаточные статистики и нормализаторы этих и других распределений из экспоненциального класса, можно посмотреть навикипедии. К экспоненциальным семействам не относятся, например, равномерное распределение,-распределение Стьюдента, распределение Коши, смесь нормальных распределений. Если распределениепринадлежит экспоненциальному классу, то моменты его достаточных статистикмогут быть получены дифференцированием функции. Утверждение.. Доказательство.По правилу дифференцирования сложной функции имеем Нормализаторзаписывается в виде интеграла который мы продифференцируем внесением градиента внутрь под знак интеграла: Таким образом, Если, то в соответствии с только что доказанным частная производнаядаёт-й момент распределения. Упражнение. Вычислите производные по натуральным параметрам от log-partition для распределения Бернуллии нормального распределенияи проверьте, что они совпадают со значениями соответствующих моментов. Кстати, можно продифференцировать ещё раз и доказать, что Возможно, вас удивил странный и на первый взгляд не очень естественный вид. Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе. Запишем функцию правдоподобия i.i.d. выборки: Её логарифм равен Дифференцируя по, получаем Приравниваяк нулю и пользуясь равенством, находим Таким образом, теоретические матожидания всех компонентдолжны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов дляв качестве моментов. И в следующем пункте выяснится, что распределения из экспоненциальных семейств обладают максимальной энтропией среди тех, что имеют заданные моменты. Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса. В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так. Теорема. Пусть параметрраспределениявыбран так, что для некоторого фиксированного. Тогда распределениеобладает наибольшей энтропией среди распределенийс тем же носителем, для которых. Выше мы уже находили обладающее максимальной энтропией распределение на множестве натуральных чисел с заданным математическим ожиданием. Таковым оказалось геометрическое распределение. Теорема Купмана—Питмана—Дармуа позволяет сделать это гораздо быстрее.В данном случае у нас лишь одна функция, которая соответствует фиксации математического ожидания. Искомое дискретное распределение имеет вид Это уже похоже на геометрическое распределение с параметром. Его математическое ожидание равно, что по условию должно равняться. Итак, наше распределение с максимальной этропией выглядит так: Пример. Среди распределений на всей вещественной прямой с заданным математическим ожиданиемнайдём распределение с максимальной энтропией.",
    "useful_links": [
      {
        "text": "логистической регрессии",
        "url": "https://education.yandex.ru/handbook/ml/article/linear-models#linejnaya-klassifikaciya"
      },
      {
        "text": "википедии",
        "url": "https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions"
      }
    ]
  }
]