[
  {
    "document_title": "Обобщающая способность – классическая теория",
    "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
    "section_title": "Оценка супремума",
    "text": "Попробуем оценить супремум разницы рисков. Будем считать, что выборкавыбирается случайным (и равновероятным) образом из распределения данных. Некоторые из выборок могут быть катастрофически плохими, поэтому мы будем рассматривать оценки, которые верны не обязательно всегда, а просто с достаточно большой вероятностью. Предположим сначала, что класс моделейконечен. Тогда Заметим, что. Поэтому при фиксированномразницу рисковможно оценить с помощью неравенства Хёффдинга. Неравенство Хёффдинга (Hoeffding's inequality). Пусть– независимые одинаково распределённые случайные величины со значениями в. Тогда для всехимеют место неравенства Как следствие неравенства Хёффдинга, получаем, что для любогои для любой. Заметим, что тогда для любой, Несмотря на то, что эта оценка является оценкой на обобщающую способность, она не имеет смысла, так как модельв ней задана априори и не зависит от. Другими словами, она верна для необученных моделей. Возвращаясь к нашей оценке, получаем: где– мощность класса. Следовательно, В случае бесконечногоиспользуем следующее обобщение неравенства Хёффдинга: Неравенство МакДайармида (McDiarmid's inequality). Пусть– независимые одинаково распределённые случайные величины,– скалярная функция саргументами, такая что для некоторых. Тогда для любогоимеет место неравенство Применяя теорему к, получаем: из чего следует: где– эмпирический риск на выборке. В следующем подразделе мы постараемся оценить жёлтое слагаемое. Оценим сверху матожидание супремума: Этот шаг называется «симметризация»: теперь выражение выше зависит от двух равнозначных обучающих выбороки. Ниже для краткости будем обозначатьи. Как оценить сверху супремум разности рисков? Наивная оценка, супремум суммы, слишком слаба: в самом деле, при фиксированном наборе данных вполне вероятно может существовать модель, имеющая большой риск на нём (достаточно взять модель, обученную на тех же данных, но с «неправильными» метками), поэтому матожидание супремума эмпирического риска может быть велико. Для обхода этой сложности заметим, что выражение выше симметрично относительно перестановки местами двух выборок: Более того, так как элементы обеих выборок выбираются независимо, значение выражения не меняется и при перестановке местами отдельно-ых элементов двух выборок. А именно, для любого набора Будем выбиратьнезависимо и равновероятно из. Такие случайные величины называютсяпеременными Радемахера. Поскольку оценки выше были верны для любых сигм, они верны и в среднем по переменным Радемахера, выбранным независимо от выборки: После введения переменных Радемахера оценка супремума разницы рисков через сумму супремумов становится не такой плохой. В самом деле, рассмотрим бинарную классификацию с помощью линейной модели. Если данные хорошо разделяются плоскостью, тобудет большим, так как в качествеможно взять линейную модель с противоположно ориентированной разделяющей плоскостью для. В то же время для того, чтобыбыло большим, необходимо, чтобы существовала модель, отвечающая правильно на тех примерах, где, и неправильно, где; для линейной модели это невозможно при большинстве конфигураций сигм. Величина называетсясложностью Радемахеракласса функций(для распределениянаи длины выборок). Она велика, если в классесодержатся функции, принимающие большие значения с заданными знаками на любом наборе данных фиксированного размера. Другими словами, сложность Радемахера измеряет, насколько выходы функций из классамогут коррелировать со случайным шумом. Для нас актуальна сложность Радемахера классов вида, то есть композиций моделей из классаи функции риска. Если– класс линейных моделей в пространстве размерности меньшей, чем, то сложность Радемахера невелика. В то же время если– множество всех возможных решающих деревьев, то, если только наборы данных непротиворечивы, она равна единице. В самом деле, решающее дерево способно запомнить всю обучающую выборку, то есть добиться единичной корреляции с любым случайным шумом. Вернёмся к оценке разницы рисков: Сложность Радемахера зависит от функции риска. Рассмотрим задачу бинарной классификации с классамии. Возьмём в качестве функции риска индикатор ошибки бинарной классификации, или «0/1-риск»: Название «0/1-риск» обусловлено тем, что риск принимает значенияи. Заметим следующее: где– класс эквивалентности функций из, в котором две функции считаются эквивалентными тогда и только тогда, когда их образы на выборкеимеют одинаковые знаки. Другими словами, среди всех функций, принимающих одни и те же знаки на, мы выберем по одной и сформируем из них множество. Заметим, что это множество конечно:. Нам понадобится следующая Лемма. Пусть– случайная величина со значениями ви нулевым средним. Тогда для любыхимеет место неравенство С её помощью получаем: Эта оценка верна для любого. Минимизируем её по. Легко видеть, что оптимальноеравняется; подставляя его, получаем: Определимфункцию ростаклассакак Эта функция показывает, сколько различных разметок класс функцийможет породить на наборе данных, в зависимости от размера этого набора. Очевидно, чтои монотонно не убывает. Например, для линейной модели на-мерном пространстве признаковпри(любое подмножествоточек в общем положении в-мерном пространстве всегда можно отделить гиперплоскостью), но строго меньше этого числа при(например, если точки – углы квадрата на плоскости, его диагонали нельзя разделить прямой). Когда, будем говорить, что «разделяет». Определимразмерность Вапника-Червоненкиса(илиVC-размерность) как максимальное, при котором семействоразделяет любой датасет: Таким образом, VC-размерность линейной модели равна. Следующая лемма даёт связь между размерностью Вапника-Червоненкиса и функцией роста: Лемма(Sauer–Shelah, см. подробнеездесь) Изучим асимптотическое поведение сложности Радемахера при. Обозначим. Дляимеем, а для: Подставляя это выражение в (1), получаем окончательную оценку на сложность Радемахера: Соответствующая оценка на истинный риск тогда примёт вид: Для того, чтобы эта оценка была осмыслена, необходимо гарантировать. Для линейных моделей, при условии(данных намного больше, чем признаков), оценки действительно получаются осмысленными. К сожалению, для нейронных сетей это подчас неверно. В работеNearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networksпоказано, что еслиобозначает класс моделей, реализуемых полносвязной сетью шириныспараметрами, то. Таким образом, наша оценка на сложность Радемахера становится бесполезной в реалистичных сценариях, когда число весов сетимного больше числа примеров в обучающей выборке. Если априори известно, что результат обучения лежит в некотором классе, то в оценке сложности Радемахера можно использовать именно этот класс, а не полный класс моделей. Очевидно, что сложность, лежащего в, не больше сложности. Так, в работеSpectrally-normalized margin bounds for neural networksполучены оценки для сложности полносвязной сети с липшицевыми функциями активации при условии, что нормы весов ограничены; см. такжеполный конспект лекций. В этом случае подбудем понимать класс сетей с весами нормы не больше. Обозначим соответствующую оценку через: К сожалению, нет гарантий, что градиентный спуск всегда сходится в решение с нормой меньше какого-то числа. Чтобы обойти это ограничение, используют следующую технику. Возьмём последовательность ограничений, такую что Также возьмём последовательность, монотонно убывающую к нулю и суммирующуюся в. Тогда для любого А значит, Из этого следует, что где– минимальное, при котором. Такая техника используется, например, в работахSpectrally-normalized margin bounds for neural networksиA PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks.",
    "useful_links": [
      {
        "text": "здесь",
        "url": "https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma"
      },
      {
        "text": "Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks",
        "url": "https://arxiv.org/pdf/1703.02930.pdf"
      },
      {
        "text": "Spectrally-normalized margin bounds for neural networks",
        "url": "https://arxiv.org/pdf/1706.08498.pdf"
      },
      {
        "text": "полный конспект лекций",
        "url": "https://arxiv.org/pdf/2012.05760.pdf"
      },
      {
        "text": "Spectrally-normalized margin bounds for neural networks",
        "url": "https://arxiv.org/pdf/1706.08498.pdf"
      },
      {
        "text": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
        "url": "https://arxiv.org/pdf/1707.09564.pdf"
      }
    ]
  },
  {
    "document_title": "Обобщающая способность – классическая теория",
    "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
    "section_title": "Фундаментальная проблема равномерных оценок",
    "text": "Напомним, что построение равномерных оценок проходило в несколько шагов: Оценка супремумом Применение неравенства макДайармида: Оценка матожидания супремума (жёлтое слагаемое выше) с помощью симметризации с дальнейшим выходом на сложность Радемахера: На каждом шаге предыдущая величина оценивается сверху, и потенциально каждое из этих неравенств может оказаться слишком слабым и привести к бессмысленной оценке. Давайте это проиллюстрируем. Выше мы уже отмечали, что если класссодержит модель, для котороймал, авелик, то равномерная оценка становится бессмысленной. По этой причине, имеет смысл выбирать класскак можно более маленьким. Самым лучшим из возможных классов мог бы быть класс моделей, к которым сходится наш алгоритм обучения с высокой вероятностью. Рассмотрим случай, близкий к идеальному: тот, в котором существует, для которого при любыхимеем. Иными словами, предположим, что все модели классахорошо обобщают. В этом случае оценка выше близка к идеальной: Но что будет, если мы начнём честно воспроизводить процесс построения равномерных оценок? После второго шага мы получаем оценку вида которая не сильно хуже предыдущей, но в которой всё равно появилось лишнее слагаемое. Но допустим, что мы хотим честно проделать третий шаг процедуры получения равномерных оценок. Для этого нам необходимо было оценить матожидание супремума, которое после симметризации получает вот такую верхнюю оценку: Таким образом, малость истинного риска не гарантирует малость эмпирического риска на любом наборе данных. Так, авторы статьиUniform convergence may be unable to explain generalization in deep learningпредъявили пример, в котором для любогосуществует модель, такая что, но при этомималы. Иллюстрация такой ситуации приведена в начале параграфа. Тогдавелик, и оценки теряют смысл. К счастью, даже эта фундаментальная проблема не ставит крест на равномерных оценках. Так, работыUniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign OverfittingиStability and Deviation Optimal Risk Bounds with Convergence Rateрассматривают равномерную оценку в классе интерполирующих моделей, то есть, имеющих нулевой эмпирический риск: Для таких моделей контрпример выше не работает.",
    "useful_links": [
      {
        "text": "Uniform convergence may be unable to explain generalization in deep learning",
        "url": "https://arxiv.org/pdf/1902.04742.pdf"
      },
      {
        "text": "Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting",
        "url": "https://arxiv.org/pdf/2106.09276.pdf"
      },
      {
        "text": "Stability and Deviation Optimal Risk Bounds with Convergence Rate",
        "url": "https://arxiv.org/pdf/2103.12024.pdf"
      }
    ]
  }
]