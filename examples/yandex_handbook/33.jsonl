{"document_title": "Bias-variance decomposition", "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition", "section_title": "Вывод разложения bias-variance для MSE", "text": "Рассмотрим задачу регрессии с квадратичной функцией потерь. Представим также для простоты, что целевая переменная— одномерная и выражается через переменнуюкак: где— некоторая детерминированная функция, а— случайный шум со следующими свойствами: В зависимости от природы данных, которые описывает эта зависимость, её представление в виде точнойи случайнойможет быть продиктовано тем, что: данные на самом деле имеют случайный характер; измерительный прибор не может зафиксировать целевую переменную абсолютно точно; имеющихся признаков недостаточно, чтобы исчерпывающим образом описать объект, пользователя или событие. Функция потерь на одном объектеравна Однако знание значения MSE только на одном объекте не может дать нам общего понимания того, насколько хорошо работает наш алгоритм. Какие факторы мы бы хотели учесть при оценке качества алгоритма? Например, то, что выход алгоритма на объектезависит не только от самого этого объекта, но и от выборки, на которой алгоритм обучался: Кроме того, значениена объектезависит не только от, но и от реализации шума в этой точке: Наконец, измерять качество мы бы хотели на тестовых объектах— тех, которые не встречались в обучающей выборке, а тестовых объектов у нас в большинстве случаев более одного. При включении всех вышеперечисленных источников случайности в рассмотрение логичной оценкой качества алгоритмакажется следующая величина: Внутреннее матожидание позволяет оценить качество работы алгоритма в одной тестовой точкев зависимости от всевозможных реализацийи, а внешнее матожидание усредняет это качество по всем тестовым точкам. Замечание.Записьв общем случае обозначает взятие матожидания по совместному распределениюи. Однако, посколькуинезависимы, она равносильна последовательному взятию матожиданий по каждой из переменных:, но последний вариант выглядит несколько более громоздко. Попробуем представить выражение дляв более удобном для анализа виде. Начнём с внутреннего матожидания: Из общего выражения длявыделилась шумовая компонента. Продолжим преобразования: Таким образом, итоговое выражение дляпримет вид где —смещениепредсказания алгоритма в точке, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости; —дисперсия (разброс)предсказаний алгоритма в зависимости от обучающей выборки; — неустранимыйшумв данных. Смещение показывает, насколько хорошо с помощью данного алгоритма можно приблизить истинную зависимость, а разброс характеризует чувствительность алгоритма к изменениям в обучающей выборке. Например, деревья маленькой глубины будут в большинстве случаев иметь высокое смещение и низкий разброс предсказаний, так как они не могут слишком хорошо запомнить обучающую выборку. А глубокие деревья, наоборот, могут безошибочно выучить обучающую выборку и потому будут иметь высокий разброс в зависимости от выборки, однако их предсказания в среднем будут точнее. На рисунке ниже приведены возможные случаи сочетания смещения и разброса для разных моделей: Синяя точка соответствует модели, обученной на некоторой обучающей выборке, а всего синих точек столько, сколько было обучающих выборок. Красный круг в центре области представляет ближайшую окрестность целевого значения. Большое смещение соответствует тому, что модели в среднем не попадают в цель, а при большом разбросе модели могут как делать точные предсказания, так и довольно сильно ошибаться. Полученное нами разложение ошибки на три компоненты верно только для квадратичной функции потерь. Для других функций потерь существуют более общие формы этого разложения (Domigos, 2000,James, 2003) с похожими по смыслу компонентами. Это позволяет предполагать, что для большинства основных функций потерь имеется некоторое представление в виде смещения, разброса и шума (хоть и, возможно, не в столь простой аддитивной форме). Попробуем вычислить разложение на смещение и разброс на каком-нибудь практическом примере. Наши обучающие и тестовые примеры будут состоять из зашумлённых значений целевой функции, гдеопределяется как В качестве шума добавляется нормальный шум с нулевым средним и дисперсией, равной во всех дальнейших примерах 9. Такое большое значение шума задано для того, чтобы задача была достаточно сложной для классификатора, который будет на этих данных учиться и тестироваться. Пример семпла из таких данных: Посмотрим на то, как предсказания деревьев зависят от обучающих подмножеств и максимальной глубины дерева. На рисунке ниже изображены предсказания деревьев разной глубины, обученных на трёх независимых подвыборках размера 20 (каждая колонка соответствует одному подмножеству): Глядя на эти рисунки, можно выдвинуть гипотезу о том, что с увеличением глубины дерева смещение алгоритма падает, а разброс в зависимости от выборки растёт. Проверим, так ли это, вычислив компоненты разложения для деревьев со значениями глубины от 1 до 15. Для обучения деревьев насемплируем 1000 случайных подмножествразмера 500, а для тестирования зафиксируем случайное тестовое подмножество точектакже размера 500. Чтобы вычислить матожидание по, нам нужно несколько экземпляров шумадля тестовых лейблов: Положим количество семплов случайного шума равным 300. Для фиксированныхиквадратичная ошибка вычисляется как Взяв среднее отпо,и, мы получим оценку для, а оценки для компонент ошибки мы можем вычислить по ранее выведенным формулам. На графике ниже изображены компоненты ошибки и она сама в зависимости от глубины дерева: По графику видно, что гипотеза о падении смещения и росте разброса при увеличении глубины подтверждается для рассматриваемого отрезка возможных значений глубины дерева. Правда, если нарисовать график до глубины 25, можно увидеть, что разброс становится равен дисперсии случайного шума. То есть деревья слишком большой глубины начинают идеально подстраиваться под зашумлённую обучающую выборку и теряют способность к обобщению: Код для подсчёта разложения на смещение и разброс, а также код отрисовки картинок можно найти в данномноутбуке.", "useful_links": [{"text": "Domigos, 2000", "url": "https://www.researchgate.net/publication/221345426_A_Unifeid_Bias-Variance_Decomposition_and_its_Applications"}, {"text": "James, 2003", "url": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=193A2D240404AB056822F188BAB09F94?doi=10.1.1.95.4138&rep=rep1&type=pdf"}, {"text": "ноутбуке", "url": "https://github.com/yandexdataschool/ML-Handbook-materials/blob/main/chapters/ensembles/bias_variance.ipynb"}]}
{"document_title": "Bias-variance decomposition", "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition", "section_title": "Bias-variance trade-off: в каких ситуациях он применим", "text": "В книжках и различных интернет-ресурсах часто можно увидеть следующую картинку: Она иллюстрирует утверждение, которое в литературе называетсяbias-variance trade-off: чем выше сложность обучаемой модели, тем меньше её смещение и тем больше разброс, и поэтому общая ошибка на тестовой выборке имеет вид-образной кривой. С падением смещения модель всё лучше запоминает обучающую выборку, поэтому слишком сложная модель будет иметь нулевую ошибку на тренировочных данных и большую ошибку на тесте. Этот график призван показать, что существует оптимальная сложность модели, при которой соблюдается баланс между переобучением и недообучением и ошибка при этом минимальна. Существует достаточное количество подтверждений bias-variance trade-off для непараметрических моделей. Например, его можно наблюдать для методаближайших соседей при ростеи для ядерной регрессии при увеличении ширины окна(Geman et al., 1992): Чем больше соседей учитывает-NN, тем менее изменчивым становится его предсказание, и аналогично для ядерной регрессии, из-за чего сложность этих моделей в некотором смысле убывает с ростоми. Поэтому традиционный график bias-variance trade-off здесь симметрично отражён по оси. Однако, как показывают последние исследования, непременное возрастание разброса при убывании смещения не является абсолютно истинным предположением. Например, для нейронных сетей с ростом их сложности может происходить снижение и разброса, и смещения. Одна из наиболее известных статей на эту тему — статьяБелкина и др. (Belkin et al., 2019), в которой, в частности, была предложена следующая иллюстрация: Слева — классический bias-variance trade-off: убывающая часть кривой соответствует недообученной модели, а возрастающая — переобученной. А на правой картинке — график, называемый в статьеdouble descent risk curve. На нём изображена эмпирически наблюдаемая авторами зависимость тестовой ошибки нейросетей от мощности множества входящих в них параметров (). Этот график разделён на две части пунктирной линией, которую авторы называют interpolation threshold. Эта линия соответствует точке, в которой в нейросети стало достаточно параметров, чтобы без особых усилий почти идеально запомнить всю обучающую выборку. Часть до достижения interpolation threshold соответствует «классическому» режиму обучения моделей: когда у модели недостаточно параметров, чтобы сохранить обобщающую способность при почти полном запоминании обучающей выборки. А часть после достижения interpolation threshold соответствует «современным» возможностям обучения моделей с огромным числом параметров. На этой части графика ошибка монотонно убывает с ростом количества параметров у нейросети. Авторы также наблюдают похожее поведение и для «древесных» моделей: Random Forest и бустинга над решающими деревьями. Для них эффект проявляется при одновременном росте глубины и числа входящих в ансамбль деревьев. В качестве вывода к этому разделу хочется сформулировать два основных тезиса: Bias-variance trade-off нельзя считать непреложной истиной, выполняющейся для всех моделей и обучающих данных. Разложение на смещение и разброс не влечёт немедленного выполнения bias-variance trade-off и остаётся верным и для случая, когда все компоненты ошибки (кроме неустранимого шума) убывают одновременно. Этот факт может оказаться незамеченным из-за того, что в учебных пособиях часто разговор о разложении дополняется иллюстрацией с-образной кривой, благодаря чему в сознании эти два факта могут слиться в один.", "useful_links": [{"text": "(Geman et al., 1992)", "url": "http://doursat.free.fr/docs/Geman_Bienenstock_Doursat_1992_bv_NeurComp.pdf"}, {"text": "Белкина и др. (Belkin et al., 2019)", "url": "https://arxiv.org/pdf/1812.11118.pdf"}]}
{"document_title": "Bias-variance decomposition", "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition", "section_title": "Список литературы", "text": "Блог-постпро bias-variance отЙоргоса Папахристудиса Блог-постпро bias-variance от Скотта Фортмана-Роу Статьи отДомингоса (2000)иДжеймса (2003)про обобщённые формы bias-variance decomposition Блог-постот Брейди Нила про необходимость пересмотра традиционного взгляда на bias-variance trade-off СтатьяГемана и др. (1992), в которой была впервые предложена концепция bias-variance trade-off СтатьяБелкина и др. (2019), в которой был предложен double-descent curve", "useful_links": [{"text": "Блог-пост", "url": "https://link.medium.com/X5Cpg1WITjb"}, {"text": "Блог-пост", "url": "http://scott.fortmann-roe.com/docs/BiasVariance.html"}, {"text": "Домингоса (2000)", "url": "https://www.researchgate.net/publication/221345426_A_Unifeid_Bias-Variance_Decomposition_and_its_Applications"}, {"text": "Джеймса (2003)", "url": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=193A2D240404AB056822F188BAB09F94?doi=10.1.1.95.4138&rep=rep1&type=pdf"}, {"text": "Блог-пост", "url": "https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update#double-descent"}, {"text": "Статья", "url": "http://doursat.free.fr/docs/Geman_Bienenstock_Doursat_1992_bv_NeurComp.pdf"}, {"text": "Статья", "url": "https://arxiv.org/pdf/1812.11118.pdf"}]}
