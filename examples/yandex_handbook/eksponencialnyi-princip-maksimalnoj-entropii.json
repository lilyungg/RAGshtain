[
  {
    "document_title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
    "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
    "section_title": "Мотивация: метод моментов",
    "text": "Метод моментов — это ещё один способ, наряду с методом максимального правдоподобия, оценки параметров распределения по данным. Суть его в том, что мы выражаем через параметры распределения теоретические значения моментовнашей случайной величины, затем считаем их выборочные оценки, приравниваем их все друг к другу и, решая полученную систему, находим оценки параметров. Можно доказать, что полученные оценки являются состоятельными, хотя могут быть смещены. Пример 1. Оценим параметры нормального распределенияс помощью метода моментов. Теоретические моменты равны Запишем систему: Из неё очевидным образом находим Легко видеть, что полученные оценки совпадают с оценками максимального правдоподобия Пример 2. Оценим параметрлогнормального распределения при известном. Будет ли оценка совпадать с оценкой, полученной с помощью метода максимального правдоподобия? Теоретическое математическое ожидание равно, откуда мы сразу находим оценку. Теперь запишем логарифм правдоподобия: Дифференцируя пои приравнивая производную к нулю, получаем что вовсе не совпадает с оценкой выше. Несколько приукрасив ситуацию, можно сделать вывод, что первые два выборочных момента позволяют если не править миром, то уверенно восстанавливать параметры распределений. А теперь давайте представим, что мы посчиталии, а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию? Почему-то хочется сказать, что в первом. Почему? Второе не симметрично — но почему мы так думаем? Если мы выберем третье, то добавим дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет. Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Но чтобы эти нестрогие рассуждения превратить в формулы, придётся немного обогатить наш математический аппарат и научиться измерять количество информации.",
    "useful_links": []
  },
  {
    "document_title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
    "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
    "section_title": "Энтропия и дивергенция Кульбака-Лейблера",
    "text": "Измерять «знание» можно с помощьюэнтропии Шэннона. Она определяется как для дискретного распределения и для непрерывного. В классическом определении логарифм двоичный, хотя, конечно, варианты с разным основанием отличаются лишь умножением на константу. Неформально можно представлять, что энтропия показывает, насколько сложно предсказать значение случайной величины. Чуть более строго — сколько в среднем бит нужно потратить, чтобы передать информацию о её значении. Пример 1. Рассмотрим схему Бернулли с вероятностью успеха. Энтропия её результата равна Давайте посмотрим на график этой функции: Минимальное значение (нулевое) энтропия принимает при. В самом деле, для такого эксперимента мы всегда можем наверняка сказать, каков будет его исход; обращаясь к другой интерпретации — чтобы сообщить кому-то о результате эксперимента, достаточнобит (ведь получатель сообщения и так понимает, что вышло). Максимальное значение принимается в точке, что вполне соответствует тому, что припредсказать исход эксперимента сложнее всего. Пример 2. Энтропия нормального распределенияравна, и чем меньше дисперсия, тем меньше энтропия, что и логично: ведь когда дисперсия мала, значения сосредоточены возле матожидания, и они становятся менее «разнообразными». Энтропия тесно связана с другим важным понятием из теории информации —дивергенцией Кульбака-Лейблера. Она определяется длякак в непрерывном случае и точно так же, но только с суммой вместо интеграла в дискретном. Дивергенцию можно представить в виде разности: Вычитаемое — это энтропия, которая, как мы уже поняли, показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины. Уменьшаемое похоже по виду, и можно показать, что оно говорит о том, сколько в среднем бит потребуется на кодирование случайной величины с плотностьюалгоритмом, оптимизированным для кодирования случайной величины. Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений, если при настройке алгоритма кодирования вместоиспользовать. Более подробно вы можете почитать, например, вэтом посте. Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности,, причём дивергенция равна нулю, только если распределения совпадают почти всюду. Но при этом она не является симметричной: вообще говоря,. Вопрос на подумать. Пусть— распределение, заданное на отрезке. Выразите энтропию через дивергенцию Кульбака-Лейблерас равномерным на отрезке распределением.",
    "useful_links": [
      {
        "text": "этом посте",
        "url": "https://habr.com/ru/post/484756/"
      }
    ]
  },
  {
    "document_title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
    "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
    "section_title": "Принцип максимальной энтропии",
    "text": "Теперь наконец мы готовы сформулировать, какие распределения мы хотим искать. Принцип максимальной энтропии. Среди всех распределений на заданном носителе, удовлетворяющих условиям, ...,, где— некоторые функции, мы хотим иметь дело с тем, которое имеет наибольшую энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше — тем более «произвольное распределение», по крайней мере, в теории. Давайте рассмотрим несколько примеров, которые помогут ещё лучше понять, почему некоторые распределения так популярны: Пример 1. На конечном множественаибольшую энтропию имеет равномерное распределение (носитель — конечное множество изэлементов, других ограничений нет). Доказательство: Пусть,— некоторое распределение,— равномерное. Запишем их дивергенцию Кульбака-Лейблера: Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что. При этом равенство возможно, только если распределения совпадают. Пример 2. Среди распределений, заданных на всей вещественной прямой и имеющих заданные матожиданиеи дисперсиюнаибольшую энтропию имеет нормальное распределение. Доказательство: Пусть— некоторое распределение,. Запишем их дивергенцию Кульбака-Лейблера: Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что. При этом равенство возможно, только если распределенияисовпадают почти всюду, а с точки зрения теории вероятностей такие распределения различать не имеет смысла. Пример 3. Среди распределений, заданных на множестве положительных вещественных чисел и имеющих заданное матожиданиенаибольшую энтропию имеет показательное распределение с параметром(его плотность равна). Все хорошо знакомые нам распределения, не правда ли? Проблема в том, что они свалились на нас чудесным образом. Возникает вопрос, можно ли их было не угадать, а вывести как-нибудь? И как быть, если даны не эти конкретные, а какие-то другие ограничения? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Давайте же познакомимся с ними поближе.",
    "useful_links": []
  },
  {
    "document_title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
    "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
    "section_title": "Экспоненциальное семейство распределений",
    "text": "Говорят, что семейство распределений относится кэкспоненциальному классу, если оно может быть представлено в следующем виде: где— вектор вещественнозначных параметров (различные значения которых дают те или иные распределения из семейства),,— некоторая вектор-функция, и, разумеется, сумма или интеграл поравняется единице. Последнее, в частности, означает, что (или сумма в дискретном случае). Пример 1. Покажем, что нормальное распределение принадлежит экспоненциальному классу. Для этого мы должны представить привычную нам функцию плотности в виде Распишем Определим Если теперь всё-таки честно выразитьчерез(это мы оставляем в качестве лёгкого упражнения), то получится В данном случае функцияпросто равна единице. Пример 2. Покажем, что распределение Бернулли принадлежит экспоненциальному классу. Для этого попробуем преобразовать функцию вероятности (нижепринимает значенияили): Теперь мы можем положить,, и всё получится. Единственное, что смущает, — это то, что компоненты векторалинейно зависимы. Хотя это не является формальной проблемой, но всё же хочется с этим что-то сделать. Исправить это можно, если переписать и определить уже минимальное представление с,— мы ведь уже сталкивались с этим выражением, когда изучали логистическу регрессию, не так ли? Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках? Казалось бы, да: так как: В чём может быть подвох? Как мы увидели, к экспоненциальным семействам относятся как непрерывные, так и дискретные распределения. Вообще, к ним относится большая часть распределений, которыми Вам на практике может захотеться описать. В том числе: нормальное; распределение Пуассона; экспоненциальное; биномиальное, мультиномиальное (с фиксированным числом испытаний); геометрическое; -распределение; бета-распределение; гамма-распределение; распределение Дирихле. К экспоненциальным семействам не относятся, к примеру: равномерное распределение на отрезке; -распределение Стьюдента; распределение Коши; смесь нормальных распределений.",
    "useful_links": []
  },
  {
    "document_title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
    "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
    "section_title": "MLE для семейства из экспоненциального класса",
    "text": "Возможно, вас удивил странный и на первый взгляд не очень естественный вид. Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе. Запишем функцию правдоподобия выборки: Её логарифм равен Дифференцируя по, получаем Тут нам потребуется следующая Лемма. Доказательство: Как мы уже отмечали в прошлом пункте: Следовательно, Кстати, можно ещё доказать, что Приравниваяк нулю и применяя лемму, мы получаем, что Таким образом, теоретические матожидания всех компонентдолжны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов дляв качестве моментов. И в следующем пункте выяснится, что распределения из семейств, относящихся к экспоненциальному классу, это те самые распределения, которые имеют максимальную энтропию из тех, что имеют заданные моменты. **Пример.**Рассмотрим вновь логнормальное распределение: Как видим, логнормальное распределение тоже из экспоненциального класса. Вас может это удивить: ведь выше мы обсуждали, что для него метод моментов и метод максимального правдоподобия дают разные оценки. Но никакого подвоха тут нет: мы просто брали не те моменты. В данном случае,, их матожидания и надо брать; тогда для параметров, получаемых из MLE, должно выполняться Матожидания в левых частых мы должны выразить через параметры — и нам для этого совершенно не обязательно что-то интегрировать! В самом деле:",
    "useful_links": []
  },
  {
    "document_title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
    "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
    "section_title": "Теорема Купмана-Питмана-Дармуа",
    "text": "Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса. В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так. Теорема. Пусть— распределение, причём— вектор длиныидля некоторых фиксированных,. Тогда распределениеобладает наибольшей энтропией среди распределений с тем же носителем, для которых,. При этом оно — единственное с таким свойством: в том смысле, что любое другое распределение, обладающее этим свойством, совпадает с ним почти всюду. Рассмотрим несколько примеров: Пример 1. Среди распределений на множественеотрицательных целых чисел с заданным математическим ожиданиемнайдём распределение с максимальной энтропией. В данном случае у нас лишь одна функция, которая соответствует фиксации матожидания. Плотность будет вычисляться только в точках,и будет иметь вид В этой формуле уже безошибочно угадывается геометрическое распределение с. Параметрможно подобрать из соображений того, что математическое ожидание равно. Матожидание геометрического распределения равно, так что. Окончательно, Пример 2. Среди распределений на всей вещественной прямой с заданным математическим ожиданиемнайдём распределение с максимальной энтропией.",
    "useful_links": []
  }
]