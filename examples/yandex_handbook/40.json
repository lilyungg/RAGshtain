[
  {
    "document_title": "Implicit bias",
    "url": "https://education.yandex.ru/handbook/ml/article/implicit-bias",
    "section_title": "Случай линейных сетей",
    "text": "Каков implicit bias нейронных сетей? Сходится ли градиентный спуск в решение наименьшей нормы и если да, то о какой норме идёт речь? Частичный ответ на этот вопрос удаётся получить для линейных сетей. Следуя работеExact solutions to the nonlinear dynamics of learning in deep linear neural networks, рассмотрим линейную сеть с одним скрытым слоем: гдеи– матрицы. Поставим задачу многомерной (метка– вектор) регрессии с квадратичной функцией потерь: Шаг градиентного спуска выглядит следующим образом: где точка надозначает производную по времени (то есть по). Это нелинейная система матричных дифференциальных уравнений второго порядка; чтобы проинтегрировать её аналитически, нам придётся сделать ряд предположений. Определим ковариационную матрицу входови матрицу ковариации меток со входами. Предположим, что данные декоррелированы:; этого можно добиться, заменив входына. Что касается матрицы ковариации меток со входами, рассмотрим её сингулярное разложение: Назовёмсилой моды с индексомковариации между метками и входами. Сделаем замену координат: В новых координатах градиентный спуск принимает вид: Пустьи. Тогда в терминах векторови Получилась система векторных дифференциальных уравнений порядка, всё ещё нелинейная. К счастью, при определённом предположении об инициализации эта система распадается нанезависимых систем порядка. Предположим, что существует ортогональная матрица, такая что при всехимеет место равенство и для некоторых скалярных величини. Нетрудно заметить, что в этом случае при всехи в любой момент времениимеемидля некоторых скалярных величини. Тогда для различныхвыражения выше становятся независимыми друг от друга: Теперь это система нелинейных дифференциальных уравнений второго порядка. Еслив начальный момент, то это верно и в любой момент времени. Тогда система выше превращается в одно уравнение первого порядка. В самом деле, обозначив, получаем: Это уравнение задаёт следующую динамику градиентного спуска для функции потерь(её глобальный минимум – это). Перед тем, как интегрировать уравнение (1), напомним, как изперейти обратно к исходными. Имеем для: Аналогично для: Теперь проинтегрируем уравнение (1) в предположении, чтоидля выбранного: Рассмотрим время, необходимое, чтобы выучить фиксированную долю силы данной моды, где, стартуя из точки из окрестности нуля. Оно равняется Видим, что чем сильнее мода (то есть чем больше), тем быстрее она сходится. Рассмотрим две моды с силамии, такие что. Насколько вторая (более слабая) мода выучится к моменту, когда первая уже выучится на долю? Из уравнения выше имеем: Подставляяи, получаем: Поскольку, это выражение стремится к минус бесконечности при, из чего следует, чтостремится к нулю. Это означает, что если веса в инициализации лежат в окрестности нуля, то к моменту, когда данная мода выучивается на любую фиксированную долю, более слабые моды не успевают выучиться вообще. Таким образом, в любой момент времениматрицаявляется наилучшим малоранговым приближением заданного ранга матрицы корреляций, причём чем больше, тем больше ранг. Можно сказать, чтоградиентный спуск с фиксированным числом шагов «предпочитает» решения малого ранга. В выводе выше, мы использовали ряд предположений, в частности, что вектора, образующие матрицу, ортогональны в инициализации. Эмпирически те же выводы оказываются верными и без этого предположения, см. графики в оригинальной работеExact solutions to the nonlinear dynamics of learning in deep linear neural networks. Можно ли их обосновать строго математически? В работахTowards resolving the implicit bias of gradient descent for matrix factorizationиDeep Linear Networks Dynamicsдоказывается, что самая сильная мода выучивается в первую очередь. Тем не менее, на момент написания этого текста остаётся недоказанным, что все моды выучиваются последовательно от сильных к слабым. К сожалению, implicit bias градиентного спуска для нелинейных сетей пока остаётся почти неизученным.",
    "useful_links": [
      {
        "text": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "url": "https://arxiv.org/pdf/1312.6120.pdf"
      },
      {
        "text": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "url": "https://arxiv.org/pdf/1312.6120.pdf"
      },
      {
        "text": "Towards resolving the implicit bias of gradient descent for matrix factorization",
        "url": "https://arxiv.org/pdf/2012.09839.pdf"
      },
      {
        "text": "Deep Linear Networks Dynamics",
        "url": "https://arxiv.org/pdf/2106.15933v1.pdf"
      }
    ]
  }
]