{"document_title": "Нормализующие потоки", "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki", "section_title": "Введение", "text": "В главеГенеративный подход к классификациимы уже познакомились с типом моделей, которые оценивают совместное распределение. Такие модели называютгенеративными. Для простоты предположим, что мы имеем всего один класс, тогда задача моделированиясводится к задаче моделирования. Научившись моделировать это распределение, мы сможем: генерировать объекты, где– параметры модели; оценивать вероятность встретить данный объектсреди набора наблюдаемых данных; выучивать скрытые представления для объекта. Известными примерами генеративных моделей являются: Авторегрессионные модели: Вариационные автокодировщики: Но оба эти метода не позволяют одновременно: получать скрытые представления для объектов точно вычислять функцию правдоподобия Нормализующие потокиспособны решить обе эти задачи.", "useful_links": [{"text": "Генеративный подход к классификации", "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii"}]}
{"document_title": "Нормализующие потоки", "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki", "section_title": "Мотивация", "text": "Пусть, гденеизвестно, а. Мы хотим найти отображение, для которогои. Отображениепреобразует базовую функцию плотностик более сложной. С его помощью мы можем генерировать сложный объект путем сэмплинга простого объекта(скрытой переменной) из распределенияи применения «генератора». Обратное отображение«нормализует» сложное распределение, приводя его к простому. Найдя такое отображение, мы сможем генерировать новые объекты, а оценить плотностьпоможет формула преобразования плотности случайной величины. Давайте её вспомним. Пусть,, при этом отображениедифференцируемо, обратимо и. Тогда: где– якобиан отображения.", "useful_links": []}
{"document_title": "Нормализующие потоки", "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki", "section_title": "Определение", "text": "Итак,нормализующий поток– это обратимое дифференцируемое отображение, которое переводит исходные представления объектов в скрытые:и. При этом функция правдоподобиявычисляется по формуле: Умея вычислять функцию правдоподобия, мы можем обучать нашу модельметодом максимума правдоподобия (ММП): где– выборка наблюдаемых данных из распределения. Обычно модель нормализующего потока составляет композицию изне очень сложных отображений, чтобы она была, с одной стороны, достаточно контролируемой, а с другой – достаточно выразительной: Тогда якобиан вычисляется по формуле: Но вычисление якобиана является очень затратной операцией. Для того, чтобы мы могли обучать модели эффективно на высокоразмерных данных (аудио, изображения), необходимо использовать такое отображение, подсчет якобиана которого был бы эффективен! Примером такого отображения являетсяпланарный поток(Planar Flow), где отображениепринадлежит следующему семейству функций: где– обучаемые параметры, а– гладкая нелинейная функция, например,. Якобиан такого отображения можно будет посчитать за. Обозначим Тогда", "useful_links": []}
{"document_title": "Нормализующие потоки", "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki", "section_title": "Развитие идеи", "text": "В планарных потоках нам удалось быстро посчитать якобиан, потому что матрица имела специальный вид (сумма единичной и низкоранговой). Но мы знаем и другие случаи, когда определитель можно посчитать быстрее – треугольные матрицы. Их определитель равен произведению элементов на диагонали. Следующие модели активно использовали этот трюк. Авторы моделиNICEпредложили использовать в качествеследующее семейство преобразований: где, а– произвольная нейросеть свходами ивыходами. Такое преобразование называютаддитивным связыванием(additive coupling). Обратное преобразование вычисляется с такой же легкостью, а якобиан равен. То есть,, что является довольно сильным ограничением модели. Далее, из-за того, что первыеканалов векторасовпадают с координатами нормального шума, то есть моделирования этих каналовне происходит. Из-за этого выразительная способность модели NICE была относительно невысокой. Позже авторы NICE позже предложили использовать между слоями нормализующих потоков зафиксированные перестановки признаков/каналов, что стало основой работыRealNVP. Использование перестановок позволяет добиться того того, чтобы все выходные каналы оказались затронуты преобразованием; при этом градиент перестановки вычисляется легко. где– поэлементное умножение, а– нейросеть, которая может быть произвольной, но, как правило, выбирается такой же архитектуры, как и. Такое преобразование называютаффинным связыванием(affine coupling). Получившееся отображение тоже легко обращается, а его якобиан равен: Заметим, что, как и в случае аддитивного связывания, значительная часть каналов остается неизменной при использовании аффинного связывания. Для того, чтобы преобразованиемоделировало распределениево всех каналах, на разных слоях неизменными оставляют разные подмножества изканалов. Чтобы улучшить сходимость глубоких () нормализующих потоков, авторы предложили использовать Batch Normalization. Данное преобразование тоже является обратимым, а его якобиан вычисляется крайне просто. В результате, выразительная способность модели сильно повысилась, и она стала способна выучивать сложные распределения: Ссылка на статью Данный вид нормализующих потоков также обладает нижнетреугольным якобианом, но он использует другое семейство функций: гдеи– нейросети произвольной архитектуры. Как видно из формулы,напрямую зависит от. Таким образом, элементы генерируютсяавторегрессивно, что и дало название архитектуре. Якобиан такого преобразования вычисляется по следующей формуле: Таким образом, шаг генерации выглядит следующим образом: ... Однако вычисление скрытых переменныхне является авторегрессивным: Несмотря на то, что данная разновидность нормализующих потоков кажется более мощной моделью, её трудно применить на практике к данным высокой размерности. Это происходит из-за того, что генерация нового объекта осуществляется авторегрессивно по координатам, что становится слишком затратным при обучении на высокоразмерных данных, например, на изображениях. Ссылка на статью Чтобы быстро генерировать объекты из сложного распределения, мы можем избавиться от авторегрессивности на шаге генерации, поставив в авторегрессивную зависимость не наблюдаемые, а латентные переменные: Можем заметить, что проблема долгого вычисления авторегрессивных выражений никуда не уходит. Мы лишь изменяем построение модели таким образом, чтобы генерировать объектыбыстрее: Но вычисление, а значит и правдоподобия, становится долгим, и обучение занимает больше времени. Нормализующие потоки стали наиболее актуальны в задаче генерации звука, поскольку они обладают достаточно высокой выразительностью и эффективностью, чтобы быстро генерировать аудиозаписи высокого качества. В этом контексте, модель нормализующего потока должна генерировать аудио, получая на вход описание того, что ей необходимо сгенерировать. То есть модель обуславливается на дополнительные признаки. Нормализующие потоки могут быть обусловлены на входные данные путем использования дополнительных входных данных в качестве переменной, от которой зависят преобразования, применяемые к данным. Обусловливающей переменной может быть любая дополнительная информация, имеющая отношение к задаче генерации, такая как текстовые описания, изображения или другие характеристики данных. В контексте генерации аудио обуславливающей переменной обычно служитmel-спектрограмма, которая позволяет отобразить интенсивность различных частот аудио-сигнала в разные моменты времени. Нормализующий поток учится генерировать сигнал в виде waveform-а на основе спектрограммы путем обратного преобразования. Чтобы генерировать более длинные фрагменты звука, модель генерирует короткие звуковые кадры (фреймы) за раз, которые затем объединяются для формирования полного waveform-а. Теперь мы готовы узнать про применение нормализующих потоков в генерации аудио! Ссылка на статью Архитектура Inverse Autoregressive Flow (IAF) была изначально предложена для задачи генерации аудио. Она позволяет генерировать объекты крайне эффективно, но обучение методом максимального правдоподобия занимает много времени из-за авторегрессивности вычислений. Метод Probability Density Estimation позволяет решить эту проблему с помощью использования второй предобученной авторегрессивной модели в качестве учителя. IAF обучается в качестве модели-студента, минимизируя KL дивергенциюгдеи– распределения студента и учителя соответственно. Ключевым достижением данного подхода является то, что вычисление функции потерь требует вычисления кросс-энтропии между учителем и студентом, а не правдоподобия, что позволяет максимально распараллелить все вычисления ввиду отсутствия авторегрессивности в вычислениях. Вместе с тем в данной работе в качестве учителя выбирается не случайная модель, а оригинальная авторегрессивная модельWaveNet, которая в 2016 году позволила достичь state-of-the-art качества генерации аудио. Эта модель является не нормализующим потоком, а обыкновенной авторегрессивной моделью, которая обучается предсказывать следующий кусочек аудио (фрейм) длиной в несколько миллисекунд. Таким образом, с помощью IAF и Probability Density Distillation авторам удалось ускорить генерацию более чем в 1000 раз без потери качества! На картинке выше мы видим, что модель использует лингвистические признаки для генерации аудио. Эта задача является примеромзадачи условной генерации, где на вход модели подаетсяспектрограмма, сгенерированная отдельной моделью по тексту, а на выход ожидается речь в аудио-формате (waveform). О том, как модель использует дополнительную информацию для обуславливания, поговорим в главе проWaveglow Исследователи из OpenAI в 2018 году опубликовали работуGlow: Generative Flow with Invertible 1×1 Convolutions, которая значительно улучшает результаты модели RealNVP. Опишем два главых улучшения. Во-первых, для перемешивания каналов Glow используетобратимые свертки с ядром 1x1вместо фиксированной матрицы перестановок каналов в RealNVP; Это нововведение является по-настоящему красивым, так как в нем предлагается способ вычисления якобиана 2D-свертки за. А именно, логарифм якобиана такой 1x1-свертки с числом каналовдля тензора размераравен, где– матрица свёртки 1х1. Авторы предлагают использовать следующий вариант LU-разложения для матрицы: где– фиксированная матрица перестановок,– нижнетреугольная матрица с единицами на диагонали,– верхнетреугольная матрица с нулями на диагонали, а– обучаемый вектор. Нетрудно показать, что Благодаря этому авторам удалось снизить сложность вычислений якобиана сдо Кроме того, для улучшения сходимости использовали собственно разработанныйactnorm-слой(activation normalization). Поскольку нормализующие потоки требуют много вычислительных ресурсов, для обучения используются мини-батчи маленького размера, из-за чего батч-нормализация работает не очень хорошо. Авторы предлагают использовать следующий тип нормализации – actnorm: Нормализуем входной тензор (промежуточное изображение) по размерности каналов; Инициализируем параметры смещенияи разбросастатистиками спервого батча; Далее обучаем их в качестве обычных параметров. Таким образом, один блок нормализующего потока выглядит так: Ссылка на статью Вторым важным с практической точки зрения применением нормализующих потоков стала модельWaveGlow. Она представляет собой версию модели Glow, адаптированную для генерации речи по тексту. Как мы помним, эта задача также является примеромзадачи условной генерации: На практике это приводит к тому, что все распределения в нашей формуле становятсяусловными. Таким образом, при генерации мы также сэмплируем из условного распределения, а в слоях нормализующих потоков используем преобразования. В качестве обуславливающего факторадля WaveGlow мы имеем сгенерированную по текстуmel-спектрограмму, а на выходе ожидаем получить соответствующую тексту и спектрограмме аудио-запись. Как мы видим на изображении и в формулах ниже, mel-спектрограмма используется как дополнительный признак для нейросети, генерирующей параметры афинного преобразования. В качестве модели, которая производит параметры афинного преобразования, используется похожая на WaveNet архитектура с dilated-свертками. Правая часть схемы ниже более подробно показывает строение слоя affine coupling: Операцияразделяет тензорпополам на два тензора меньшей размерностиидля их последующего участия в слоеаффинного связывания(affine coupling). Пример генерации:Источник", "useful_links": [{"text": "NICE", "url": "https://arxiv.org/abs/1410.8516"}, {"text": "RealNVP", "url": "https://arxiv.org/abs/1605.08803"}, {"text": "Ссылка на статью", "url": "https://arxiv.org/abs/1705.07057"}, {"text": "Ссылка на статью", "url": "https://arxiv.org/abs/1606.04934"}, {"text": "mel-спектрограмма", "url": "https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53"}, {"text": "Ссылка на статью", "url": "https://arxiv.org/abs/1711.10433"}, {"text": "WaveNet", "url": "https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio"}, {"text": "спектрограмма", "url": "https://en.wikipedia.org/wiki/Mel_scale"}, {"text": "Waveglow", "url": "#waveglow"}, {"text": "Glow: Generative Flow with Invertible 1×1 Convolutions", "url": "https://arxiv.org/pdf/1807.03039.pdf"}, {"text": "Ссылка на статью", "url": "https://arxiv.org/pdf/1811.00002.pdf"}, {"text": "mel-спектрограмму", "url": "https://en.wikipedia.org/wiki/Mel_scale"}, {"text": "Источник", "url": "https://nv-adlr.github.io/WaveGlow"}]}
{"document_title": "Нормализующие потоки", "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki", "section_title": "Out-of-distribution detection", "text": "Может показаться, что способность точно и эффективно вычислять функцию правдоподобия может позволить без труда обнаруживать аномалии в данных, что может пригодиться во многих приложениях. Однако в работеKirichenko et al.на примере задачи генерации изображений было показано, что нормализующие потоки выучивают отображение картинок в латентное пространство, основываясь на локальных корреляциях пикселей и графических деталях, а не на семантическом контенте. Из-за этого правдоподобие OOD-объектов может быть выше, чем правдоподобие in-distribution сэмплов. Однако позже было предложено использовать ряд эвристик для того, чтобы улучшить способность к детекции аномалий за счет подсчета значения функции правдоподобия: Использовать значение правдоподобия второй модели потока, обученного на отличном от исходного датасете (например, ImageNet при исходном CelebA). А затем вычислять отношенение этих двух значений для вынесения вердикта об аномальности объекта.Schirrmeister et al. В работеSerrà et al.показали, что проблема качества нормализующих потоков в задаче детекции аномалий связана с чрезмерным влиянием сложности входных данных на значение функции правдоподобия. Поэтому авторы предложили использовать в качестве поправки размер сжатого изображения с помощью одного из алгоритмов компрессии (JPEG2000/PNG).", "useful_links": [{"text": "Kirichenko et al.", "url": "https://arxiv.org/abs/2006.08545"}, {"text": "Schirrmeister et al.", "url": "https://arxiv.org/abs/2006.10848"}, {"text": "Serrà et al.", "url": "https://arxiv.org/abs/1909.11480"}]}
{"document_title": "Нормализующие потоки", "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki", "section_title": "Сравнение с другими типами генеративных моделей", "text": "Обратимся к статьеBond-Taylor et al., в которой приводится количественный анализ всех существующих семейств генеративных моделей в задаче генерации изображений из датасета CIFAR-10. В таблице выше указано, насколько представители каждого из популярных семейств генеративных моделей эффективны в следующих аспектах решения задачи: скорость обучения; скорость генерации; число обучаемых параметров; разрешение генерируемого изображения; ограничение на форму якобиана; возможность вычислять правдоподобие объекта; FID (Fréchet Inception Distance) тестовой выборки; Отрицательный логарифм правдоподобия тестовой выборки. За расшифровкой обратимся к таблице ниже: Подведя итог, можно сказать, что нормализующие потоки: требуют очень много времени на обучение, так как при обучении проводятся нетривиальные неоптимизированные вычисления; имеют скорость генерации, сравнимую с GAN-ами; менее эффективны по соотношению качество/число параметров, чем GAN-ы и диффузионные модели; позволяютбыстровычислять точное значение функции правдоподобия объекта; обладают сравнительно неплохим качеством генерации, проигрывающим GAN-ам и диффузионным моделям. Итак, нормализующие потоки явно выделяются среди других семейств генеративных моделей своими свойствами – обратимостью и способностью вычислять правдоподобие объекта. Но если для решения задачи они не требуются, то имеет смысл попробовать другие модели – в первую очередь, GAN-ы и диффузионные модели.", "useful_links": [{"text": "Bond-Taylor et al.", "url": "https://arxiv.org/abs/2103.04922"}, {"text": "Fréchet Inception Distance", "url": "https://arxiv.org/abs/1706.08500"}]}
