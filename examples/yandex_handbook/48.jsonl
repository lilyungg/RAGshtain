{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Напоминания", "text": "ОпределениеКритической точкой гладкой функцииназывается точка, для которой В выпуклой оптимизации такая точка обязательно будет точкой глобального минимума. В невыпуклой оптимизации все сильно сложнее: Бывает много локальных минимумов Бывают седловые точки Локальный минимум — это критическая точка, в которой Гессианположительно определён. Отметим, что часто в методах глобальной оптимизации рассматривается так называемая «локальная выпуклость», для которой требуется, чтобы функциябыла выпуклой внутри некоторого шара радиусас центром в точке. Критические точки, в которых гессиан не является знакоопределённым, называются седловыми. Пример: функцияимеет седловую точку. Гессиан в точке 0 Обратите внимание: во многих современных статьях про сходимость методов оптимизации первого порядка на невыпуклых функциях (пример) в качестве критерия сходимости рассматривают сходимость по норме градиента:при некотором заранее фиксированном. В выпуклой оптимизации этот критерий сходимости эквивалентен двум другим: сходимости по расстоянию до оптимума в пространстве параметров:; сходимости по расстоянию до оптимума по значениям функции. В невыпуклой оптимизации всё не так просто и поиск глобального минимума является в общем случае NP-трудной задачей. Критерийдаёт возможность исследовать сходимость к любой критической точке, но если речь об обучении нейронных сетях, то остается лишь надеяться, что эта критическая точка будет хорошим локальным минимумом.", "useful_links": [{"text": "пример", "url": "https://arxiv.org/pdf/2003.02395.pdf"}]}
{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Скользящее среднее в знаменателе AdaGrad. Методы RMSprop и Adam", "text": "В далекие 2012-2014е в мире было не так много опыта по построению хороших нейросетевых архитектур. «Канонические» методы оптимизации нейросетей RMSprop и Adam появлялись во времена, когда ещё не придумали основополагающих вещей вроде: Residual connectionиDense connection(статьи опубликованы в 2015/2016 соответственно, во всех экспериментах используется SGD, в статье и в ссылках не упоминаются методы Adam/RMSprop), плохо решались проблемы взрывов/затуханий градиентов и т.д. Batch NormalizationиLayer Normalization(2015/2016 соответственно) Также люди не умели правильно инициализировать нейросети гигантской глубины. статьи вроде1000+ layer fully connectedи10000+ layer CNNпозже. Кстати, этот цикл статей хочется особо отметить за интересную технику анализа распространения сигнала по нейронной сети. В общем, в те времена царило архитектурное средневековье со всеми родовыми проблемами нейронных сетей: Взрывы градиентов; Затухания градиентов; Взрывы-затухания сигнала на прямом проходе; Плохие начальные инициализации, нестабильный старт обучения. При попытках применять метод AdaGrad особо остро стояли проблемы 1 и 4. AdaGrad аккумулирует всю прошедшую историюбез затухания. Если в какой-то момент возникает одна из указанных проблем, знаменатель резко возрастает и больше не выправляется. Чтобы побороть проблемы 1-4, решили поработать над оптимизатором и сделать так, чтобы история в AdaGrad аккумулировалась с затуханием и метод оптимизации мог со временем забыть плохие точки. Самый популярный и простой в реализации метод — экспоненциальное скользящее среднее. Самая первая и самая простая модификация метода AdaGrad — метод RMSprop — вместо суммы использует экспоненциальное скользящее среднее в знаменателе: Методу RMSprop не было посвящено ни одной специализированной статьи, равно как и не было никаких доказательств его сходимости даже для выпуклых задач. Авторы Adam в статьеAdam: A Method For Stochastic Optimizationвводят два новшества по сравнению с RMSprop. Во-первых, это Momentum. Во вторых — Bias correction term. Напомним, как работает этот метод. Применяем bias correction Сразу перепишемив нерекурсивной форме с зависимостью только от: Авторы статьи пишут, что для правильной работы методаидолжны быть несмещенными оценкамиисоответственно. Допустим, все— независимые одинаково распредёленные случайные величины. Это довольно сильное предположение, но иначе не получатся красивые формулы. Рассмотрим на примере: Отсюда очевидно, что исходныеисмещены на множитель, поэтому авторы Adam делят на негои. Так какпри, эффект смещения сильнее всего заметен в начале итерационного процесса. Например, при классическоммы получаем смещение в 0.001 раз. В начале обучения bias correction призван уменьшить слишком большие шаги оптимизатора. В оригинальной статье приводится теорема с доказательством сублинейного Regret. Доказательство содержало ошибку, в новой работе 2018 года было доказано, что для любого набора гиперпараметров Adam существуетвыпуклаязадача, на которой он не сходится. Проблемы со сходимостью, впрочем, не являются специфичными для выпуклых задач: в нейронных сетях Adam тоже может вести себя странно, и об этом мы поговорим ниже в разделе «Как сломать адаптивные методы». Разбирать доказательство исходной статьи мы не будем, зато обратим внимание на пару неприятных фактов о различиях между «продаваемой» частью статьи и бекендом с экспериментами и доказательствами теорем. После успешного введения метода Adam в эксплуатацию в нейросети его окрестили «method of choice» в задачах стохастической оптимизации. Это было на 100% обусловлено его успехом в обучении нейронных сетей с нестабильными архитектурами. Структура статьи выглядит следующим образом: Выделенный в большую красивую видную рамочку алгоритм с дефолтными настройками вроде; Формулировка теоремы в разделе про доказательства; Эксперименты на нейросетях и выпуклых задачах. В пункте 1 описан алгоритм, который все нынче знают, как Adam. Мало кто знает, что в доказательствах сходимости и в экспериментах на выпуклых задачах использовался немного другой алгоритм: вместо константногоавторы статьи взяли. Сравним эти learning rate с AdaGrad: Авторы в экспериментах на логистической регрессииубили основное свойство Adam — неубывающие learning rate. Вспомним, как в разделе провывод AdaGradмы анализировали порядок убывания learning rate — он был. Отсюда следует, что у такого Adam learning rate убывают так же, как в AdaGrad. Словом, будьте внимательны при чтении статей: смотрите не только в описание алгоритмов, но и в их реализацию. Настоящий Adam, который в pytorch и tensorflow реализован без множителя, в выпуклой задаче разреженной логистической регрессии обычно работает намного хуже AdaGrad. Это справедливо как для чисто линейных моделей, так и для комбинированныхWide &Deepархитектур, из-за чего в одной и той же нейросети приходится использовать разные методы оптимизации для разных параметров. Тут нужно запомнить три идеи: Momentum Скользящее среднее в learning rate Bias correction На практике, часто почему-то рассматривают методы RMSprop и Adam как нечто отлитое в граните и не пытаются брать от них лучшее. Например, методу RMSprop обычно идет на пользу добавление bias correction от adam. Так что полезно помнить идеи, стоящие за методами оптимизации, и уметь их комбинировать.", "useful_links": [{"text": "Residual connection", "url": "https://arxiv.org/pdf/1512.03385.pdf"}, {"text": "Dense connection", "url": "https://arxiv.org/pdf/1608.06993.pdf"}, {"text": "Batch Normalization", "url": "https://arxiv.org/abs/1502.03167"}, {"text": "Layer Normalization", "url": "https://arxiv.org/abs/1607.06450"}, {"text": "1000+ layer fully connected", "url": "https://arxiv.org/abs/1711.04735"}, {"text": "10000+ layer CNN", "url": "https://arxiv.org/pdf/1806.05393.pdf"}, {"text": "Adam: A Method For Stochastic Optimization", "url": "https://arxiv.org/pdf/1412.6980.pdf"}, {"text": "вывод AdaGrad", "url": "https://academy.yandex.ru/handbook/ml/article/adaptivnyj-ftrl#ada-grad-nailuchshij-adaptivnyj-metod"}, {"text": "Wide &Deep", "url": "https://arxiv.org/abs/1606.07792"}]}
{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Как сломать адаптивные методы со скользящим средним", "text": "Все диагональные адаптивные методы так или иначе используют покоординатный learning rate. Методы отличаются лишь формулировкойи: Все эти методы имеют единый вид формул FTRL, аналогичный формулам FTRL-AdaGrad: Вспомним теоретические ограничения на: — выпуклый; . Адаптивные методы с регуляризаторамибудут удовлетворять этим условиям, если все. В этом месте и локаются методы со скользящим средним: никто не обещал, что последовательностьбудет монотонно неубывать. Если же то метод может ломаться Обратите внимание. Momentum в методе Adam никак не повлияет на справедливость наших рассуждений, поскольку в формулах для адаптивных learning rate он не используется. Адаптивные методы с такими learning rate сломаются и с momentum, и без него. Обратите внимание. Bias correction в методе Adam уменьшает learning rate в начале обучения, заставляя метод делать меньшие шаги. Все рекуррентные формулы из таблицы можно переписать в виде Тогда неравенствоможно записать в виде Здесь мы можем подвести общую черту и сказать, что методы Adam и RMSprop дают, когдастановится меньше предыдущей накопленной истории с точностью до некоторой константы. А когда такое бывает? Уменьшение, как правило, означает приближение к критическим точкам. Добавление квадратичных регуляризаторов с отрицательным коэффициентом приводит к тому, что метод оптимизацииштрафует за близость к критическим точкам, заставляя убегать от них. Это приводит к тому, что метод не может нормально сойтись к локальным минимумам (в выпуклых задачах — просто к минимумам, что намного более критично). Отметим, что по разным координатаммогут вести себя по-разному. Таким образом, можно получить ситуацию, когда мы поощряем близость по одним координатам и штрафуем за близость по другим. AdaGrad невозможно сломать таким способом: для него гарантируется, что. Подставим в условие, сразу сократив константный: Чисто технически, при выведении формул можно подумать, что Adam страдает от указанных эффектов гораздо сильнее RMSprop, но на самом деле это не так. Переобозначимиз статьи про Adam как простодля общности обозначений. Распишем неравенстводля метода Adam: В отличие от RMSprop, у нас появился дополнительный множитель. С одной стороны, можно подумать, что метод строго хуже. Однако, этот множитель сильно больше нуля только во время первых шагов оптимизации, тогда как рассматриваемая нами проблема играет роль только на поздних стадиях оптимизации при приближении к критическим точкам. А к тому моменту, этот множитель будет практически равен единице и мы получим формулы выше от RMSprop. Поэтому, на самом деле, методы в одинаковой степени страдают от этих эффектов, но bias correction добавляет стабильности в начале. Если представить, что нейросеть — очень плохая и жутко невыпуклая задача, то можно рассматривать подобное поведение как «защиту» от промежуточных плохих критических точек, позволяющую нам «убегать» от них. Данная интерпретация, к сожалению, имеет множество недостатков: Никто не обещал, что новая критическая точка будет лучше старой и что мы, прыгая таким образом, будем улучшать качество модели. Не каждый локальный минимум плохой. Если текущая критическая точка — хороший локальный минимум с хорошей обобщающей способностью, то мы просто нормально не сойдемся к нему и не достигнем хорошего качества модели. Общественность уже идентифицировала такое поведение как проблему и решила ее в более поздних популярных оптимизаторах (см.раздел проAMSgrad). Большинство современных рекомендаций по обучению больших неонлайновых моделей вроде GPT или картиночных моделей содержат в себе learning rate scheduler'ы как обязательный для успеха ингредиент. Эти рекомендации нивелируют проблему отрицательных регуляризаторов. Все learning rate scheduler'ы заставляют learning rate убывать, что позволяет достигать лучших результатов, чем с помощью обычных Adam и RMSprop. В параграфе про FTL мы узнали, что градиентный метод без регуляризации отвратительно работает даже на выпуклых задачах, а если мы начнём вводить отрицательную регуляризацию, да еще и на сложных невыпуклых задачах, то все может стать еще хуже. В целом, мировой опыт говорит, что полагаться на подобные интерпретации при тюнинге модели не стоит. Итак, методы RMSprop и Adam плохо работают для выпуклых задач, особенно для разреженных задач, и могут приводить к субоптимальным решениям на train. Тем не менее, есть искушение заявить, что «это такая регуляризация в классическом смысле: не слишком хорошо сходимся к оптимальной точке, не слишком сильно переобучаемся под датасет и можем лучше работать на тесте». Это искушение особенно опасно потому, что подобные эффекты действительно могут иметь место, особенно в классической (не онлайновой) постановке задачи. Любая регуляризация направлена на то, чтобы сдвинуть оптимум решения исходной некорректно поставленной задачи в надежде, что точка оптимума измененной задачи будет обладать лучшей обобщающей способностью на тесте. В частности, такой эффект может иметь ранняя остановка методов оптимизации до их сходимости к точке оптимума. Однако здесь есть одно очень важное «но». Если введение регуляризации в некорректно поставленную задачу — это полностью осмысленный и контролируемый гиперпараметрами процесс, то хаотично разваливающийся вокруг точки оптимума метод оптимизации — нет. Подумайте: вдруг ваша задача фактически не является некорректно поставленной? Вдруг у вас огромный и очень репрезентативный датасет, благодаря чему оптимум на train всегда отлично работает в проде? В этом случае кривой метод оптимизации способен подпортить качество вашей модели. В задачах с разреженными параметрами ситуациюполучить еще легче. Допустим, у нас есть некоторый параметр, который встречается в 0.1% объектов выборки. В такой ситуации между появлениями этого объекта в выборке и очередным расчетом градиентов для него проходит значительное время. За это значительное время модель дообучалась, и за счет других, менее разреженных параметров могла научиться лучше прогнозировать очередной объект с этим параметром. Тогдауменьшается и, следовательно, больше шансов попасть в плохую ситуацию. Ниже мы рассмотрим метод AMSgrad и наперёд скажем, что для оптимизации разреженных параметров Adam/RMSprop добавление AMSgrad очень часто дает прибавку в качестве. На первый взгляд, парадоксальным кажется следующий факт: чем меньше learning rate, тем в бОльшую сторону может отклониться отрицательный регуляризатор: Однако в «жадных» формулах все с точностью до наоборот: Из жадных формул очевидно, что уменьшениеведет к уменьшению шага и, как следствие, увеличению стабильности алгоритма. Чтобы разрешить парадокс, надо вспомнить, что в FTRL решающее значение имеет не один отдельный регуляризатор, асумма. В начале процесса оптимизации, первый регуляризатор точно не сломается. Чем меньше learning rate, тем меньшие шаги мы делаем от начальной точки и, следовательно,тем меньше должна отличаться норма градиентов. Если от шага к шагу норма градиента меняется не слишком сильно, то мы накопим огромную кумулятивную регуляризациюк моменту, когда регуляризатор решит отклониться в отрицательную сторону. При бОльшем learning rate мы шагаем быстрее, и точки, когда ломается регуляризатор, достигаем тоже быстрее, накопив гораздо меньшую сумму. Если теперь для очередной точки мы получили отрицательный регуляризатор, то насколько сильно он может всё поломать? Окей, допустим, мы шагнули к критической точке. А насколько сильно может расколбаситьоднаплохая точка в регуляризаторе? Так, чтобы он перекрыл всю предыдущую сумму? Если градиенты ограничены по норме, то катастрофы, очевидно, не будет. Ограниченность градиентов по норме мы, с одной стороны, гарантировать не можем, с другой — проблемам взрыва/затухания градиентов в архитектурах уделяется столько внимания, что на практике это условие зачастую выполняется.", "useful_links": [{"text": "AMSgrad", "url": "https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#am-sgrad"}]}
{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Чиним RMSprop и Adam", "text": "Время шло, люди учились строить хорошо обучаемые архитектуры. Стали даже появляться революционные идеи вродеReZero(не путать с аниме) с полным отказом от batchnorm/layernorm нормализаций в глубоких сетях и с улучшением качества работы и скорости сходимости. Ситуация со стабильностью обучения нейросетей кардинально изменилась. Несмотря на улучшение стабильности обучения, люди стали замечать, что при длительном процессе оптимизации Adam начинает сбоить. Авторы метода AMSgrad в статьеOn the Convergence of Adam and Beyondбыли одними из первых, кто провел почти аналогичный нашему анализ и добавили в Adam костыль, который обеспечивает выполнение условияи исключает отрицательные регуляризаторы. Обратите внимание: в разделее проLearning Rate Scheduling vs AdaGradмы поговорим о «цикличности истории» развития методов оптимизации в deep learning. Авторы статьиOn the Convergence of Adam and Beyondанализируют последовательность и говорят, что отрицательные значения в ней вызывают проблемы с процессом оптимизации. Их анализ в целом аналогичен приведённому выше, поэтому мы не будем его здесь дублировать. Авторы статьи не стали предлагать новых схем learning rate и просто модифицировали старую: выполнениеобеспечивается «в лоб» при помощи. Итоговое правило апдейта без momentum и без bias correction (оригинальный Algorithm 2 из статьи bias correction не использует): Если нужен метод с momentum, то можно просто заменитьв последней формуле на Оригинальные формулы из статьипредполагают, что для расчетамы держим два параметра:и. RMSprop и Adam хранят только один параметр. Таким образом, включение метода требует дополнительных расходов памяти (х1.5 относительно RMSprop и x1.33 относительно Adam). Выше при разборе методов RMSprop/Adam мы сказали, что на практике AMSgrad помогает разреженным параметрам. Для разреженных моделей потребление памяти — краеугольный камень, поэтому простое включение дефолтной реализации amsgrad из статьи может быть болезненным и, к сожалению, не оправданным. На практике же эвристика видадля разреженных параметров обычно работает так же хорошо и не требует дополнительной памяти. Никаких теоретических гарантий для нее нет, но на практике она работает. Оригинальная статья (и следующие букве оригинала стандартные реализации алгоритма, например, в PyTorch) предполагает убирание bias correction. Эксперименты на разреженных данных показывают, что убирание bias correction вредит сходимости, это полезная вещь. С практической точки зрения, есть два способа реализовать bias correction в AMSgrad: Post-correction:, Pre-correction:. С точки зрения корректности метода AMSgrad, правильный вариант — pre-correction, так как он не ломает максимум. А вот эксперименты показывают, что добавление pre-correction ничего не даёт, а вот post-correction действительно помогает в том смысле, что AMSgrad + post-bias correction лучше, чем просто RMSProp/Adam с bias correction. Итоговые формулы можно использовать такие: Другим способом улучшения сходимости методов RMSprop/Adam/SGD является learning rate scheduling (расписание learning rate, шедулер). Learning rate scheduler — это мета-алгоритм: они берёт любой стандартный метод оптимизации с константным параметром learning rateи предписывает схему измененияна каждом шаге, или на каждой эпохе, или на любом другом заданном периоде. Поскольку мы работаем с одним параметром, мы можем с ним делать всего две вещи: увеличивать или уменьшать. Эти два варианта имеют свои названия: Learning rate decay — уменьшение learning rate с течением времени с целью нивелировать осцилляцию RMSprop/Adam около критических точек. (Warm)Restart — обычно резкое увеличение learning rate. Warm — потому что мы уже сошлись в какую-то хорошую точку и сбрасываем только состояние оптимизатора в ней, но не переинициализируем сами параметры. WarmRestart может заключаться не только в увеличении, но и, например, в дополнительном сбросе состояния оптимизатора (обнуление momentum или), хотя автор статьи такой подход встречали достаточно редко Существует огромное количество вариантов расписания, каждый со своим графиком измененияи со своим любовно подобранным множеством задач, на которых данный метод показывает себя лучше других. Приводить здесь их список особого смысла нет, лучше просто откройтедокументацию любого фреймворкаи наслаждайтесь разнообразием вариантов. Мы же обсудим влияние learning rate decay на осцилляцию вокруг критических точек и дадим практические рекомендации по подбору расписаний. Для выпуклых задач в разделе про схемы убывания learning rate для FTRL-методов (константный регуляризатор,и AdaGrad) мы буквально на оценках на regret видели, что это важный аспект для асимптотики сходимости. В выпуклом случае, при приближении к минимуму мы должны оптимизировать решение с куда большей точностью. Норма градиентов при приближении к минимуму тоже уменьшаются, поэтому даже с константнымlearning rate шаги будут становиться меньше, но — как показывают и теоретические оценки на regret, и многочисленные их валидации в статьях — этого недостаточно. Уменьшение learning rate с правильной асимптотикой уменьшения дает куда более хорошие результаты. Для глубинного обучения и оптимизации к каким-то локальным минимумам эта логика тоже применима. Возвращаясь к методам Adam/RMSprop — напомним, что у них асимптотика learning rate. Им в любом случае пойдет на пользу уменьшение learning rate, даже если не брать во внимание их проблемы вокруг критических точек и взять метод AMSgrad, который от этих проблем не страдает. Отсюда же очевидно, что проблемы adam/rmsprop начинают стрелять гораздо меньше. Learning rate уменьшается =>от критической точки мы в плохих ситуациях шагаем на гораздо меньшее расстояние =>область, вокруг которой мы будем «прыгать», сужается =>мы худо-бедно, но сходимся. Как мы уже отмечали выше, шедулеров существует поистине фантастическое количество, гораздо больше, чем базовых оптимизаторов, к которым они применяются. Без структуризации подхода к ним работать становится сложно. Мы хотели бы дать вам следующие рекомендации: Выучите свою модель без learning rate scheduling со стандартными методами оптимизации и посмотрите, как ведёт себя loss для различных learning rate. Обязательно переберите learning rate на этом шаге. Начинать внедрение расписаний рекомендуем с шедулеров, которые только уменьшают learning rate. Классические варианты — ReduceOnPlateou или linear decay. Правильный подбор learning rate и темпа его уменьшения очень важны в любой задаче стохастической оптимизации. Только после того, как вы хорошенько потюните learning rate decay, можно смотреть в сторону WarmRestart. Иногда рестарты могут помочь. Автор статьи занимается в основном рекомендательными моделями и там эту технику практически никто не применяет. У методов SGD/RMSprop/Adam последовательностьне является асимптотически убывающей, и для того, чтобы это скомпенсировать, используется расписание learning rate. А вот у AdaGrad си так всё в порядке. Давайте восстановим хронологию событий: Метод AdaGrad пытаются применять к нейросетям в 2012+ годах, но тогда архитектуры были нестабильны, градиенты взрывались и навсегда портили знаменатель AdaGrad, сильно уменьшая learning rate. Появляются методы RMSprop/Adam(2013/2014) со скользящим средним в знаменателе, которые могут оправиться от взрыва градиента. Развитие архитектур нейронных сетей не стоит на месте, появляются разные видыresidual connection(2015),LayerNorm/BatchNorm(2015-2016), крутые методыначальной инициализации— огромное количество способов улучшения стабильности обучения. С развитием архитектур люди замечают, что RMSProp/Adam умеют застревать на одном уровне значений функции потерь, и начинают применять техники для уменьшения learning rate. В дальнейших работах метод AdaGrad часто рассматривается наравне с Adam/RSMprop и дает очень похожее, либо даже лучшее качество (см, например, статью проShampoo). А дело в том, что архитектуры уже очень хорошо инициализируются и правильно проектируются так, чтобы не было взрывов/затуханий градиентов ни на какой стадии оптимизации. Развитие методов оптимизации в deep learning сделало небольшой круг, и мы рекомендуем об этом помнить. Порой люди могут одновременно рассуждать о бесценной пользе learning rate decay (особенно с линейным убыванием как) и корить AdaGrad за бесконечное аккумулирование квадратов градиентов (которые убывают как). Так что если у вас вдруг хорошо заработал шедулер с— возможно, обычный AdaGrad будет лучше? В последнее время в литературе часто появляются заявления, что решения, полученные адаптивными методами в нейросетях, обладают худшей обобщающей способностью. Сразу хотим отметить, что большинство этих статей исследуют эти эффекты только на задачах Computer Vision на одних и тех же датасетах MNIST/CIFAR/ImageNet. В реальной жизни куда большее разнообразие постановок задач и датасетов, что сразу заставляет сомневаться в воспроизводимости этих эффектов. Рекомендация тут одна, как и всегда — досконально сами все проверяйте. Данные методы предложены авторами в статьеDecoupled Weight Decay Regularization, которую мы подробно разобрали в разделее про продвинутуюрегуляризацию. Методы AdamW и SGDW — это просто модификации методов Adam и SGD с momentum, которые используют линеаризованный decoupled. Авторы статьи изучали проблему, почему в их экспериментах SGD обобщает лучше Adam (но учится дольше и требует более аккуратной настройки). Они пришли к выводу, что дело не в магии SGD, а в том, что-регуляризация у этих двух методов работает по-разному. Добавив decoupling, авторы сумели показать, что decoupled Adam обгоняет SGD. Эти эффекты, повторимся, были уже рассмотрены ранее в разделее про продвинутуюрегуляризацию. Единственное, что мы не обсудили тогда — это momentum. В постановке Proximal Gradient Descent градиент заменяется на momentum Покоординатныемогут рассчитываться любыми методами: AdaGrad, RMSprop или Adam, не принципиально. На всякий случай напомним, что мы вывели потенциально более правильные формулы Метод SGDW получается из формул выше, если убрать покоординатность К сожалению, здесь мы не почерпнули новых идей, так как выяснили, что это просто очередная инкарнация Proximal методов оптимизации. Этот метод заключается в том, чтобы стартовать с адаптивного метода Adam и в некоторый момент переключиться на SGD. «Некоторый момент» — это, интуитивно, момент стабилизации всех статистик в Adam, когда мы выжали все из ускоренного старта адаптивных методов и хотим получше сойтись к хорошему оптимуму в найденной им окрестности. Отметим, что позднее переключение на SGD с неубывающими learning rate автоматически починит проблемы расходимости Adam ровно там, где они чаще всего и возникают: при хорошем приближении к локальным минимумам. Мы не будем здесь подробно рассматривать их анализ, вы можете сами познакомиться с ним в статьеOn The Variance Of The Adaptive Learning Rate And Beyond", "useful_links": [{"text": "ReZero", "url": "https://arxiv.org/pdf/2003.04887.pdf"}, {"text": "On the Convergence of Adam and Beyond", "url": "https://openreview.net/pdf?id=ryQu7f-RZ"}, {"text": "Learning Rate Scheduling vs AdaGrad", "url": "https://academy.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning#learning-rate-scheduling-vs-ada-grad"}, {"text": "On the Convergence of Adam and Beyond", "url": "https://openreview.net/pdf?id=ryQu7f-RZ"}, {"text": "документацию любого фреймворка", "url": "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"}, {"text": "Adam", "url": "https://arxiv.org/abs/1412.6980"}, {"text": "residual connection", "url": "https://arxiv.org/abs/1512.03385"}, {"text": "LayerNorm", "url": "https://arxiv.org/abs/1607.06450"}, {"text": "BatchNorm", "url": "https://arxiv.org/abs/1502.03167"}, {"text": "начальной инициализации", "url": "https://arxiv.org/abs/1711.04735"}, {"text": "Shampoo", "url": "https://arxiv.org/pdf/1802.09568.pdf"}, {"text": "Decoupled Weight Decay Regularization", "url": "https://arxiv.org/abs/1711.05101"}, {"text": "регуляризацию", "url": "https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#l-2-regulyarizacziya"}, {"text": "регуляризацию", "url": "https://academy.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii#l-2-regulyarizacziya"}, {"text": "On The Variance Of The Adaptive Learning Rate And Beyond", "url": "https://arxiv.org/pdf/1908.03265.pdf"}]}
{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Online RMSprop", "text": "Особняком стоит метод, описанный в статьеVariants of RMSProp and Adagrad with Logarithmic Regret Bounds. Авторы не придумывали очередной хотфикс, а аккуратно заново выводили формулы. Также важно, что данный метод является строгим обобщением метода AdaGrad. В работе есть два нововведения: Переформулировка метода RMSprop так, чтобы:— Осталось экспоненциальное скользящее среднее;— Не было проблемы с отрицательными регуляризаторами и взрывающимися learning rate;— Метод AdaGrad являлся частным случаем нового метода;— Чтобы все эмпирически хорошо работало в т.ч. на глубоких моделях Формулировка новых алгоритмов оптимизации SC-AdaGrad и SC-RMSprop для сильно выпуклых функций с логарифмическими гарантиями на regret. SC в названии — Strongly Convex. Пока рассмотрим только первый пункт. Авторы вводят следующий общий метод: Нововведение здесь в том, что вместо фиксированногомы будем рассматривать последовательность. Авторы доказывают сублинейный regret для любых последовательностей, удовлетворяющих Докажем, что метод Adagrad — это метод OnlineRMSprop с. Аналогично выводам momentum в FTRL, перепишем рекуррентное выражение для: Подставив, получим Далее, подставляя это в формулу, получаем Докажем, что OnlineRMSprop не может сломать регуляризаторы в regret. Для этого преобразуем неравенство Из условияполучаем, что правая часть неравенства неположительна, а левая неотрицательно. Значит, последнее неравенство невозможно, то есть все. Таким образом, регуляризаторы не сломаются, сходимость будет иметь место и данный метод можно использовать в выпуклых задачах. Строгое доказательство сходимости и оценки на Regret можно прочитать в исходной статье. Как и ранее в методе AdaGrad, допустим, что. Тогда Привыполнено Докажем, что все элементы предела <1. Из этого, в частности, будет следовать, что learning rate у OnlineRMSprop не меньше, чем learning rate в AdaGrad. Если все все, то итерационный процесс OnlineRMSprop превращается в Предположим, что. Тогда: По индукции разворачиваем вплоть до, получаем противоречие. Полное доказательство предела оставляем читателям. Надо бы чем-нибудь снизу подпереть, что тоже к 1 сходится. Автор сдавал матан почти 10 лет назад и ему было очень неохота откапывать все эти прекрасные пределы, поэтому ответ был получен с помощью wolfram. Вывод: learning rate у OnlineRMSprop убывает со скоростью. Мы исправили ошибку предыдущего RMSprop, изменивтолькоперевзвешивание, но не асимптотику в. Такой RMSprop можно пробовать использовать в выпуклых задачах", "useful_links": [{"text": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "url": "http://proceedings.mlr.press/v70/mukkamala17a/mukkamala17a.pdf"}]}
{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Momentum", "text": "Попробуем расписать классический momentum с константным learning rate в стиле FTRL: Всё, что нам нужно сделать — это взять все рекурсивные зависимости от предыдущей итерации и «размотать» их, получив явное выражение. Зависимостьотпереписать довольно просто, мы это уже делали для обычного градиентного спуска: Теперь надо размотать Теперь будет чуть сложнее. Подставим это и попробуем расписать, как суммус определенными коэффициентами: Множительсразу выносим за сумму и пока забываем. Отлично, а теперь нам нужно получить последовательность функций. В линеаризованной задаче это фактически эквивалентно получению зависимостиот, где, напомним,— это сумма градиентов. Теперь мы можем записать функцию, градиент которой равени онлайн-оптимизация которой эквивалентна процедуре с моментумом: Получаем, что для онлайн-обучения мы на самом деле каждую итерацию скармливаем экспоненциально взвешенную последовательность всех предыдущих функций исходной последовательности. В принципе, нечто такое мы и ожидали увидеть. Функции, очевидно, выпуклы, так что для данной измененной последовательности функций будет сублинейный regret. Рассмотрим классический SGD с momentum, для всех adaptive методов рассуждения аналогичны. Градиент функциипосчитан в предыдущей точке. Идея nesterov momentum в том, чтобы применить momentum на параметрыдо вычисления градиента: У метода много всяких «интуитивных объяснений», но изначально Nesterov Momentum был выведен сугубо аналитическими методами. Увы, попытки добавлять его в стохастическую оптимизацию «в лоб» обычно улучшением качества не заканчиваются. Анализ того, почему так нельзя и делать и как можно сделать правильно, проводится в работахKatyusha: The First Direct Acceleration of Stochastic Gradient MethodsиNatasha-2(мотивация их автора Zeyuan Allen-Zhu для выбора таких наименований доподлинно неизвестна). Katuysha правильным образом использует nesterov momentum для выпуклого случая, Natasha — для невыпуклого. Данные методы используют подход SVRG для улучшения сходимости и ускорение оптимизации происходиттолько при приближении к точке оптимума. До недавнего времени громких историй успеха для nesterov momentum в глубоком обучении не было. Метод Natasha распространения не нашел. Наконец, авторы статьиAdan(2022) нашли способ правильной обработки Nesterov Momentum. Метод показал отличные результаты и обновил SOTA метрики на широком спектре задач.", "useful_links": [{"text": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "url": "https://arxiv.org/pdf/1603.05953.pdf"}, {"text": "Natasha-2", "url": "https://arxiv.org/abs/1708.08694"}, {"text": "Adan", "url": "https://arxiv.org/abs/2208.06677"}]}
{"document_title": "Методы оптимизации в Deep Learning", "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning", "section_title": "Собираем все идеи воедино", "text": "Авторы данного обзора очень хотят, чтобы читатель ушел не с знанием набора наименований методов оптимизации, а с знанием набора концепций, которые тот или иной метод реализует, и при случае мог сам подстроить метод под свои нужды. Тюнинг методов оптимизации — один из главных способов улучшения качества модели на фиксированном датасете. Adaptive learning rate — автоматическое подстраивание метода под геометрию задачи оптимизации. Крайне важный класс методов для выпуклых/невыпуклых задач. Must-have для разреженных моделей. Методы: AdaGrad/RMSprop/Adam. Скользящее среднее в adaptive learning rate представлено в методах RMSprop/Adam. Не забывайте про их плохое поведение вокруг критических точек и проблемы со сходимостью на финальных этапах оптимизации. BiasCorrection: стабилизация обучения на старте для адаптивных методов со скользящим средним. Большинство экспериментов показывают, что это крайне полезная штука и стоит всегда её использовать. В том числе стоит использовать RMSprop с bias correction, если вам не нужны momentum и Adam. AMSgrad: способ починить сходимость RMSprop/Adam. Не забывайте, что стандартные реализации при использовании AMSgrad отключают bias correction, а это на самом деле может навредить, а также о том, что можно реализовать AMSgrad без дополнительной памяти, и всё будет хорошо работать. Learning rate decay: убывание learning rate зачастую является очень важной деталью в стохастической оптимизации. Помните, что можно брать как AdaGrad, в котором это есть из коробки со скоростью(но архитектура нейросети должна быть хорошей), так и комбинацию RMSProp/Adam + learning rate scheduler. WarmRestart: эвристика, резко увеличивающая learning rate после достижения некоторой точки в процессе оптимизации. Практически всегда идет бок о бок с learning rate decay. Где-то помогает Проксимальные методы для функций потерь с регуляризаторами: ProximalGD/AdamW/SGDW/FTRL-Proximal. Must-have для-регуляризаторов, без проксимальности они вообще не работают. FTRL-Proximal: lazy vs greedy представление. Переписываем представление любого метода оптимизации в не-жадный вид. Позволяет по-новому взглянуть на любые регуляризаторы, особенно негладкие. Must-have для-регуляризации. -регуляризация в FTRL-Proximal: Incremental/Fixed/SquareIncremental. Все три имеют разные свойства и разную область применения. Fixed является наилучшим для отбора разреженных признаков/эмбеддингов. -регуляризатор для отбора эмбеддингов или автоматического подбора размерности. Можно использовать как аналог FSTR. Крайне полезный подход для разреженных нейросетей в рекомендательных системах, для которых рекомендуется использовать адаптивную схему SquareIncremental. Heavy-ball Momentum: используется для ускорения процесса оптимизации. В выпуклых задачах имеет доказанные оценки на улучшение скорости сходимости, в нейросетях используется как эвристика (зачастую опциональная). Nesterov momentum: в выпуклом случае гораздо мощнее для batch gradient descent, чем обычный momentum, и это подверждается теоретическими гарантиями. В стохастических методах оптимизации и в онлайн обучении «в лоб» применять нельзя: для выпуклого случая подойдет Katyusha, для нейросетей — Adan. Главное, что мы хотим подчеркнуть, — эти идеи друг другу не противоречат и их можно свободно комбинировать друг с другом. Например, можно собрать себе FTRL-Proximal метод с-регуляризацией, любым momentum и RMSprop learning rate с AMSgrad. Или любую другую комбинацию. Всегда можно выбрать оптимальный набор под задачу. Эти формулы используют все подходы выше в едином фреймворке, чтобы наглядно убедиться в том, что все можно друг с другом комбинировать. Generic FTRL-Proximal Generic Mirror (Proximal) Gradient Descent Связь:", "useful_links": []}
