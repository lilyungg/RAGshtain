{"document_title": "Ландшафт функции потерь", "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter", "section_title": "Все минимумы достаточно широкой нелинейной сети глобальны", "text": "Рассмотрим нейронную сеть с одним скрытым слоем: где функция активацииприменяется поэлементно. Рассмотрим набор данныхразмера, где, а. Применяя соотношения выше к этому набору, получим следующие значения выходов слоёв: Поставим задачу оптимизации квадратичной функции потерь где– норма Фробениуса. Теорема 1(On the local minima free condition of backpropagation learning) Еслианалитична, ограничена и не тождественно равна нулю, ширина скрытого слояне меньшеи все столбцы матрицыразличны, то все локальные минимумыглобальны. Доказательство. Пусть– локальный минимум, и пусть,– соответствующие ему скрытые представления. Тогда– локальный минимум. Задача оптимизациивыпуклая, поэтому– глобальный минимум. Если, то система, где– неизвестная матрица, гарантировано имеет решение для каждого. Следовательно, а значит,– глобальный минимум. Заметим, что для выполнения равенстванеобходимо. Пусть теперь. Если тем не менее, то– по-прежнему глобальный минимум. Пусть Докажем, чтоне может быть локальным минимумомдо тех пор, пока выполнены условия следующей леммы, которую мы докажем позже: Лемма 1. Если, функцияаналитична, ограничена и не тождественно равна нулю, а все столбцы матрицыразличны, то лебегова мера множестваравна нулю. Так каки– непрерывна как функция от, существует, для которого где черезмы обозначили-окрестность точкив пространстве весов. Из леммы 1 следует, что для любогонайдётся, для которого. Возьмём. Для соответствующегоимеем; при этом. Как было отмечено выше, задача минимизациивыпуклая, и оптимум её равен нулю, так как. Поэтому градиентный спуск, применённый ки стартующий в, сойдётся в некоторую точку, для которой. Мы знаем, что в нашей эпсилон-окрестности функция потерь положительна, значит, найденная точка находится вне её:. Таким образом, найдётсятакое, что для любыхсуществует паратакая, что градиентный спуск, примененный к, стартующий ви действующий только на, сходится в точку. Очевидно, что если «для любых» заменить на «для любых», утверждение выше останется верным. Это означает, что динамика градиентного спуска, действующего только на, не устойчива по Ляпунову в точке. Следовательно,не может быть точкой минимума (иначе градиентный спуск был бы устойчив), а значит, условиеневыполнимо в условиях леммы 1. Таким образом, все локальные минимумыглобальны.Теорема 1 доказана. Доказательство леммы 1. Пусть– наборов индексов издлины. Рассмотрим– подматрицу матрицы, состоящую из строк, проиндексированных набором. В терминахусловиеэквивалентно. Так каканалитична, а определитель – аналитическая функция элементов матрицы,– аналитическая функция отдля любого. Нам понадобится следующая лемма, доказательство которой вы можете найти вThe loss surface of deep and wide neural networks(лемма 4.3): Лемма 2.В условиях леммы 1, найдётся, для которого. Из леммы 2 и эквивалентности выше следует, что найдётся, такой что для некоторогоимеет место неравенство. Так как определитель– аналитическая функция, а всякая не тождественно нулевая аналитическая функция принимает значение ноль лишь на множестве меры ноль по Лебегу, то лебегова мера множестваравна нулю. Таким образом,лемма 1 доказана.", "useful_links": [{"text": "On the local minima free condition of backpropagation learning", "url": "https://ieeexplore.ieee.org/document/410380"}, {"text": "The loss surface of deep and wide neural networks", "url": "https://arxiv.org/pdf/1704.08045.pdf"}]}
{"document_title": "Ландшафт функции потерь", "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter", "section_title": "Обобщения", "text": "При доказательстве теоремы 1 мы воспользовались следующими условиями: Все обучающие примеры (столбцы матрицы) различны; Число скрытых слоёвравно одному; Ширина (последнего) скрытого слоя не меньше числа примеров:; Функция активациианалитична, ограничена и не тождественно равна нулю; Функция ошибки квадратична. Можем ли мы ослабить какие-то из них? Если какие-то из примеров совпадают и соответствующие метки также одинаковы, теорема обобщается тривиально. Если же метки не совпадают, то нулевая ошибка, вообще говоря, недостижима. Тем не менее, доказательство меняется по большому счёту лишь в том, что вместобудет фигурировать число различных примеров. Рассмотрим сеть сскрытыми слоями, действующую на набор данныхразмера: Для обобщения теоремы 1 на глубокие сети с широким последним скрытым слоем, достаточно обобщить лемму 1. Например, можно воспользоваться следующим результатом (лемма 4.4 изThe loss surface of deep and wide neural networks) Лемма 3. Пустьаналитична, ограничена и не тождественно равна нулю, и пусть. Тогда еслии все строки матрицыразличны, то лебегова мера множестваравна нулю. Третье предположение можно попытаться ослабить с двух сторон. Во-первых, можно требовать меньшего числа нейронов в скрытом слое. В общем случае этот подход не работает: в статьеA note on connectivity of sublevel sets in deep learningдоказывается, что– это наименьшая ширина, при которой теорема выполняется для набора данных общего вида с различными примерами. Тем не менее, для реальных нейронных сетей градиентный спуск нередко находит глобальный минимум, хотя их ширина часто гораздо меньше размера набора данных, на которых они обучаются. Возможно, оценки на минимальную ширину удастся улучшить, если учесть структуру данных: например, если все примеры разбиваются на подмножества с элементами, находящимися близко друг к другу и имеющими одинаковые метки. Во-вторых, можно предположить, что самым широким является не последний скрытый слой, а один из промежуточных:для некоторого. Но тогда задачане выпукла, а значит, изне следует, что, и градиентный спуск, действующий на, не обязан сходиться в точку, в которой(он может застрять в локальном минимуме). Тем не менее, поставив ряд дополнительных условий, теорему 1 можно обобщить: Теорема 2. Пусть– локальный минимуми выполнены следующие условия: аналитична, ограничена, не тождественно равна нулю; производнаянигде не обращается в ноль; ; ; . Тогда– глобальный минимум. Условие 4 необходимо, чтобы изследовало. Отметим, что из условия 4 также следует, что, то есть нейронная сеть должна сужаться, начиная со следующего после самого широкого слоя. Условие 5 необходимо, чтобы в случаепостроить малое возмущение минимума, которое снова является минимумом, но для которого; невырожденный гессиан позволяет применить для этого теорему об обратной функции. Если функция активациине аналитична, то лемма 3 неверна. В самом деле, для однородной(например, для ReLU или leaky ReLU) паттерны активаций,, не меняются при малом возмущении весов. Значит, мы, вообще говоря, не можем найти такое малое возмущение, для которого рангбудет полным. Вместо малого возмущения в работеOn Connected Sublevel Sets in Deep Learningявно строятся пути в пространстве весов, на которых функция потерь не возрастает и достигает нуля. Если такой путь можно построить из произвольной точки, то все (строгие) локальные минимумы глобальны. Оказывается, что для построения такого пути аналитичность функции активации не требуется. Более того, при определённых условиях можно доказать, что из любых двух точек в пространстве весов можно построить соответствующие пути так, чтобы они сходились в одной точке. Это значит, чтомножество подуровнясвязно при любом– эффект, впервые эмпирически обнаруженный в работахLoss surfaces, mode connectivity, and fast ensembling of DNNsиEssentially No Barriers in Neural Network Energy Landscape. Связность множеств подуровня сильнее глобальности всех строгих минимумов. В самом деле, если бы существовал строгий локальный минимум уровня, то для достаточно малогоему бы соответствовала отдельная связная компонента множества подуровня. С другой стороны, если все локальные минимумы глобальны, но изолированы, то множество подуровнянесвязно для достаточно малого. Вместо того, чтобы строить пути, на которых функция потерь достигает нуля, можно строить пути, на которых функция потерь достигает сколь угодно малого значения. Это позволяет обобщить результат на функции потерь, для которых минимум по второму аргументу, ответу сети, не достигается. Пример такой функции – кросс-энтропия. Так мы приходим к следующей теореме: Теорема(On Connected Sublevel Sets in Deep Learning). Пусть выполнены следующие условия: ,строго монотонна и не найдётся ненулевыхс, таких что; выпукла по второму аргументу идля любого; Существует, для которогоидля всех. Тогда если, то Для каждогонайдутся веса, для которых; Множество подуровнясвязно для каждого.", "useful_links": [{"text": "The loss surface of deep and wide neural networks", "url": "https://arxiv.org/pdf/1704.08045.pdf"}, {"text": "A note on connectivity of sublevel sets in deep learning", "url": "https://arxiv.org/pdf/2101.08576.pdf"}, {"text": "On Connected Sublevel Sets in Deep Learning", "url": "https://arxiv.org/pdf/1901.07417.pdf"}, {"text": "Loss surfaces, mode connectivity, and fast ensembling of DNNs", "url": "https://arxiv.org/pdf/1802.10026.pdf"}, {"text": "Essentially No Barriers in Neural Network Energy Landscape", "url": "https://arxiv.org/pdf/1803.00885.pdf"}, {"text": "On Connected Sublevel Sets in Deep Learning", "url": "https://arxiv.org/pdf/1901.07417.pdf"}]}
