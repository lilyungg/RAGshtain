{"document_title": "Первое знакомство с полносвязными нейросетями", "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami", "section_title": "Forward &backward propagation", "text": "Информация может течь по графу в двух направлениях. Применение нейронной сети к данным (вычисление выхода по заданному входу) часто называютпрямым проходом, или жеforward propagation(forward pass). На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым. Приобратном проходе, или жеbackward propagation(backward pass), информация (обычно об ошибке предсказания целевого представления) движется от финального представления (а чаще даже от функции потерь) к исходному через все преобразования. Механизм обратного распространения ошибки, играющий важнейшую роль в обучении нейронных сетей, как раз предполагаетобратноедвижение по вычислительному графу сети. С ним вы познакомитесь в следующемпараграфе.", "useful_links": [{"text": "параграфе", "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki"}]}
{"document_title": "Первое знакомство с полносвязными нейросетями", "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami", "section_title": "Архитектуры для простейших задач", "text": "Как мы уже упоминали выше, нейросети — это универсальный конструктор, который из простых блоков позволяет собрать орудия для решения самых разных задач. Давайте посмотрим на конкретные примеры. Безусловно, мир намного разнообразнее того, что мы покажем вам в этом параграфе, но с чего-то ведь надо начинать, не так ли? В тех несложных ситуациях, которые мы сейчас рассмотрим, архитектура будет отличаться лишь самыми последними этапами вычисления (у сетей будут разные «головы»). Для иллюстрации приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных: Для решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число отдо, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв где— некоторая функция, превращающая представлениев число (если— матрица, то подойдёт, где— вектор-столбец), а— наша любимая сигмоида. При этомможет получаться как угодно, лишь бы хватало оперативной памяти и не было переобучения. В качестве функции потерь удобно брать уже знакомый нам log loss. Работая с другими моделями, мы порой вынуждены были выдумывать сложные стратегии многоклассовой классификации; нейросети позволяют это делать легко и элегантно. Достаточно построить сеть, которая будет выдаватьнеотрицательных чисел, суммирующихся в 1 (где— число классов); тогда им можно придать смысл вероятностей классов и предсказывать тот класс, «вероятность» которого максимальна. Превратить произвольный набор изчисел в набор из неотрицательных чисел, суммирующихся в 1, позволяет, к примеру, функция Наиболее популярные архитектуры для многоклассовой классификации имеют вид где— функция, превращающаяв матрицу(где— размер батча), аможет быть получен любым приятным вам образом. Но какой будет функция потерь для такой сети? Мы должны научиться сравнивать «распределение вероятностей классов» с истинным (в котором на месте истинного класса стоит 1, а в остальных местах 0). Сделать это позволяеткросс-энтропия, она жеnegative log-likelihood— некоторый аналог расстояния между распределениями: где снова— размер батча, а— число классов. Легко видеть, что приполучается та самая функция потерь, которую мы использовали для обучения бинарной классификации. С помощью нейросетей легко создать модель, которая предсказывает не одно число, а сразу несколько. Например, координаты ключевых точек лица — кончика носа, уголков рта и так далее. Достаточно сделать, чтобы последнее представление было матрицей, где— размер батча, а— количество предсказываемых чисел. Особенностью большинства моделей регрессии является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например,по всей матрице. Если вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация). Лосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так что, по крайней мере в теории, сетям подвластны любые задачи. На практике, конечно, всё гораздо хитрей: для обучения слишком сложной сети у вас может не хватить данных или вычислительных мощностей.", "useful_links": []}
{"document_title": "Первое знакомство с полносвязными нейросетями", "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami", "section_title": "Популярные функции активации", "text": "Для начала поговорим о том, зачем они нужны. Казалось бы, можно последовательно выстраивать лишь линейные слои, но так не делают: после каждого линейного слоя обязательно вставляют функцию активации. Но зачем? Попробуем разобраться. Рассмотрим нейронную сеть из двух линейных слоёв. Что произойдёт, если между ними будет отсутствовать нелинейная функция активации? Линейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою. Добавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Вдобавок правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами. В качестве функции активации может использоваться, например, уже знакомая вам из логистической регрессии сигмоидаили ReLU (Rectified linear unit). К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее. Примечание. На самом деле бывают ситуации, когда два линейных слоя подряд — это полезно. Например, если вы понимаете, что у вас очень много параметров, а информации в данных не так много, вы можете заменить линейный слой, превращающий-мерные векторы в-мерные, на два, вставив посередине-мерное представление, где: С точки зрения линейной алгебры это примерно то же самое, что потребовать, чтобы матрица исходного линейного слоя имела ранг не выше. И с точки зрения сужения «информационного канала» это иногда может сработать. Но в любом случае вы должны понимать, что два линейных слоя подряд стоит ставить, только если вы хорошо понимаете, чего хотите добиться. Вернёмся к функциям активации. Вот наиболее популярные: Рассмотрим их подробнее. Формула: ReLU это простая кусочно-линейная функция. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением. Плюсы: простота вычисления активации и производной. Минусы: область значений является смещённой относительно нуля; для отрицательных значений производная равна нулю, что может привести к затуханию градиента. ReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды. Формула: Гиперпараметробеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля. Формула: Аналогична Leaky ReLU, но параметрнастраивается градиентными методами. ELU – это гладкая аппроксимация ReLU. Обладает более высокой вычислительной сложностью, достаточно редко используется на практике. Формула: Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона. Плюсы: Минусы: область значений смещена относительно нуля; сигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflowquestion 1,question 2; на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента; максимальное значение производной составляет, что также приводит к затуханию градиента. На практике сигмоида редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM). Формула: Плюсы: как и сигмоида, имеет ограниченную область значений; в отличие от сигмоиды, область значений симметрична. Минусы: требует вычисления экспоненты, что является достаточно сложной вычислительной операцией; на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента. Вопрос на подумать. А почему симметричность области значений может быть ценным свойством?", "useful_links": [{"text": "question 1", "url": "https://stackoverflow.com/questions/53419270/how-does-numpy-compute-an-exponential"}, {"text": "question 2", "url": "https://stackoverflow.com/questions/9799041/efficient-implementation-of-natural-logarithm-ln-and-exponentiation"}]}
{"document_title": "Первое знакомство с полносвязными нейросетями", "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami", "section_title": "Немного о мощи нейросетей", "text": "Рассмотрим для начала задачу регрессии. Ясно, что линейная модель (то есть однослойная нейросеть) может приблизить только линейную функцию, но уже двухслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них. Обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art. Теорема Цыбенко (1989).Для любой непрерывной функциии для любогонайдётся число, а также числа,, для которых для любыхиз единичного кубав. В сумме из теоремы Цыбенко легко опознать двуслойную нейросеть с сигмоидной функцией активации. В самом деле, сперва мы превращаемв— это можно представить в виде одной матричной операции (линейный слой!): где— вектор-столбцы, а каждое изприбавляется к-му столбцу, после чего поэлементно берём отсигмоиду (активация), после чего вычисляем и это второй линейный слой (без свободного члена). Правда, теорема не очень помогает находить такие функции, но это уже другое дело. В любом случае — если дать нейросети достаточно данных, она действительно может выучить почти что угодно. Упражнение.Мы не будем приводить результатов, касающихся классификации, но рекомендуем воспользоваться замечательнойпесочницей. Убедитесь сами, что при использовании одного скрытого слоя из двух нейронов и сигмоиды в качестве функции активации, можно неплохо классифицировать данные со сложной, совсем даже не линейной границей между классами. Вы также можете поиграть с разными функциями активации. А для получения решения нам необходим метод автоматической настройки всех параметров нейронной сети —метод обратного распространения ошибки, или жеerror backpropagation. Рассмотрим его в деталях в следующем параграфе.", "useful_links": [{"text": "песочницей", "url": "https://playground.tensorflow.org"}]}
