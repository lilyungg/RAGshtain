[
  {
    "document_title": "PAC-байесовские оценки риска",
    "url": "https://education.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska",
    "section_title": "Применение пак-байесовских оценок к детерминированным алгоритмам обучения",
    "text": "Выше были рассмотрены две PAC-байесовские оценки: одна для не более, чем счётного множества моделей, другая – для произвольного. За возможность использования несчётных классов моделей мы заплатили тем, что алгоритм обучения должен быть недетерминированным (для детерминированных алгоритмов KL-дивергенция в Теореме Макаллестера может вырождаться в бесконечность; например, это так, если априорное распределение гауссово). Чаще всего класс моделейвсё-таки несчетён: например, если это класс всех сетей фиксированной архитектуры, то он индексируется весами, которых несчётное множество. При этом, хотя используемый алгоритм обучения и в самом деле недетерминирован (стохастический градиентный спуск зависит от случайного выбора батчей и от инициализации весов) и теорема Макаллестера выполняется, финальное распределение моделей очень сложно охарактеризовать, и из-за этого непонятно, как считать KL-дивергенцию. Предположим, что алгоритм обучения всё-таки детерминирован; этого можно добиться, зафиксировав сид генератора случайных чисел при обучении. Как получить осмысленную PAC-байесовскую оценку для детерминированного алгоритма на несчётном множестве моделей? Мы рассмотримдва способа. Первый способ – добавить известный шум в финальную модель, выданную детерминированным алгоритмом. Так, для нейронных сетей, результатом работы алгоритма обучения является набор весов. Если добавить в этот набор гауссовский шум, а также в качестве априорного распределения взять гауссовское, то KL-дивергенцию в теореме Макаллестера можно будет посчитать аналитически. Дисперсию шума в апостериорном распределении тоже можно обучить с помощью градиентного спуска одновременно с весами, тем самым минимизируя правую часть оценки из вышеупомянутой теоремы. Если в найденную модель удастся добавить шум так, чтобы KL-дивергенция значительно уменьшилась, но при этом риск на обучающей выборке не сильно вырос, то оценка на истинный риск получится хорошей. Это рассуждение связывает PAC-байесовские оценки и гипотезу о том, что «плоские» («широкие») минимумы хорошо обобщают. В самом деле, если минимум «плоский», то в модель из него можно добавить много шума, не испортив качество на обучении. Оценки, основанные на этом принципе, можно найти в работахComputing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training dataиA PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. Второй способ состоит в том, чтобы взять дискретное кодированиеи применить дискретную PAC-байесовскую оценку к закодированной модели вместо оригинальной. Обозначим закодированную модельчерез. Следуя работеNon-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach, возьмём априорное распределение с массой, убывающей с ростом длины кода: Здесь– длина кода модели,– некоторое вероятностное распределение на, а– нормализующая константа. Тогда KL-дивергенция примет следующий вид: Для того, чтобы KL-дивергенция выше была как можно меньше, необходимо, чтобы наш алгоритм обучения на реалистичных данных сходился в модели с маленькой длиной кода. Для этого будем применять наше кодирование не к оригинальной модели, а к сжатой с помощью некоторого алгоритма сжатия. Здесь мы предполагаем, что модели, к которым сходится наш алгоритм обучения, можно сжать с малыми потерями до моделей с малой длиной кода. Другими словами, мы опираемся на предположение, что обученные модели в некоторым смысле «простые». Если модель параметризована весами, типичный алгоритм сжатия выдаст набор, где – позиции ненулевых весов; – «словарь» весов; ,– квантизованные значения весов. Выход алгоритма будет выглядеть как, если, иначе. Тогда наивное 32-битное кодирование даст следующую длину: В работеNon-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approachописанный выше способ применяется к модели MobileNet (свёрточной сети, сконструированной специально для мобильных устройств), обученной на наборе данных ImageNet, и получают верхнюю оценку на истинный риск, равную(риск случайного угадывания –). Хотя такой результат и выглядит очень скромным, но это первая осмысленная оценка обобщающей способности реально используемой нейронной сети на реалистичном наборе данных.",
    "useful_links": [
      {
        "text": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
        "url": "https://arxiv.org/pdf/1703.11008.pdf"
      },
      {
        "text": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
        "url": "https://arxiv.org/pdf/1707.09564.pdf"
      },
      {
        "text": "Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach",
        "url": "https://arxiv.org/pdf/1804.05862.pdf"
      },
      {
        "text": "Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach",
        "url": "https://arxiv.org/pdf/1804.05862.pdf"
      }
    ]
  }
]