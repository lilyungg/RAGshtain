# BE-ALWAYS-READY — RAG-система для подготовки к техническим собеседованиям

BE-ALWAYS-READY — это Retrieval-Augmented Generation (RAG) система, предназначенная для помощи в подготовке к техническим собеседованиям в IT.  
Система использует внешний корпус знаний, векторный поиск и цепочку обработки запросов для генерации ответов с опорой на источники.

Проект не сводится к простому промптированию LLM: реализован полноценный RAG-пайплайн с retrieval, агрегацией контекста, агентной логикой и оценкой качества ответов.

---

## Основные возможности

- Retrieval из векторной базы **FAISS**
- Агрегация контекста из нескольких источников
- Query rewriting и роутинг запросов
- Генерация ответов с опорой на retrieved-документы
- Возврат источников (URL, title)
- LLM Chain / агентный слой с нелинейной логикой
- Оценка качества retrieval и генерации
- End-to-end интерфейс (бот)
- Docker-деплой и воспроизводимость пайплайна

---

## Источники данных

В проекте используются материалы для подготовки к техническим интервью:

- **Yandex Handbook**
- **Habr**
- **Материалы Lena Voita**

Для автоматического сбора и индексации реализованы специальные индексаторы:
- `HabrIndex`
- `LenaVoitaIndex`

Все данные:
- сохраняются в формате JSON,
- разбиваются на чанки,
- снабжаются метаданными (`source`, `url`, `title`),
- загружаются в FAISS.

Примеры чанков находятся в папке `examples`.
Через бота можно грузить новые источнкии.

---

## Архитектура системы

### Общий пайплайн

1. Пользователь задаёт вопрос  
2. Query rewriting формирует вспомогательные подзапросы  
3. Router определяет тип запроса  
4. Запрос векторизуется  
5. Выполняется retrieval в FAISS  
6. Возвращаются `top-k` релевантных чанков с метаданными  
7. Формируется контекст с учётом лимита токенов  
8. Контекст + вопрос передаются в LLM  
9. Генерируется ответ  
10. Ответ возвращается пользователю вместе со списком источников  

---

### Компоненты

#### Ingestion / Indexing
- Сплитинг документов по структуре (заголовки 1–2 уровня)
- Чанкирование по количеству слов
- Векторизация чанков
- Сохранение FAISS-индекса и метаданных

#### Retrieval
- Векторизация запроса
- `top-k` поиск в FAISS
- Возможность расширения (rerank, фильтры, diversity)

#### Generation
- LLM: **DeepSeek** или **OpenAI** (выбор через `.env`)
- Промпт: вопрос + контекст + инструкции по цитированию
- Выход: ответ + список источников

#### Agent / Chain layer
- Планирование шагов обработки запроса
- Повторный retrieval при необходимости
- Проверка качества ответа (LLM-as-Judge)
- Сбор финального ответа

---

## Оценка качества

### Retrieval-метрики

Используются стандартные IR-метрики:
- Recall@k
- Precision@k
- MRR
- nDCG@k

### Метрики генерации

Для оценки финального ответа применяется подход **LLM-as-Judge** и библиотека `ragas`:
- Answer relevancy
- Answer correctness
- Опора на источники

### Результаты экспериментов

**Baseline (all-MiniLM-L6-v2):**
```text
MRR:        0.1884
Recall@k:   0.2261
Precision@k:0.0962
nDCG@k:     0.1987
Основная модель (e5-large):
```

```text
MRR:        0.9945
Recall@k:   0.9982
Precision@k:0.7365
nDCG@k:     0.9975
Answer Correctness Score: 4.95
Answer Relevancy: 0.923
Answer Correctness: 0.959
```
### Быстрый старт
1. Перейдите в папку service
2. В .env все параметры дефольны кроме: MODEL_SOURCE(openai или deepseak), OPENAI_API_KEY, DEEPSEEK_API_KEY, BOT_TOKEN.
Укажите необходимые поля.
3. Сборка и запуск Docker
```
docker compose up
```
4. Через бота можно общаться с RAG системой


### Интерфейс

Реализован пользовательский интерфейс (бот), который:
* принимает запросы пользователя,
* возвращает ответы и источники,
* корректно обрабатывает ошибки,
* подключён ко всей системе end-to-end.
### Репозиторий и код
* Оформленный README
* Описание архитектуры
* Инструкции по запуску
* Примеры использования
* Примеры валидации и метрик
* Актуальный requirements.txt
* Код структурирован, читаем и соответствует стандартам PEP8.

### Командная работа
* Зафиксированы зоны ответственности участников
* Коммиты распределены между членами команды
* В презентации проекта предусмотрен отдельный слайд с вкладом каждого участника

### Итог
В рамках проекта реализована и задеплоена полноценная RAG-система:
* с реальным корпусом данных,
* retrieval-компонентом,
* генерацией ответов на основе контекста,
* метриками качества,
* рабочим пользовательским интерфейсом.
*  Проект полностью соответствует формальным требованиям курса и готов к проверке.

## Пример работы

![img.png](img.png)
Выбор двух режимов, либо чат, либо загрузка.
____
![img_1.png](img_1.png)
Успешная загрзука данных.
___
![img_2.png](img_2.png)
Переписка с ботом.
